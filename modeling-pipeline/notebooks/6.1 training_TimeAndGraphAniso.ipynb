{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook \n",
    "\n",
    "In this notebook, we will create the graph, build the STGNN model, perform training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SoilWaterDataset object \n",
    "\n",
    "SoilWaterDataset is a fundamental object created based on TabularDataset from the TSL library. Using this object, we will be able to enabling efficient loading, preprocessing, and spatiotemporal structuring of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from tsl.datasets.prototypes import TabularDataset\n",
    "from tsl.ops.similarities import gaussian_kernel\n",
    "\n",
    "class SoilWaterDataset(TabularDataset):\n",
    "\n",
    "    similarity_options = {'distance', 'grid'}\n",
    "\n",
    "    def __init__(self,\n",
    "                 root: str = None\n",
    "                 ):\n",
    "\n",
    "        self.root = root\n",
    "\n",
    "        # Load data\n",
    "        target, mask, u, dist, metadata = self.load()\n",
    "\n",
    "        covariates = {\n",
    "            'u': (u),\n",
    "            'metadata' : (metadata),\n",
    "            'distances': (dist)\n",
    "        }\n",
    "\n",
    "        super().__init__(target=target,\n",
    "                         mask=mask,\n",
    "                         covariates=covariates,\n",
    "                         similarity_score='distance',\n",
    "                         temporal_aggregation='mean',\n",
    "                         spatial_aggregation='mean',\n",
    "                         name='SoilWaterDataset')\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load data from files.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Containing target, mask, covariates, distances, and metadata.\n",
    "        \"\"\"\n",
    "        target_path = f\"{self.root}target.npy\"\n",
    "        mask_path = f\"{self.root}mask.npy\"\n",
    "        dist_path = f\"{self.root}distance_matrix.npy\"\n",
    "        covariates_path = f\"{self.root}covariates.npy\"\n",
    "        metadata_path = f\"{self.root}metadata.npy\"\n",
    "\n",
    "        target = np.load(target_path)\n",
    "        mask = np.load(mask_path)\n",
    "        u = np.load(covariates_path)\n",
    "        dist = np.load(dist_path)\n",
    "        metadata = np.load(metadata_path)\n",
    "\n",
    "        return target, mask, u, dist, metadata\n",
    "\n",
    "\n",
    "    def compute_similarity(self, method: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute similarity matrix based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            method (str): The similarity computation method ('distance' or 'grid').\n",
    "            **kwargs: Additional keyword arguments for similarity computation.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Computed similarity matrix.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an unknown similarity method is provided.\n",
    "        \"\"\"\n",
    "        if method == \"distance\":\n",
    "            # Calculate a Gaussian kernel similarity from the distance matrix, using a default or provided 'theta'\n",
    "            theta = kwargs.get('theta', np.std(self.distances))\n",
    "            return gaussian_kernel(self.distances, theta=theta)\n",
    "        elif method == \"grid\":\n",
    "            dist = self.distances.copy()\n",
    "            dist[dist > 16] = np.inf  # keep only grid edges\n",
    "            theta = kwargs.get('theta', 20)\n",
    "            return gaussian_kernel(dist, theta=theta)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown similarity method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SoilWaterDataset(root='ml-drought-forecasting/soil-water-forecasting/data/05_model_input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.6496307e-01],\n",
       "        [ 1.6496307e-01],\n",
       "        [ 1.6496307e-01],\n",
       "        ...,\n",
       "        [ 2.9243529e-07],\n",
       "        [ 2.9243529e-07],\n",
       "        [ 2.9243529e-07]],\n",
       "\n",
       "       [[ 1.6687536e-01],\n",
       "        [ 1.6687536e-01],\n",
       "        [ 1.6687536e-01],\n",
       "        ...,\n",
       "        [ 5.2526593e-06],\n",
       "        [ 5.2526593e-06],\n",
       "        [ 5.2526593e-06]],\n",
       "\n",
       "       [[ 1.6724145e-01],\n",
       "        [ 1.6724145e-01],\n",
       "        [ 1.6724145e-01],\n",
       "        ...,\n",
       "        [-1.0136282e-05],\n",
       "        [-1.0136282e-05],\n",
       "        [-1.0136282e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.2181239e-01],\n",
       "        [ 3.2181239e-01],\n",
       "        [ 3.2181239e-01],\n",
       "        ...,\n",
       "        [ 4.5273919e-06],\n",
       "        [ 4.5273919e-06],\n",
       "        [ 4.5273919e-06]],\n",
       "\n",
       "       [[ 3.2685500e-01],\n",
       "        [ 3.2685500e-01],\n",
       "        [ 3.2685500e-01],\n",
       "        ...,\n",
       "        [-3.5299454e-06],\n",
       "        [-3.5299454e-06],\n",
       "        [-3.5299454e-06]],\n",
       "\n",
       "       [[ 3.2872993e-01],\n",
       "        [ 3.2872993e-01],\n",
       "        [ 3.2872993e-01],\n",
       "        ...,\n",
       "        [-5.4172706e-06],\n",
       "        [-5.4172706e-06],\n",
       "        [-5.4172706e-06]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has missing values: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Has missing values: {dataset.has_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_mask(dataset.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u': array([[[ 3.0585480e-01, -1.2989902e-01,  2.4335590e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  1.0000000e+00],\n",
       "         [ 3.0585480e-01, -1.2989902e-01,  2.4335590e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  1.0000000e+00],\n",
       "         [ 3.0585480e-01, -1.2989902e-01,  2.4335590e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  1.0000000e+00],\n",
       "         ...,\n",
       "         [-7.7445984e-02,  3.2210350e-02,  2.5064105e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  1.0000000e+00],\n",
       "         [-7.7445984e-02,  3.2210350e-02,  2.5064105e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  1.0000000e+00],\n",
       "         [-7.7445984e-02,  3.2210350e-02,  2.5064105e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  1.0000000e+00]],\n",
       " \n",
       "        [[ 4.1980553e-01,  1.1558628e-01,  2.3336655e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  1.0000000e+00],\n",
       "         [ 4.1980553e-01,  1.1558628e-01,  2.3336655e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  1.0000000e+00],\n",
       "         [ 4.1980553e-01,  1.1558628e-01,  2.3336655e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  1.0000000e+00],\n",
       "         ...,\n",
       "         [-4.7479630e-02,  6.0898781e-02,  2.5171616e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  1.0000000e+00],\n",
       "         [-4.7479630e-02,  6.0898781e-02,  2.5171616e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  1.0000000e+00],\n",
       "         [-4.7479630e-02,  6.0898781e-02,  2.5171616e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  1.0000000e+00]],\n",
       " \n",
       "        [[ 6.0301781e-01,  8.3531380e-02,  2.2287155e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  2.0000000e+00],\n",
       "         [ 6.0301781e-01,  8.3531380e-02,  2.2287155e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  2.0000000e+00],\n",
       "         [ 6.0301781e-01,  8.3531380e-02,  2.2287155e+02, ...,\n",
       "           0.0000000e+00,  2.7355104e+04,  2.0000000e+00],\n",
       "         ...,\n",
       "         [-5.2743912e-02,  6.9371223e-02,  2.5145749e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  2.0000000e+00],\n",
       "         [-5.2743912e-02,  6.9371223e-02,  2.5145749e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  2.0000000e+00],\n",
       "         [-5.2743912e-02,  6.9371223e-02,  2.5145749e+02, ...,\n",
       "           0.0000000e+00,  3.2275391e-01,  2.0000000e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 4.6406879e+00,  1.9648352e+00,  2.2614645e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  4.0000000e+00],\n",
       "         [ 4.6406879e+00,  1.9648352e+00,  2.2614645e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  4.0000000e+00],\n",
       "         [ 4.6406879e+00,  1.9648352e+00,  2.2614645e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  4.0000000e+00],\n",
       "         ...,\n",
       "         [ 1.4898872e-01, -4.2977333e-02,  2.5982614e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  4.0000000e+00],\n",
       "         [ 1.4898872e-01, -4.2977333e-02,  2.5982614e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  4.0000000e+00],\n",
       "         [ 1.4898872e-01, -4.2977333e-02,  2.5982614e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  4.0000000e+00]],\n",
       " \n",
       "        [[ 2.4678459e+00,  2.2457905e+00,  2.3805811e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  4.0000000e+00],\n",
       "         [ 2.4678459e+00,  2.2457905e+00,  2.3805811e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  4.0000000e+00],\n",
       "         [ 2.4678459e+00,  2.2457905e+00,  2.3805811e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  4.0000000e+00],\n",
       "         ...,\n",
       "         [-2.9545174e+00, -1.2335014e-01,  2.5271436e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  4.0000000e+00],\n",
       "         [-2.9545174e+00, -1.2335014e-01,  2.5271436e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  4.0000000e+00],\n",
       "         [-2.9545174e+00, -1.2335014e-01,  2.5271436e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  4.0000000e+00]],\n",
       " \n",
       "        [[ 2.4026718e+00,  2.3765755e+00,  2.4505371e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  1.0000000e+00],\n",
       "         [ 2.4026718e+00,  2.3765755e+00,  2.4505371e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  1.0000000e+00],\n",
       "         [ 2.4026718e+00,  2.3765755e+00,  2.4505371e+02, ...,\n",
       "           0.0000000e+00,  2.7718275e+04,  1.0000000e+00],\n",
       "         ...,\n",
       "         [ 1.7806244e-01, -1.7955780e-02,  2.5267090e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  1.0000000e+00],\n",
       "         [ 1.7806244e-01, -1.7955780e-02,  2.5267090e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  1.0000000e+00],\n",
       "         [ 1.7806244e-01, -1.7955780e-02,  2.5267090e+02, ...,\n",
       "           0.0000000e+00, -1.2397461e+00,  1.0000000e+00]]], dtype=float32),\n",
       " 'metadata': array([[-90.,   0.],\n",
       "        [-90.,   1.],\n",
       "        [-90.,   2.],\n",
       "        ...,\n",
       "        [ 90., 357.],\n",
       "        [ 90., 358.],\n",
       "        [ 90., 359.]], dtype=float32),\n",
       " 'distances': array([[0.0000000e+00, 6.8086486e-15, 1.3616779e-14, ..., 2.0015115e+04,\n",
       "         2.0015115e+04, 2.0015115e+04],\n",
       "        [6.8086486e-15, 0.0000000e+00, 6.8086486e-15, ..., 2.0015115e+04,\n",
       "         2.0015115e+04, 2.0015115e+04],\n",
       "        [1.3616779e-14, 6.8086486e-15, 0.0000000e+00, ..., 2.0015115e+04,\n",
       "         2.0015115e+04, 2.0015115e+04],\n",
       "        ...,\n",
       "        [2.0015115e+04, 2.0015115e+04, 2.0015115e+04, ..., 0.0000000e+00,\n",
       "         6.8086486e-15, 1.3616779e-14],\n",
       "        [2.0015115e+04, 2.0015115e+04, 2.0015115e+04, ..., 6.8086486e-15,\n",
       "         0.0000000e+00, 6.8086486e-15],\n",
       "        [2.0015115e+04, 2.0015115e+04, 2.0015115e+04, ..., 1.3616779e-14,\n",
       "         6.8086486e-15, 0.0000000e+00]], dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create connectivity to our graph \n",
    "\n",
    "Here, we adjust the connectivity to retain only the five nearest neighbors (knn=5) per node, while excluding self-loops and normalizing along the specified axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T18:38:41.500430Z",
     "start_time": "2024-11-08T18:38:41.498443Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset.distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim = dataset.compute_similarity(\"distance\")  # or dataset.compute_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust connectivity to reduce the number of edges\n",
    "# connectivity = dataset.get_connectivity(  \n",
    "#     method='distance',\n",
    "#     knn=4,     \n",
    "#     include_self=False,\n",
    "#     force_symmetric=True, \n",
    "#     layout=\"csr\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5239/2660955998.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  connectivity = torch.load(\"ml-drought-forecasting/soil-water-forecasting/data/05_model_input/connectivity.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "connectivity = torch.load(\"ml-drought-forecasting/soil-water-forecasting/data/05_model_input/connectivity.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create torch_dataset \n",
    "\n",
    "torch_dataset, created using SpatioTemporalDataset from the tsl library, structures time-series and spatial data (target, covariates, mask, and connectivity) into a form optimized for spatiotemporal model training, enabling easy handling of lookback windows and prediction horizons in forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.data import SpatioTemporalDataset\n",
    "\n",
    "# covariates=dict(u=dataset.covariates['u'])\n",
    "covariates=dataset.covariates\n",
    "mask = dataset.mask\n",
    "\n",
    "torch_dataset = SpatioTemporalDataset(target=dataset.dataframe(),\n",
    "                                      mask=mask,\n",
    "                                      covariates=covariates,\n",
    "                                      connectivity=connectivity,\n",
    "                                      horizon=6, # Predict 7 step ahead\n",
    "                                      window=12, # Use 30 timestamps to predict the next one\n",
    "                                      stride=1 # Move 7 step forward each time\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dataset.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datamodule\n",
    "\n",
    "datamodule, created with SpatioTemporalDataModule, manages the SpatioTemporalDataset by applying scaling, splitting data into train/validation/test sets, and preparing data loaders with batch processing, enabling efficient, modular, and scalable data handling for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.data.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scalers = {\n",
    "    'target': MinMaxScaler(axis=(0, 1)),\n",
    "    'u': MinMaxScaler(axis=(0, 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.data.datamodule import (SpatioTemporalDataModule,\n",
    "                                 TemporalSplitter)\n",
    "                                 \n",
    "# Split data sequentially:\n",
    "#   |------------ dataset -----------|\n",
    "#   |--- train ---|- val -|-- test --|\n",
    "splitter = TemporalSplitter(val_len=0.35, test_len=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatioTemporalDataModule(train_len=None, val_len=None, test_len=None, scalers=[target, u], batch_size=1)\n"
     ]
    }
   ],
   "source": [
    "# Create a SpatioTemporalDataModule\n",
    "datamodule = SpatioTemporalDataModule(\n",
    "    dataset=torch_dataset,\n",
    "    scalers=scalers,\n",
    "    mask_scaling=True,\n",
    "    splitter=splitter,\n",
    "    batch_size=1,  # Reduce batch size\n",
    "    # workers=2,     # Reduce number of workers\n",
    "    )\n",
    "\n",
    "print(datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatioTemporalDataModule(train_len=35, val_len=13, test_len=7, scalers=[target, u], batch_size=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34]),\n",
       " 'val': array([47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " 'test': array([72, 73, 74, 75, 76, 77, 78])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.splitter.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create STGNN Model Architecture\n",
    "\n",
    "The **TimeAndGraphAnisoModel** is built based on code from [HD-TTS\n",
    " repository](https://github.com/marshka/hdtts/blob/main/lib/nn/models/baselines/stgnns/time_and_graph_anisotropic.py) and the research paper by Cini et al. (2023d). This model utilizes spatiotemporal architectures equipped with anisotropic message passing for effective time and space representation.\n",
    "\n",
    "### Reference\n",
    "Cini, A., Marisca, I., Zambon, D., and Alippi, C. *Taming Local Effects in Graph-Based Spatiotemporal Forecasting.* In *Advances in Neural Information Processing Systems,* volume 36, pp. 55375–55393. Curran Associates, Inc., 2023.  \n",
    "[https://arxiv.org/abs/2302.04071](https://arxiv.org/abs/2302.04071)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/marshka/hdtts/blob/main/lib/nn/layers/anisotropic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from tsl.nn.blocks import RNNBase\n",
    "from tsl.nn.layers import Dense, GraphGRUCellBase, Activation\n",
    "\n",
    "\n",
    "class GraphAnisoConv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 1,\n",
    "                 edge_dim: Optional[int] = None,\n",
    "                 activation: str = 'leaky_relu'):\n",
    "        super(GraphAnisoConv, self).__init__(aggr=\"add\", node_dim=-2)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.msg_mlps = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(2 * (in_channels if i == 0 else out_channels),\n",
    "                          out_channels),\n",
    "                Activation(activation),\n",
    "                nn.Linear(out_channels, out_channels),\n",
    "            )\n",
    "            for i in range(kernel_size)\n",
    "        ])\n",
    "\n",
    "        edge_dim = edge_dim or 1  # accommodate for edge_weight\n",
    "        self.lin_edge = nn.Linear(edge_dim, out_channels, bias=False)\n",
    "\n",
    "        self.gate_mlp = Dense(out_channels, 1, activation='sigmoid')\n",
    "\n",
    "        self.skip_conn = nn.Linear(in_channels, out_channels)\n",
    "        self.activation = Activation(activation)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr: Optional[Tensor] = None):\n",
    "        \"\"\"\"\"\"\n",
    "        out, x_ = 0, x\n",
    "        for idx in range(self.kernel_size):\n",
    "            x_ = self.propagate(edge_index, idx=idx, x=x_, edge_attr=edge_attr)\n",
    "            out += x_\n",
    "        out = self.activation(out + self.skip_conn(x))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, idx, edge_attr: Optional[Tensor] = None):\n",
    "        mij = self.msg_mlps[idx](torch.cat([x_i, x_j], -1))\n",
    "        if edge_attr is not None:\n",
    "            if edge_attr.ndim == 1:  # accommodate for edge_weight\n",
    "                edge_attr = edge_attr.view(-1, 1)\n",
    "            mij = mij + self.lin_edge(edge_attr)\n",
    "        return self.gate_mlp(mij) * mij\n",
    "\n",
    "\n",
    "class GraphAnisoGRUCell(GraphGRUCellBase):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int,\n",
    "                 edge_dim: Optional[int] = None,\n",
    "                 activation: str = 'leaky_relu'):\n",
    "        self.input_size = input_size\n",
    "        # instantiate gates\n",
    "        forget_gate = GraphAnisoConv(input_size + hidden_size, hidden_size,\n",
    "                                     edge_dim=edge_dim, activation=activation)\n",
    "        update_gate = GraphAnisoConv(input_size + hidden_size, hidden_size,\n",
    "                                     edge_dim=edge_dim, activation=activation)\n",
    "        candidate_gate = GraphAnisoConv(input_size + hidden_size, hidden_size,\n",
    "                                        edge_dim=edge_dim,\n",
    "                                        activation=activation)\n",
    "        super(GraphAnisoGRUCell, self).__init__(hidden_size=hidden_size,\n",
    "                                                forget_gate=forget_gate,\n",
    "                                                update_gate=update_gate,\n",
    "                                                candidate_gate=candidate_gate)\n",
    "\n",
    "\n",
    "class GraphAnisoGRU(RNNBase):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int,\n",
    "                 edge_dim: Optional[int] = None,\n",
    "                 n_layers: int = 1, cat_states_layers: bool = False,\n",
    "                 return_only_last_state: bool = False,\n",
    "                 activation: str = 'leaky_relu'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        rnn_cells = [\n",
    "            GraphAnisoGRUCell(input_size if i == 0 else hidden_size,\n",
    "                              hidden_size, edge_dim=edge_dim,\n",
    "                              activation=activation)\n",
    "            for i in range(n_layers)\n",
    "        ]\n",
    "        super(GraphAnisoGRU, self).__init__(rnn_cells, cat_states_layers,\n",
    "                                            return_only_last_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/marshka/hdtts/blob/main/lib/nn/models/baselines/stgnns/prototypes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch_geometric.typing import Adj\n",
    "from tsl.nn.blocks import MLPDecoder\n",
    "from tsl.nn.layers import MultiLinear, NodeEmbedding\n",
    "from tsl.nn.models import BaseModel\n",
    "from tsl.nn.utils import maybe_cat_exog\n",
    "from tsl.utils import ensure_list\n",
    "\n",
    "\n",
    "def maybe_cat_emb(x: Tensor, emb: Optional[Tensor]):\n",
    "    if emb is None:\n",
    "        return x\n",
    "    if emb.ndim < x.ndim:\n",
    "        emb = emb[[None] * (x.ndim - emb.ndim)]\n",
    "    emb = emb.expand(*x.shape[:-1], -1)\n",
    "    return torch.cat([x, emb], dim=-1)\n",
    "\n",
    "\n",
    "class STGNN(BaseModel):\n",
    "    available_embedding_pos = {'encoding', 'decoding'}\n",
    "\n",
    "    def __init__(self, input_size: int, horizon: int,\n",
    "                 n_nodes: int = None,\n",
    "                 output_size: int = None,\n",
    "                 exog_size: int = 0,\n",
    "                 hidden_size: int = 32,\n",
    "                 emb_size: int = 0,\n",
    "                 add_embedding_before: Optional[\n",
    "                     Union[str, List[str]]] = 'encoding',\n",
    "                 use_local_weights: Union[str, List[str]] = None,\n",
    "                 activation: str = 'elu'):\n",
    "        super(STGNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        self.n_nodes = n_nodes\n",
    "        self.output_size = output_size or input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.exog_size = exog_size\n",
    "        self.activation = activation\n",
    "\n",
    "        # EMBEDDING\n",
    "        if add_embedding_before is None:\n",
    "            add_embedding_before = set()\n",
    "        else:\n",
    "            add_embedding_before = set(ensure_list(add_embedding_before))\n",
    "            if not add_embedding_before.issubset(self.available_embedding_pos):\n",
    "                raise ValueError(\"Parameter 'add_embedding_before' must be a \"\n",
    "                                 f\"subset of {self.available_embedding_pos}\")\n",
    "        self.add_embedding_before = add_embedding_before\n",
    "\n",
    "        if emb_size > 0:\n",
    "            self.emb = NodeEmbedding(n_nodes, emb_size)\n",
    "        else:\n",
    "            self.register_module('emb', None)\n",
    "\n",
    "        # ENCODER\n",
    "        self.encoder_input = input_size + exog_size\n",
    "        if 'encoding' in self.add_embedding_before and self.emb is not None:\n",
    "            self.encoder_input += emb_size\n",
    "\n",
    "        if use_local_weights is not None:\n",
    "            self.use_local_weights = set(ensure_list(use_local_weights))\n",
    "            if len(self.use_local_weights.difference(['encoder', 'decoder'])):\n",
    "                raise ValueError(\"Parameter 'use_local_weights' must be \"\n",
    "                                 \"'encoder', 'decoder', or both.\")\n",
    "        else:\n",
    "            self.use_local_weights = set()\n",
    "\n",
    "        if 'encoder' in self.use_local_weights:\n",
    "            self.encoder = MultiLinear(self.encoder_input, hidden_size, n_nodes)\n",
    "        else:\n",
    "            self.encoder = nn.Linear(self.encoder_input, hidden_size)\n",
    "\n",
    "        # DECODER\n",
    "        self.decoder_input = hidden_size\n",
    "        if 'decoding' in self.add_embedding_before and self.emb is not None:\n",
    "            self.decoder_input += emb_size\n",
    "        if 'decoder' in self.use_local_weights:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            self.decoder = MLPDecoder(input_size=self.decoder_input,\n",
    "                                      hidden_size=self.hidden_size,\n",
    "                                      output_size=self.output_size,\n",
    "                                      horizon=self.horizon,\n",
    "                                      activation=self.activation)\n",
    "\n",
    "    def stmp(self, x: Tensor, edge_index: Adj,\n",
    "             edge_weight: Optional[Tensor] = None,\n",
    "             emb: Optional[Tensor] = None) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: Optional[Tensor] = None,\n",
    "                u: Optional[Tensor] = None,\n",
    "                node_idx: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        # x: [batches steps nodes features]\n",
    "        x = maybe_cat_exog(x, u)\n",
    "        batch_size = x.size(0)\n",
    "        emb = self.emb(expand=(batch_size, -1, -1),\n",
    "                       node_index=node_idx) if self.emb is not None else None\n",
    "\n",
    "        if 'encoding' in self.add_embedding_before and emb is not None:\n",
    "            x = maybe_cat_emb(x, emb[:, None])\n",
    "\n",
    "        # ENCODER   ###########################################################\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # SPATIOTEMPORAL MESSAGE-PASSING   ####################################\n",
    "        out = self.stmp(x, edge_index, edge_weight, emb)\n",
    "\n",
    "        # DECODER   ###########################################################\n",
    "        if 'decoding' in self.add_embedding_before:\n",
    "            out = maybe_cat_emb(out, emb)\n",
    "\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TimeAndSpace(STGNN):\n",
    "\n",
    "    def __init__(self, input_size: int, horizon: int, stmp_conv: nn.Module,\n",
    "                 n_nodes: int = None,\n",
    "                 output_size: int = None,\n",
    "                 exog_size: int = 0,\n",
    "                 hidden_size: int = 32,\n",
    "                 emb_size: int = 0,\n",
    "                 add_embedding_before: Union[str, List[str]] = 'encoding',\n",
    "                 use_local_weights: Union[str, List[str]] = None,\n",
    "                 activation: str = 'elu'):\n",
    "        super(TimeAndSpace, self).__init__(input_size=input_size,\n",
    "                                           horizon=horizon,\n",
    "                                           n_nodes=n_nodes,\n",
    "                                           output_size=output_size,\n",
    "                                           exog_size=exog_size,\n",
    "                                           hidden_size=hidden_size,\n",
    "                                           emb_size=emb_size,\n",
    "                                           add_embedding_before=add_embedding_before,\n",
    "                                           use_local_weights=use_local_weights,\n",
    "                                           activation=activation)\n",
    "\n",
    "        # STMP\n",
    "        self.stmp_conv = stmp_conv\n",
    "\n",
    "    def stmp(self, x: Tensor, edge_index: Adj,\n",
    "             edge_weight: Optional[Tensor] = None,\n",
    "             emb: Optional[Tensor] = None) -> Tensor:\n",
    "        # spatiotemporal encoding\n",
    "        out = self.stmp_conv(x, edge_index, edge_weight)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/marshka/hdtts/blob/main/lib/nn/models/baselines/stgnns/time_and_graph_anisotropic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "class TimeAndGraphAnisoModel(TimeAndSpace):\n",
    "\n",
    "    def __init__(self, input_size: int, horizon: int, n_nodes: int = None,\n",
    "                 output_size: int = None,\n",
    "                 exog_size: int = 0,\n",
    "                 hidden_size: int = 32,\n",
    "                 emb_size: int = 0,\n",
    "                 add_embedding_before: Union[str, List[str]] = 'encoding',\n",
    "                 use_local_weights: Union[str, List[str]] = None,\n",
    "                 n_layers: int = 1,\n",
    "                 activation: str = 'elu'):\n",
    "        stmp_conv = GraphAnisoGRU(input_size=hidden_size,\n",
    "                                  hidden_size=hidden_size,\n",
    "                                  n_layers=n_layers,\n",
    "                                  activation=activation,\n",
    "                                  return_only_last_state=True)\n",
    "        super(TimeAndGraphAnisoModel, self).__init__(\n",
    "            input_size=input_size,\n",
    "            horizon=horizon,\n",
    "            stmp_conv=stmp_conv,\n",
    "            n_nodes=n_nodes,\n",
    "            output_size=output_size,\n",
    "            exog_size=exog_size,\n",
    "            hidden_size=hidden_size,\n",
    "            emb_size=emb_size,\n",
    "            add_embedding_before=add_embedding_before,\n",
    "            use_local_weights=use_local_weights,\n",
    "            activation=activation\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model \n",
    "\n",
    "Model is configured with hidden units, feed-forward layers, multiple SpatioTemporalConvNet blocks, and utilizes temporal and spatial convolution kernels, layer normalization, and gated mechanisms; it adapts to the dataset’s input size, number of nodes, horizon, and available exogenous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32          # Number of hidden units\n",
    "ff_size = 64             # Number of units in the feed-forward layers\n",
    "n_layers = 3              # Number of SpatioTemporalConvNet blocks\n",
    "temporal_kernel_size = 3  # Size of the temporal convolution kernel\n",
    "spatial_kernel_size = 3   # Order of the spatial diffusion process\n",
    "norm='layer'\n",
    "gated=True\n",
    "\n",
    "input_size = torch_dataset.n_channels\n",
    "n_nodes = torch_dataset.n_nodes\n",
    "horizon = torch_dataset.horizon\n",
    "exog_size = torch_dataset.input_map.u.shape[-1] if 'u' in torch_dataset else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeAndGraphAnisoModel(\n",
      "  (emb): None\n",
      "  (encoder): Linear(in_features=23, out_features=32, bias=True)\n",
      "  (decoder): MLPDecoder(\n",
      "    (readout): MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Dense(\n",
      "          (affinity): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (activation): ELU(alpha=1.0)\n",
      "          (dropout): Identity()\n",
      "        )\n",
      "      )\n",
      "      (readout): Linear(in_features=32, out_features=6, bias=True)\n",
      "    )\n",
      "    (rearrange): Rearrange('b n (h f) -> b h n f', f=1, h=6)\n",
      "  )\n",
      "  (stmp_conv): GraphAnisoGRU(cell=GraphAnisoGRUCell, return_only_last_state=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TimeAndGraphAnisoModel(\n",
    "    input_size=input_size,\n",
    "    horizon=horizon,\n",
    "    n_nodes=n_nodes,\n",
    "    output_size=input_size,\n",
    "    exog_size=exog_size,\n",
    "    hidden_size=hidden_size\n",
    ")\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(model):\n",
    "    tot = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "    out = f\"Number of model ({model.__class__.__name__}) parameters:{tot:10d}\"\n",
    "    print(\"=\" * len(out))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Number of model (TimeAndGraphAnisoModel) parameters:     24009\n"
     ]
    }
   ],
   "source": [
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training \n",
    "\n",
    "This setup initializes a Predictor for the model with a masked mean-squared error loss function and multiple evaluation metrics (MSE, MAE, MAPE, and specific MSE at selected timesteps), then configures a Trainer using PyTorch Lightning with early stopping and model checkpointing based on validation MSE, gradient clipping to prevent exploding gradients, and 16-bit precision for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.metrics.torch import MaskedMSE, MaskedMAE, MaskedMAPE\n",
    "from tsl.engines import Predictor\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = MaskedMSE()\n",
    "\n",
    "# Setup metrics\n",
    "metrics = {\n",
    "    'mse': MaskedMSE(),\n",
    "    'mae': MaskedMAE(),\n",
    "    'mape': MaskedMAPE(),\n",
    "    'mse_at_3': MaskedMSE(at=2),  # '2' indicates the third time step\n",
    "    'mse_at_6': MaskedMSE(at=5)\n",
    "}\n",
    "\n",
    "# Setup predictor\n",
    "predictor = Predictor(\n",
    "    model=model,\n",
    "    optim_class=torch.optim.Adam,\n",
    "    optim_kwargs={'lr': 0.001},\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lightning.ai/docs/pytorch/stable/common/trainer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_mse',\n",
    "    patience=30,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Logs \n",
    "dirpath = Path('ml-drought-forecasting/soil-water-forecasting/data/06_models/TimeAndGraphAniso/logs')\n",
    "\n",
    "dirpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=dirpath,\n",
    "    save_top_k=1,\n",
    "    monitor='val_mse', \n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(max_epochs=2,\n",
    "                    #  logger=logger,\n",
    "                    #  limit_train_batches=100,  # end an epoch after 200 updates\n",
    "                     callbacks=[early_stop_callback, checkpoint_callback],\n",
    "                     log_every_n_steps=2,\n",
    "                     gradient_clip_val=1.0,    # Prevent exploding gradients\n",
    "                     precision=16\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set float32 matmul precision to 'medium' or 'high'\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                   | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | loss_fn       | MaskedMSE              | 0      | train\n",
      "1 | train_metrics | MetricCollection       | 0      | train\n",
      "2 | val_metrics   | MetricCollection       | 0      | train\n",
      "3 | test_metrics  | MetricCollection       | 0      | train\n",
      "4 | model         | TimeAndGraphAnisoModel | 24.0 K | train\n",
      "-----------------------------------------------------------------\n",
      "24.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.0 K    Total params\n",
      "0.096     Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a59bd25beb4404aa74521415d53e943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5daada924bbc45c1bbcaf0bccbbe9280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bedabb5a7d04c4d907d9c8cd84a0642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6c474092d5430fa00f165d1af91cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "trainer.fit(predictor, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeAndGraphAnisoModel(\n",
    "    input_size=input_size,\n",
    "    horizon=horizon,\n",
    "    n_nodes=n_nodes,\n",
    "    output_size=input_size,\n",
    "    exog_size=exog_size,\n",
    "    hidden_size=hidden_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m      8\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m: MaskedMSE(),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m: MaskedMAE(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse_at_6\u001b[39m\u001b[38;5;124m'\u001b[39m: MaskedMSE(at\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Setup predictor\u001b[39;00m\n\u001b[1;32m     17\u001b[0m predictor \u001b[38;5;241m=\u001b[39m Predictor(\n\u001b[0;32m---> 18\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m,\n\u001b[1;32m     19\u001b[0m     optim_class\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam,\n\u001b[1;32m     20\u001b[0m     optim_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m},\n\u001b[1;32m     21\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     22\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mmetrics\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tsl.metrics.torch import MaskedMSE, MaskedMAE, MaskedMAPE\n",
    "from tsl.engines import Predictor\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = MaskedMSE()\n",
    "\n",
    "# Setup metrics\n",
    "metrics = {\n",
    "    'mse': MaskedMSE(),\n",
    "    'mae': MaskedMAE(),\n",
    "    'mape': MaskedMAPE(),\n",
    "    'mse_at_3': MaskedMSE(at=2),  # '2' indicates the third time step\n",
    "    'mse_at_6': MaskedMSE(at=5)\n",
    "}\n",
    "\n",
    "# Setup predictor\n",
    "predictor = Predictor(\n",
    "    model=model,\n",
    "    optim_class=torch.optim.Adam,\n",
    "    optim_kwargs={'lr': 0.001},\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/ml-drought-forecasting/soil-water-forecasting/data/06_models/TimeAndGraphAniso/logs/epoch=1-step=70.ckpt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the best checkpoint\n",
    "best_checkpoint_path = checkpoint_callback.best_model_path\n",
    "best_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tsl/engines/predictor.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  storage = torch.load(filename, lambda storage, loc: storage)\n",
      "Predictor with already instantiated model is loading a state_dict from /teamspace/studios/this_studio/ml-drought-forecasting/soil-water-forecasting/data/06_models/TimeAndGraphAniso/logs/epoch=1-step=70.ckpt. Cannot  check if model hyperparameters are the same.\n"
     ]
    }
   ],
   "source": [
    "predictor.load_model(best_checkpoint_path)\n",
    "predictor.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd1ac4e31da43dea8d12688eefc1461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0012702906969934702   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.018240731209516525    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mape         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      9354.306640625       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0012702909298241138   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_mse_at_3       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0012959683081135154   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_mse_at_6       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0013875252334401011   </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0012702906969934702  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.018240731209516525   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mape        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     9354.306640625      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0012702909298241138  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_mse_at_3      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0012959683081135154  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_mse_at_6      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0013875252334401011  \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_mae': 0.018240731209516525,\n",
       "  'test_mape': 9354.306640625,\n",
       "  'test_mse': 0.0012702909298241138,\n",
       "  'test_mse_at_3': 0.0012959683081135154,\n",
       "  'test_mse_at_6': 0.0013875252334401011,\n",
       "  'test_loss': 0.0012702906969934702}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(predictor, dataloaders=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd638c68d3f54b7ead4b45bfb74f3822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.001153454533778131    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.01783650927245617    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_mape          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     12957.0205078125      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0011534543009474874   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_mse_at_3        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0011509478790685534   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_mse_at_6        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.001245370483957231    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.001153454533778131   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.01783650927245617   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_mape         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    12957.0205078125     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0011534543009474874  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_mse_at_3       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0011509478790685534  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_mse_at_6       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.001245370483957231   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_mae': 0.01783650927245617,\n",
       "  'val_mape': 12957.0205078125,\n",
       "  'val_mse': 0.0011534543009474874,\n",
       "  'val_mse_at_3': 0.0011509478790685534,\n",
       "  'val_mse_at_6': 0.001245370483957231,\n",
       "  'val_loss': 0.001153454533778131}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(predictor, dataloaders=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6138cd52596c4e38a08c016bd376a78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_test = trainer.predict(predictor, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'y': tensor([[[[ 2.9829e-01],\n",
       "            [ 2.9829e-01],\n",
       "            [ 2.9829e-01],\n",
       "            ...,\n",
       "            [-5.7735e-06],\n",
       "            [-5.7735e-06],\n",
       "            [-5.7735e-06]],\n",
       "  \n",
       "           [[ 2.9800e-01],\n",
       "            [ 2.9800e-01],\n",
       "            [ 2.9800e-01],\n",
       "            ...,\n",
       "            [ 4.2701e-07],\n",
       "            [ 4.2701e-07],\n",
       "            [ 4.2701e-07]],\n",
       "  \n",
       "           [[ 3.1431e-01],\n",
       "            [ 3.1431e-01],\n",
       "            [ 3.1431e-01],\n",
       "            ...,\n",
       "            [-9.6592e-06],\n",
       "            [-9.6592e-06],\n",
       "            [-9.6592e-06]],\n",
       "  \n",
       "           [[ 3.1568e-01],\n",
       "            [ 3.1568e-01],\n",
       "            [ 3.1568e-01],\n",
       "            ...,\n",
       "            [-5.9584e-06],\n",
       "            [-5.9584e-06],\n",
       "            [-5.9584e-06]],\n",
       "  \n",
       "           [[ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            ...,\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06]],\n",
       "  \n",
       "           [[ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            ...,\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06]]]]),\n",
       "  'y_hat': tensor([[[[ 0.2305],\n",
       "            [ 0.2897],\n",
       "            [ 0.2891],\n",
       "            ...,\n",
       "            [-0.0046],\n",
       "            [-0.0046],\n",
       "            [ 0.0143]],\n",
       "  \n",
       "           [[ 0.1691],\n",
       "            [ 0.2831],\n",
       "            [ 0.2826],\n",
       "            ...,\n",
       "            [-0.0070],\n",
       "            [-0.0069],\n",
       "            [-0.0139]],\n",
       "  \n",
       "           [[ 0.1598],\n",
       "            [ 0.2994],\n",
       "            [ 0.2991],\n",
       "            ...,\n",
       "            [ 0.0059],\n",
       "            [ 0.0060],\n",
       "            [ 0.1026]],\n",
       "  \n",
       "           [[ 0.2903],\n",
       "            [ 0.2908],\n",
       "            [ 0.2904],\n",
       "            ...,\n",
       "            [-0.0056],\n",
       "            [-0.0056],\n",
       "            [ 0.0156]],\n",
       "  \n",
       "           [[ 0.2462],\n",
       "            [ 0.2755],\n",
       "            [ 0.2751],\n",
       "            ...,\n",
       "            [-0.0048],\n",
       "            [-0.0048],\n",
       "            [ 0.0132]],\n",
       "  \n",
       "           [[-0.0231],\n",
       "            [ 0.3150],\n",
       "            [ 0.3144],\n",
       "            ...,\n",
       "            [ 0.0026],\n",
       "            [ 0.0026],\n",
       "            [ 0.0539]]]]),\n",
       "  'mask': tensor([[[[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]]]])},\n",
       " {'y': tensor([[[[ 2.9800e-01],\n",
       "            [ 2.9800e-01],\n",
       "            [ 2.9800e-01],\n",
       "            ...,\n",
       "            [ 4.2701e-07],\n",
       "            [ 4.2701e-07],\n",
       "            [ 4.2701e-07]],\n",
       "  \n",
       "           [[ 3.1431e-01],\n",
       "            [ 3.1431e-01],\n",
       "            [ 3.1431e-01],\n",
       "            ...,\n",
       "            [-9.6592e-06],\n",
       "            [-9.6592e-06],\n",
       "            [-9.6592e-06]],\n",
       "  \n",
       "           [[ 3.1568e-01],\n",
       "            [ 3.1568e-01],\n",
       "            [ 3.1568e-01],\n",
       "            ...,\n",
       "            [-5.9584e-06],\n",
       "            [-5.9584e-06],\n",
       "            [-5.9584e-06]],\n",
       "  \n",
       "           [[ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            ...,\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06]],\n",
       "  \n",
       "           [[ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            ...,\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06]],\n",
       "  \n",
       "           [[ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            ...,\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05]]]]),\n",
       "  'y_hat': tensor([[[[ 0.2286],\n",
       "            [ 0.2921],\n",
       "            [ 0.2919],\n",
       "            ...,\n",
       "            [-0.0063],\n",
       "            [-0.0063],\n",
       "            [ 0.0101]],\n",
       "  \n",
       "           [[ 0.1694],\n",
       "            [ 0.2869],\n",
       "            [ 0.2867],\n",
       "            ...,\n",
       "            [-0.0091],\n",
       "            [-0.0091],\n",
       "            [-0.0147]],\n",
       "  \n",
       "           [[ 0.1597],\n",
       "            [ 0.3071],\n",
       "            [ 0.3071],\n",
       "            ...,\n",
       "            [ 0.0063],\n",
       "            [ 0.0063],\n",
       "            [ 0.1041]],\n",
       "  \n",
       "           [[ 0.2910],\n",
       "            [ 0.2947],\n",
       "            [ 0.2942],\n",
       "            ...,\n",
       "            [-0.0064],\n",
       "            [-0.0064],\n",
       "            [ 0.0115]],\n",
       "  \n",
       "           [[ 0.2449],\n",
       "            [ 0.2788],\n",
       "            [ 0.2788],\n",
       "            ...,\n",
       "            [-0.0072],\n",
       "            [-0.0072],\n",
       "            [ 0.0114]],\n",
       "  \n",
       "           [[-0.0263],\n",
       "            [ 0.3219],\n",
       "            [ 0.3221],\n",
       "            ...,\n",
       "            [ 0.0012],\n",
       "            [ 0.0014],\n",
       "            [ 0.0545]]]]),\n",
       "  'mask': tensor([[[[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]]]])},\n",
       " {'y': tensor([[[[ 3.1431e-01],\n",
       "            [ 3.1431e-01],\n",
       "            [ 3.1431e-01],\n",
       "            ...,\n",
       "            [-9.6592e-06],\n",
       "            [-9.6592e-06],\n",
       "            [-9.6592e-06]],\n",
       "  \n",
       "           [[ 3.1568e-01],\n",
       "            [ 3.1568e-01],\n",
       "            [ 3.1568e-01],\n",
       "            ...,\n",
       "            [-5.9584e-06],\n",
       "            [-5.9584e-06],\n",
       "            [-5.9584e-06]],\n",
       "  \n",
       "           [[ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            ...,\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06]],\n",
       "  \n",
       "           [[ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            ...,\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06]],\n",
       "  \n",
       "           [[ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            ...,\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05]],\n",
       "  \n",
       "           [[ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            ...,\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08]]]]),\n",
       "  'y_hat': tensor([[[[ 0.2891],\n",
       "            [ 0.3039],\n",
       "            [ 0.3039],\n",
       "            ...,\n",
       "            [-0.0078],\n",
       "            [-0.0078],\n",
       "            [ 0.0096]],\n",
       "  \n",
       "           [[ 0.1836],\n",
       "            [ 0.3043],\n",
       "            [ 0.3045],\n",
       "            ...,\n",
       "            [-0.0114],\n",
       "            [-0.0115],\n",
       "            [-0.0150]],\n",
       "  \n",
       "           [[ 0.1718],\n",
       "            [ 0.3236],\n",
       "            [ 0.3238],\n",
       "            ...,\n",
       "            [ 0.0051],\n",
       "            [ 0.0051],\n",
       "            [ 0.1046]],\n",
       "  \n",
       "           [[ 0.3285],\n",
       "            [ 0.3153],\n",
       "            [ 0.3157],\n",
       "            ...,\n",
       "            [-0.0072],\n",
       "            [-0.0072],\n",
       "            [ 0.0109]],\n",
       "  \n",
       "           [[ 0.2693],\n",
       "            [ 0.2921],\n",
       "            [ 0.2923],\n",
       "            ...,\n",
       "            [-0.0089],\n",
       "            [-0.0089],\n",
       "            [ 0.0109]],\n",
       "  \n",
       "           [[ 0.0301],\n",
       "            [ 0.3303],\n",
       "            [ 0.3303],\n",
       "            ...,\n",
       "            [-0.0012],\n",
       "            [-0.0013],\n",
       "            [ 0.0545]]]]),\n",
       "  'mask': tensor([[[[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]]]])},\n",
       " {'y': tensor([[[[ 3.1568e-01],\n",
       "            [ 3.1568e-01],\n",
       "            [ 3.1568e-01],\n",
       "            ...,\n",
       "            [-5.9584e-06],\n",
       "            [-5.9584e-06],\n",
       "            [-5.9584e-06]],\n",
       "  \n",
       "           [[ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            ...,\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06]],\n",
       "  \n",
       "           [[ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            ...,\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06]],\n",
       "  \n",
       "           [[ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            ...,\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05]],\n",
       "  \n",
       "           [[ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            ...,\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08]],\n",
       "  \n",
       "           [[ 3.1582e-01],\n",
       "            [ 3.1582e-01],\n",
       "            [ 3.1582e-01],\n",
       "            ...,\n",
       "            [-7.3153e-06],\n",
       "            [-7.3153e-06],\n",
       "            [-7.3153e-06]]]]),\n",
       "  'y_hat': tensor([[[[ 0.3065],\n",
       "            [ 0.3144],\n",
       "            [ 0.3146],\n",
       "            ...,\n",
       "            [-0.0079],\n",
       "            [-0.0079],\n",
       "            [ 0.0112]],\n",
       "  \n",
       "           [[ 0.1878],\n",
       "            [ 0.3150],\n",
       "            [ 0.3152],\n",
       "            ...,\n",
       "            [-0.0124],\n",
       "            [-0.0125],\n",
       "            [-0.0158]],\n",
       "  \n",
       "           [[ 0.1754],\n",
       "            [ 0.3326],\n",
       "            [ 0.3326],\n",
       "            ...,\n",
       "            [ 0.0037],\n",
       "            [ 0.0036],\n",
       "            [ 0.1049]],\n",
       "  \n",
       "           [[ 0.3395],\n",
       "            [ 0.3303],\n",
       "            [ 0.3305],\n",
       "            ...,\n",
       "            [-0.0082],\n",
       "            [-0.0083],\n",
       "            [ 0.0115]],\n",
       "  \n",
       "           [[ 0.2762],\n",
       "            [ 0.3006],\n",
       "            [ 0.3009],\n",
       "            ...,\n",
       "            [-0.0098],\n",
       "            [-0.0099],\n",
       "            [ 0.0108]],\n",
       "  \n",
       "           [[ 0.0473],\n",
       "            [ 0.3346],\n",
       "            [ 0.3346],\n",
       "            ...,\n",
       "            [-0.0021],\n",
       "            [-0.0021],\n",
       "            [ 0.0534]]]]),\n",
       "  'mask': tensor([[[[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]]]])},\n",
       " {'y': tensor([[[[ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            [ 3.1438e-01],\n",
       "            ...,\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06],\n",
       "            [ 1.1064e-06]],\n",
       "  \n",
       "           [[ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            ...,\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06]],\n",
       "  \n",
       "           [[ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            ...,\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05]],\n",
       "  \n",
       "           [[ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            ...,\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08]],\n",
       "  \n",
       "           [[ 3.1582e-01],\n",
       "            [ 3.1582e-01],\n",
       "            [ 3.1582e-01],\n",
       "            ...,\n",
       "            [-7.3153e-06],\n",
       "            [-7.3153e-06],\n",
       "            [-7.3153e-06]],\n",
       "  \n",
       "           [[ 3.2181e-01],\n",
       "            [ 3.2181e-01],\n",
       "            [ 3.2181e-01],\n",
       "            ...,\n",
       "            [ 4.5274e-06],\n",
       "            [ 4.5274e-06],\n",
       "            [ 4.5274e-06]]]]),\n",
       "  'y_hat': tensor([[[[ 3.0242e-01],\n",
       "            [ 3.1179e-01],\n",
       "            [ 3.1179e-01],\n",
       "            ...,\n",
       "            [-8.9727e-03],\n",
       "            [-8.9727e-03],\n",
       "            [ 2.8540e-02]],\n",
       "  \n",
       "           [[ 1.8688e-01],\n",
       "            [ 3.1591e-01],\n",
       "            [ 3.1610e-01],\n",
       "            ...,\n",
       "            [-1.5480e-02],\n",
       "            [-1.5492e-02],\n",
       "            [-1.7178e-02]],\n",
       "  \n",
       "           [[ 1.7452e-01],\n",
       "            [ 3.3426e-01],\n",
       "            [ 3.3426e-01],\n",
       "            ...,\n",
       "            [-2.4925e-04],\n",
       "            [-2.2072e-04],\n",
       "            [ 1.0457e-01]],\n",
       "  \n",
       "           [[ 3.3707e-01],\n",
       "            [ 3.2939e-01],\n",
       "            [ 3.2977e-01],\n",
       "            ...,\n",
       "            [-1.0272e-02],\n",
       "            [-1.0307e-02],\n",
       "            [ 2.5988e-02]],\n",
       "  \n",
       "           [[ 2.7433e-01],\n",
       "            [ 2.9905e-01],\n",
       "            [ 2.9905e-01],\n",
       "            ...,\n",
       "            [-1.2092e-02],\n",
       "            [-1.2150e-02],\n",
       "            [ 1.3207e-02]],\n",
       "  \n",
       "           [[ 4.2796e-02],\n",
       "            [ 3.3445e-01],\n",
       "            [ 3.3426e-01],\n",
       "            ...,\n",
       "            [-3.1483e-03],\n",
       "            [-3.2156e-03],\n",
       "            [ 4.6565e-02]]]]),\n",
       "  'mask': tensor([[[[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]]]])},\n",
       " {'y': tensor([[[[ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            [ 3.1875e-01],\n",
       "            ...,\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06],\n",
       "            [-9.4743e-06]],\n",
       "  \n",
       "           [[ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            ...,\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05]],\n",
       "  \n",
       "           [[ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            ...,\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08]],\n",
       "  \n",
       "           [[ 3.1582e-01],\n",
       "            [ 3.1582e-01],\n",
       "            [ 3.1582e-01],\n",
       "            ...,\n",
       "            [-7.3153e-06],\n",
       "            [-7.3153e-06],\n",
       "            [-7.3153e-06]],\n",
       "  \n",
       "           [[ 3.2181e-01],\n",
       "            [ 3.2181e-01],\n",
       "            [ 3.2181e-01],\n",
       "            ...,\n",
       "            [ 4.5274e-06],\n",
       "            [ 4.5274e-06],\n",
       "            [ 4.5274e-06]],\n",
       "  \n",
       "           [[ 3.2686e-01],\n",
       "            [ 3.2686e-01],\n",
       "            [ 3.2686e-01],\n",
       "            ...,\n",
       "            [-3.5299e-06],\n",
       "            [-3.5299e-06],\n",
       "            [-3.5299e-06]]]]),\n",
       "  'y_hat': tensor([[[[ 0.2320],\n",
       "            [ 0.3142],\n",
       "            [ 0.3135],\n",
       "            ...,\n",
       "            [-0.0078],\n",
       "            [-0.0078],\n",
       "            [ 0.0238]],\n",
       "  \n",
       "           [[ 0.1701],\n",
       "            [ 0.3109],\n",
       "            [ 0.3099],\n",
       "            ...,\n",
       "            [-0.0150],\n",
       "            [-0.0150],\n",
       "            [-0.0109]],\n",
       "  \n",
       "           [[ 0.1604],\n",
       "            [ 0.3339],\n",
       "            [ 0.3331],\n",
       "            ...,\n",
       "            [-0.0008],\n",
       "            [-0.0008],\n",
       "            [ 0.0984]],\n",
       "  \n",
       "           [[ 0.2929],\n",
       "            [ 0.3249],\n",
       "            [ 0.3238],\n",
       "            ...,\n",
       "            [-0.0093],\n",
       "            [-0.0093],\n",
       "            [ 0.0269]],\n",
       "  \n",
       "           [[ 0.2462],\n",
       "            [ 0.2970],\n",
       "            [ 0.2964],\n",
       "            ...,\n",
       "            [-0.0111],\n",
       "            [-0.0111],\n",
       "            [ 0.0180]],\n",
       "  \n",
       "           [[-0.0231],\n",
       "            [ 0.3399],\n",
       "            [ 0.3397],\n",
       "            ...,\n",
       "            [-0.0035],\n",
       "            [-0.0035],\n",
       "            [ 0.0526]]]]),\n",
       "  'mask': tensor([[[[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]]]])},\n",
       " {'y': tensor([[[[ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            [ 3.2019e-01],\n",
       "            ...,\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05],\n",
       "            [ 1.0451e-05]],\n",
       "  \n",
       "           [[ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            [ 3.2091e-01],\n",
       "            ...,\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08],\n",
       "            [-9.4995e-08]],\n",
       "  \n",
       "           [[ 3.1582e-01],\n",
       "            [ 3.1582e-01],\n",
       "            [ 3.1582e-01],\n",
       "            ...,\n",
       "            [-7.3153e-06],\n",
       "            [-7.3153e-06],\n",
       "            [-7.3153e-06]],\n",
       "  \n",
       "           [[ 3.2181e-01],\n",
       "            [ 3.2181e-01],\n",
       "            [ 3.2181e-01],\n",
       "            ...,\n",
       "            [ 4.5274e-06],\n",
       "            [ 4.5274e-06],\n",
       "            [ 4.5274e-06]],\n",
       "  \n",
       "           [[ 3.2686e-01],\n",
       "            [ 3.2686e-01],\n",
       "            [ 3.2686e-01],\n",
       "            ...,\n",
       "            [-3.5299e-06],\n",
       "            [-3.5299e-06],\n",
       "            [-3.5299e-06]],\n",
       "  \n",
       "           [[ 3.2873e-01],\n",
       "            [ 3.2873e-01],\n",
       "            [ 3.2873e-01],\n",
       "            ...,\n",
       "            [-5.4173e-06],\n",
       "            [-5.4173e-06],\n",
       "            [-5.4173e-06]]]]),\n",
       "  'y_hat': tensor([[[[ 0.2367],\n",
       "            [ 0.3204],\n",
       "            [ 0.3197],\n",
       "            ...,\n",
       "            [-0.0074],\n",
       "            [-0.0074],\n",
       "            [ 0.0243]],\n",
       "  \n",
       "           [[ 0.1712],\n",
       "            [ 0.3176],\n",
       "            [ 0.3168],\n",
       "            ...,\n",
       "            [-0.0125],\n",
       "            [-0.0125],\n",
       "            [-0.0097]],\n",
       "  \n",
       "           [[ 0.1613],\n",
       "            [ 0.3376],\n",
       "            [ 0.3373],\n",
       "            ...,\n",
       "            [-0.0010],\n",
       "            [-0.0010],\n",
       "            [ 0.0971]],\n",
       "  \n",
       "           [[ 0.2961],\n",
       "            [ 0.3303],\n",
       "            [ 0.3292],\n",
       "            ...,\n",
       "            [-0.0094],\n",
       "            [-0.0094],\n",
       "            [ 0.0282]],\n",
       "  \n",
       "           [[ 0.2481],\n",
       "            [ 0.3037],\n",
       "            [ 0.3032],\n",
       "            ...,\n",
       "            [-0.0083],\n",
       "            [-0.0084],\n",
       "            [ 0.0192]],\n",
       "  \n",
       "           [[-0.0189],\n",
       "            [ 0.3447],\n",
       "            [ 0.3447],\n",
       "            ...,\n",
       "            [-0.0023],\n",
       "            [-0.0023],\n",
       "            [ 0.0535]]]]),\n",
       "  'mask': tensor([[[[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]],\n",
       "  \n",
       "           [[True],\n",
       "            [True],\n",
       "            [True],\n",
       "            ...,\n",
       "            [True],\n",
       "            [True],\n",
       "            [True]]]])}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34]),\n",
       " 'val': array([47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " 'test': array([72, 73, 74, 75, 76, 77, 78])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Load the dataset\n",
    "ds = xr.open_dataset(\n",
    "    'ml-drought-forecasting/soil-water-forecasting/data/04_feature/features.nc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(ds['date'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "# 1. Load Metadata and Dates\n",
    "metadata_array = np.load('ml-drought-forecasting/soil-water-forecasting/data/05_model_input/metadata.npy')  # Shape: (21960, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes = metadata_array[:, 0]\n",
    "longitudes = metadata_array[:, 1]\n",
    "num_nodes = len(latitudes)\n",
    "total_time_steps = len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correctly Retrieve the Split Indices\n",
    "split_indices = datamodule.splitter.indices\n",
    "train_indices = split_indices['train']\n",
    "val_indices = split_indices['val']\n",
    "test_indices = split_indices['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Predictions and Actual Values\n",
    "y_list = []\n",
    "y_hat_list = []\n",
    "mask_list = []\n",
    "\n",
    "for batch in predictions_test:\n",
    "    y_tensor = batch['y']\n",
    "    y_hat_tensor = batch['y_hat']\n",
    "    mask_tensor = batch['mask']\n",
    "    \n",
    "    y_np = y_tensor.cpu().numpy().squeeze(-1)      # Shape: (batch_size, steps, nodes)\n",
    "    y_hat_np = y_hat_tensor.cpu().numpy().squeeze(-1)\n",
    "    mask_np = mask_tensor.cpu().numpy().squeeze(-1)\n",
    "    \n",
    "    y_list.append(y_np)\n",
    "    y_hat_list.append(y_hat_np)\n",
    "    mask_list.append(mask_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate along the batch dimension\n",
    "y = np.concatenate(y_list, axis=0)         # Shape: (num_test_batches, steps, nodes)\n",
    "y_hat = np.concatenate(y_hat_list, axis=0) # Shape: (num_test_batches, steps, nodes)\n",
    "mask = np.concatenate(mask_list, axis=0)   # Shape: (num_test_batches, steps, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Node IDs to Geographical Coordinates\n",
    "num_steps = y.shape[1]          # Number of prediction steps (e.g., 6)\n",
    "num_test_dates = y.shape[0]     # Number of test dates (e.g., 25)\n",
    "\n",
    "# Create node_ids, lat, lon repeated for each step\n",
    "node_ids = np.tile(np.arange(num_nodes), num_test_dates * num_steps)  # Shape: (3,294,000,)\n",
    "lat_column = np.tile(latitudes, num_test_dates * num_steps)\n",
    "lon_column = np.tile(longitudes, num_test_dates * num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align Predictions with Dates\n",
    "\n",
    "# Repeat each test date for the number of prediction steps\n",
    "test_dates_steps = np.repeat(dates[test_indices], num_steps)  # Shape: (25 * 6,) => (150,)\n",
    "\n",
    "# Now, repeat each date for all nodes to align with the predictions\n",
    "all_dates_expanded = np.repeat(test_dates_steps, num_nodes)   # Shape: (150 * 21960,) => (3,294,000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Flatten the Predictions and Mask\n",
    "y_flat = y.flatten()         # Shape: (3,294,000,)\n",
    "y_hat_flat = y_hat.flatten()\n",
    "mask_flat = mask.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Apply Mask to Filter Valid Entries\n",
    "valid_indices = mask_flat.astype(bool)  # Ensure mask is boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dates = all_dates_expanded[valid_indices]  # Shape: (num_valid_entries,)\n",
    "filtered_lat = lat_column[valid_indices]\n",
    "filtered_lon = lon_column[valid_indices]\n",
    "filtered_y = y_flat[valid_indices]\n",
    "filtered_y_hat = y_hat_flat[valid_indices]\n",
    "filtered_node_ids = node_ids[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': filtered_dates,\n",
    "    'lat': filtered_lat,\n",
    "    'lon': filtered_lon,\n",
    "    'node_id': filtered_node_ids,\n",
    "    'y': filtered_y,\n",
    "    'y_hat': filtered_y_hat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>node_id</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298288</td>\n",
       "      <td>0.230512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.298288</td>\n",
       "      <td>0.289690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.298288</td>\n",
       "      <td>0.289128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.298288</td>\n",
       "      <td>0.289128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.298288</td>\n",
       "      <td>0.289128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736715</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>90.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>65155</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.002268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736716</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>90.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>65156</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.002290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736717</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>90.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>65157</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.002290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736718</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>90.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>65158</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.002266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736719</th>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>90.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>65159</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.053494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2736720 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date   lat    lon  node_id         y     y_hat\n",
       "0       2022-01-01 -90.0    0.0        0  0.298288  0.230512\n",
       "1       2022-01-01 -90.0    1.0        1  0.298288  0.289690\n",
       "2       2022-01-01 -90.0    2.0        2  0.298288  0.289128\n",
       "3       2022-01-01 -90.0    3.0        3  0.298288  0.289128\n",
       "4       2022-01-01 -90.0    4.0        4  0.298288  0.289128\n",
       "...            ...   ...    ...      ...       ...       ...\n",
       "2736715 2022-07-01  90.0  355.0    65155 -0.000005 -0.002268\n",
       "2736716 2022-07-01  90.0  356.0    65156 -0.000005 -0.002290\n",
       "2736717 2022-07-01  90.0  357.0    65157 -0.000005 -0.002290\n",
       "2736718 2022-07-01  90.0  358.0    65158 -0.000005 -0.002266\n",
       "2736719 2022-07-01  90.0  359.0    65159 -0.000005  0.053494\n",
       "\n",
       "[2736720 rows x 6 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('ml-drought-forecasting/ml-modeling-pipeline/data/07_model_output/predictions.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_variable_on_map(\n",
    "    dataset: pd.DataFrame,\n",
    "    variable: str,\n",
    "    lat_dim: str = 'latitude',\n",
    "    lon_dim: str = 'longitude',\n",
    "    projection: str = 'natural earth',\n",
    "    color_scale: str = 'Viridis',\n",
    "    title: Optional[str] = None,\n",
    "    animation_frame: Optional[str] = None,\n",
    "    hover_precision: int = 2,\n",
    ") -> go.Figure:\n",
    "\n",
    "    df = dataset\n",
    "\n",
    "    # Drop NaNs\n",
    "    df = df.dropna(subset=[variable])\n",
    "\n",
    "    # Handle animation frame\n",
    "    if animation_frame:\n",
    "        df[animation_frame] = df[animation_frame].astype(str)\n",
    "    else:\n",
    "        df['Frame'] = 'Frame'\n",
    "    \n",
    "    # Create scatter_geo plot\n",
    "    fig = px.scatter_geo(\n",
    "        df,\n",
    "        lat=lat_dim,\n",
    "        lon=lon_dim,\n",
    "        color=variable,\n",
    "        animation_frame=animation_frame if animation_frame else 'Frame',\n",
    "        projection=projection,\n",
    "        color_continuous_scale=color_scale,\n",
    "        title=title,\n",
    "        labels={variable: variable.upper()},\n",
    "        hover_data={variable: f':.{hover_precision}f'},\n",
    "    )\n",
    "    \n",
    "    # Add Play/Pause buttons\n",
    "    fig.update_layout(\n",
    "        updatemenus=[dict(\n",
    "            type='buttons',\n",
    "            buttons=[\n",
    "                dict(label='Play',\n",
    "                     method='animate',\n",
    "                     args=[None, {\"frame\": {\"duration\": 500, \"redraw\": True}, \"fromcurrent\": True}]),\n",
    "                dict(label='Pause',\n",
    "                     method='animate',\n",
    "                     args=[[None], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\"}])\n",
    "            ],\n",
    "            showactive=False,\n",
    "            x=0.1,\n",
    "            y=0\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "fig = visualize_variable_on_map(\n",
    "    dataset=df,\n",
    "    variable='y',\n",
    "    lat_dim='lat',\n",
    "    lon_dim='lon',\n",
    "    animation_frame='date',\n",
    "    title='Volumetric soil water layer 1',\n",
    "    color_scale = [\n",
    "        [0.0, \"darkred\"],\n",
    "        [0.1, \"red\"],\n",
    "        [0.2, \"orangered\"],\n",
    "        [0.3, \"lightgreen\"],\n",
    "        [0.4, \"limegreen\"],\n",
    "        [0.5, \"green\"],\n",
    "        [0.55, \"darkseagreen\"],\n",
    "        [0.6, \"darkgreen\"],\n",
    "        [0.7, \"lightblue\"],\n",
    "        [0.8, \"skyblue\"],\n",
    "        [0.9, \"deepskyblue\"],\n",
    "        [1.0, \"blue\"]\n",
    "    ]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def visualize_predictions_plotly(predictions, metadata, idx, horizon=None, \n",
    "                                 batches_to_visualize=None, variables_to_visualize=None):\n",
    "    \"\"\"\n",
    "    Visualize predicted vs actual values using Plotly for specified batches and variables with variable-specific normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions (dict): Contains 'y' and 'y_hat' tensors.\n",
    "    - metadata (pd.DataFrame): DataFrame with 'lat' and 'lon' columns.\n",
    "    - idx (int): Index of the item being visualized.\n",
    "    - horizon (int, optional): Number of time steps to visualize. Defaults to the maximum horizon in data.\n",
    "    - batches_to_visualize (list or range, optional): List of batch indices to visualize. Defaults to all batches.\n",
    "    - variables_to_visualize (list or range, optional): List of variable indices to visualize. Defaults to all variables.\n",
    "    \"\"\"\n",
    "    # Extract predictions and actual values\n",
    "    y_hat = predictions['y_hat'].squeeze().numpy()  # Shape: [batch, horizon, spatial_dim, variables]\n",
    "    y = predictions['y'].squeeze().numpy()          # Shape: [batch, horizon, spatial_dim, variables]\n",
    "\n",
    "    # Ensure y_hat and y have the same shape\n",
    "    assert y_hat.shape == y.shape, \"Predictions and actual values must have the same shape\"\n",
    "\n",
    "    # Determine the horizon if not provided\n",
    "    if horizon is None:\n",
    "        horizon = y_hat.shape[1]\n",
    "\n",
    "    # Determine the number of variables\n",
    "    if y_hat.ndim == 3:\n",
    "        # Shape: [batch, horizon, spatial_dim]\n",
    "        num_variables = 1\n",
    "        y_hat = y_hat[..., np.newaxis]  # Add a variables dimension\n",
    "        y = y[..., np.newaxis]\n",
    "    else:\n",
    "        num_variables = y_hat.shape[-1]\n",
    "\n",
    "    if variables_to_visualize is None:\n",
    "        variables_to_visualize = list(range(num_variables))\n",
    "    else:\n",
    "        # Ensure the variable indices are within the correct range\n",
    "        variables_to_visualize = [v for v in variables_to_visualize if v < num_variables]\n",
    "        if not variables_to_visualize:\n",
    "            raise ValueError(\"No valid variables to visualize. Check 'variables_to_visualize' indices.\")\n",
    "\n",
    "    # Get latitude and longitude from metadata\n",
    "    lats = metadata['lat'].values\n",
    "    lons = metadata['lon'].values\n",
    "\n",
    "    # Ensure that the number of spatial points matches the number of lats and lons\n",
    "    spatial_dim = y_hat.shape[2]\n",
    "    if spatial_dim != len(lats):\n",
    "        raise ValueError(f\"The number of spatial points in predictions ({spatial_dim}) does not match the number of locations in metadata ({len(lats)}).\")\n",
    "\n",
    "    # Calculate the min and max for lats and lons for setting map extent\n",
    "    lat_min, lat_max = lats.min(), lats.max()\n",
    "    lon_min, lon_max = lons.min(), lons.max()\n",
    "\n",
    "    # If batches_to_visualize is None, visualize all batches\n",
    "    if batches_to_visualize is None:\n",
    "        batches_to_visualize = range(y_hat.shape[0])\n",
    "    else:\n",
    "        # Ensure the batch indices are within the correct range\n",
    "        batches_to_visualize = [b for b in batches_to_visualize if b < y_hat.shape[0]]\n",
    "        if not batches_to_visualize:\n",
    "            raise ValueError(\"No valid batches to visualize. Check 'batches_to_visualize' indices.\")\n",
    "\n",
    "    # Precompute min and max for each variable in variables_to_visualize\n",
    "    var_min_max = {}\n",
    "    for var in variables_to_visualize:\n",
    "        var_data_y = y[..., var]  # Shape: [batch, horizon, spatial_dim]\n",
    "        var_data_y_hat = y_hat[..., var]  # Shape: [batch, horizon, spatial_dim]\n",
    "        min_val = min(np.min(var_data_y), np.min(var_data_y_hat))\n",
    "        max_val = max(np.max(var_data_y), np.max(var_data_y_hat))\n",
    "        import numpy as np\n",
    "\n",
    "        # ...\n",
    "\n",
    "        # Loop through each batch and variable\n",
    "        for batch in batches_to_visualize:\n",
    "            for var in variables_to_visualize:\n",
    "                frames = []\n",
    "                slider_steps = []\n",
    "\n",
    "                # Retrieve normalization for the current variable\n",
    "                min_val, max_val = var_min_max[var]\n",
    "\n",
    "                # Get the highest and lowest values of the variable\n",
    "                var_min = np.min(y_hat[batch, :, :, var])\n",
    "                var_max = np.max(y_hat[batch, :, :, var])\n",
    "\n",
    "                # Update the min and max values if necessary\n",
    "                if var_min < min_val:\n",
    "                    min_val = var_min\n",
    "                if var_max > max_val:\n",
    "                    max_val = var_max\n",
    "\n",
    "                # Update the normalization dictionary\n",
    "                var_min_max[var] = (min_val, max_val)\n",
    "\n",
    "                # ...\n",
    "\n",
    "                # Predicted data for current month\n",
    "                pred_data = go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=y_hat[batch, time_step, :, var],\n",
    "                        colorscale='RdYlBu_r',\n",
    "                        cmin=min_val,\n",
    "                        cmax=max_val,\n",
    "                        colorbar=dict(title='Predicted'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name='Predicted',\n",
    "                    showlegend=False\n",
    "                )\n",
    "\n",
    "                # Actual data for current month\n",
    "                actual_data = go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=y[batch, time_step, :, var],\n",
    "                        colorscale='RdYlBu_r',\n",
    "                        cmin=min_val,\n",
    "                        cmax=max_val,\n",
    "                        colorbar=dict(title='Actual'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name='Actual',\n",
    "                    showlegend=False\n",
    "                )\n",
    "\n",
    "                # ...\n",
    "\n",
    "                # Initial data (first frame)\n",
    "                pred_data_initial = go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=y_hat[batch, 0, :, var],\n",
    "                        colorscale='RdYlBu_r',\n",
    "                        cmin=min_val,\n",
    "                        cmax=max_val,\n",
    "                        colorbar=dict(title='Predicted'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name='Predicted',\n",
    "                    showlegend=True\n",
    "                )\n",
    "\n",
    "                actual_data_initial = go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=y[batch, 0, :, var],\n",
    "                        colorscale='RdYlBu_r',\n",
    "                        cmin=min_val,\n",
    "                        cmax=max_val,\n",
    "                        colorbar=dict(title='Actual'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name='Actual',\n",
    "                    showlegend=True\n",
    "                )\n",
    "\n",
    "                # ...\n",
    "                        visible=True,\n",
    "                        prefix=\"Date: \",\n",
    "                        xanchor=\"right\",\n",
    "                        font=dict(size=14, color=\"#666\")\n",
    "                    ),\n",
    "                )],\n",
    "                geo=dict(\n",
    "                    scope='world',\n",
    "                    projection_type='natural earth',\n",
    "                    showland=True,\n",
    "                    landcolor='lightgray',\n",
    "                    showcountries=True,\n",
    "                    countrycolor='black',\n",
    "                    lataxis=dict(range=[lat_min - 1, lat_max + 1]),\n",
    "                    lonaxis=dict(range=[lon_min - 1, lon_max + 1]),\n",
    "                ),\n",
    "                geo2=dict(\n",
    "                    scope='world',\n",
    "                    projection_type='natural earth',\n",
    "                    showland=True,\n",
    "                    landcolor='lightgray',\n",
    "                    showcountries=True,\n",
    "                    countrycolor='black',\n",
    "                    lataxis=dict(range=[lat_min - 1, lat_max + 1]),\n",
    "                    lonaxis=dict(range=[lon_min - 1, lon_max + 1]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Update frames to assign traces to correct subplots\n",
    "            for frame in fig.frames:\n",
    "                time_step_idx = int(frame.name.split()[1]) - 1\n",
    "                frame.data = [\n",
    "                    go.Scattergeo(\n",
    "                        lon=lons,\n",
    "                        lat=lats,\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=6,\n",
    "                            color=y_hat[batch, time_step_idx, :, var],\n",
    "                            colorscale='RdYlBu_r',\n",
    "                            cmin=min_val,\n",
    "                            cmax=max_val,\n",
    "                            opacity=0.8\n",
    "                        ),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    go.Scattergeo(\n",
    "                        lon=lons,\n",
    "                        lat=lats,\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=6,\n",
    "                            color=y[batch, time_step_idx, :, var],\n",
    "                            colorscale='RdYlBu_r',\n",
    "                            cmin=min_val,\n",
    "                            cmax=max_val,\n",
    "                            opacity=0.8\n",
    "                        ),\n",
    "                        showlegend=False\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            # Update layout for better appearance\n",
    "            fig.update_layout(\n",
    "                height=600,\n",
    "                width=1200,\n",
    "                margin=dict(l=50, r=50, t=100, b=50)\n",
    "            )\n",
    "\n",
    "            # Display the figure\n",
    "            fig.show()\n",
    "\n",
    "            # Optionally, save the figure to an HTML file\n",
    "            # fig.write_html(f'item_{idx+1}_batch_{batch+1}_variable_{var+1}.html')\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "# Assuming you have the metadata DataFrame available\n",
    "# metadata = pd.read_parquet('ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/metadata.parquet')\n",
    "\n",
    "# Ensure that predictions_test is a list of dictionaries with 'y' and 'y_hat'\n",
    "# For example:\n",
    "# predictions_test = [{'y': tensor1, 'y_hat': tensor2}, {'y': tensor3, 'y_hat': tensor4}, ...]\n",
    "\n",
    "# # Specify the indices of the items you want to visualize\n",
    "# items_to_visualize = [1]  # Replace with desired item indices (0-based indexing)\n",
    "# batches_to_visualize = [0, 1]  # Replace with desired batch indices within each item\n",
    "# variables_to_visualize = [0]  # Replace with desired variable indices (e.g., if multiple EDDI metrics)\n",
    "\n",
    "# # Call the function for specified items in predictions_test\n",
    "# for idx in items_to_visualize:\n",
    "#     pred = predictions_test[idx]\n",
    "#     visualize_predictions_plotly(\n",
    "#         pred, metadata, idx, horizon=pred['y'].shape[1],\n",
    "#         batches_to_visualize=batches_to_visualize, \n",
    "#         variables_to_visualize=variables_to_visualize\n",
    "#     )\n",
    "#     print(f\"Processed item {idx + 1}/{len(predictions_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
