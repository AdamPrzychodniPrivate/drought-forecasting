{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "from rasterio.transform import Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_file_links(year_url: str, file_type: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Fetches file links from a given URL that match a specific file type and filter by additional criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    - year_url: The base URL to search for file links.\n",
    "    - file_type: The file extension to filter links by.\n",
    "    \n",
    "    Returns:\n",
    "    A list of URLs that match the specified criteria.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    try:\n",
    "        response = session.get(year_url)\n",
    "        response.raise_for_status()  # Raises a HTTPError if the status is 4xx, 5xx\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        file_links = [f\"{year_url}{link['href']}\" for link in links if link['href'].endswith('.asc')]\n",
    "        filtered_links = [link for link in file_links if file_type in link]\n",
    "        return filtered_links\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Request error occurred: {err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, destination_folder):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(destination_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File downloaded successfully: {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download the file: {filename}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_and_process_files(source_dir, transform, row_start, row_end, col_start, col_end):\n",
    "    master_df = pl.DataFrame()\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.asc'):\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "            with rasterio.open(file_path) as src:\n",
    "                data = src.read(1)  # Read the first band\n",
    "                filtered_data = data[row_start:row_end, col_start:col_end]\n",
    "            df = create_polars_dataframe(file_path, filtered_data, row_start, row_end, col_start, col_end, transform)\n",
    "            master_df = master_df.vstack(df) if not master_df.is_empty() else df\n",
    "    return master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polars_dataframe(file_name: str, data: np.ndarray, row_start: int, row_end: int, col_start: int, col_end: int, transform: Affine):\n",
    "    \"\"\"\n",
    "    Creates a Polars DataFrame from filtered raster data with columns for time, latitude, longitude, and EDDI values.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name (str): The name of the file, used to extract the time information.\n",
    "    - data (numpy.ndarray): The filtered raster data array.\n",
    "    - row_start (int): The starting index for rows of the filtered data.\n",
    "    - row_end (int): The ending index for rows of the filtered data.\n",
    "    - col_start (int): The starting index for columns of the filtered data.\n",
    "    - col_end (int): The ending index for columns of the filtered data.\n",
    "    - transform (Affine): The affine transformation used to convert indices to geographic coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: A Polars DataFrame with columns for time, latitude, longitude, and EDDI values.\n",
    "    \"\"\"\n",
    "    # Extract the date from the file name using regular expressions\n",
    "    date_match = re.search(r'\\d{8}', file_name)\n",
    "    if date_match:\n",
    "        date = date_match.group(0)\n",
    "        # Transform the date to 'yyyy-mm-dd' format\n",
    "        formatted_date = f\"{date[:4]}-{date[4:6]}-{date[6:]}\"\n",
    "    else:\n",
    "        raise ValueError(\"Date not found in file name.\")\n",
    "\n",
    "    # Generate latitude and longitude arrays\n",
    "    lon_arr = []\n",
    "    lat_arr = []\n",
    "    for row in range(row_start, row_end):\n",
    "        for col in range(col_start, col_end):\n",
    "            lon, lat = transform * (col, row)\n",
    "            lon_arr.append(lon)\n",
    "            lat_arr.append(lat)\n",
    "\n",
    "    # Flatten the data array to match the latitude and longitude arrays\n",
    "    eddi_values = data.flatten()\n",
    "\n",
    "    # Create a Polars DataFrame\n",
    "    df = pl.DataFrame({\n",
    "        \"time\": [formatted_date] * len(eddi_values),\n",
    "        \"lat\": lat_arr,\n",
    "        \"lon\": lon_arr,\n",
    "        \"eddi\": eddi_values\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geographic_to_index(lon, lat, transform):\n",
    "    \"\"\"\n",
    "    Convert geographic coordinates to raster indices.\n",
    "\n",
    "    Parameters:\n",
    "    - lon (float): The longitude to convert.\n",
    "    - lat (float): The latitude to convert.\n",
    "    - transform (Affine): The affine transform associated with the raster, defining how geographic coordinates are transformed into raster indices.\n",
    "\n",
    "    Returns:\n",
    "    - (int, int): The zero-based column index and row index corresponding to the given longitude and latitude.\n",
    "    \"\"\"\n",
    "    col, row = ~transform * (lon, lat)\n",
    "    return int(col), int(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for dowloading data \n",
    "def download_and_process_raster_data(base_url, start_year, end_year, file_type, destination_folder):\n",
    "    # Define the affine transform for a raster where each pixel represents a 0.5x0.5 degree area\n",
    "    transform = Affine(0.5, 0.0, -180, 0.0, -0.5, 90)\n",
    "\n",
    "    # Define geographic bounds and calculate index ranges\n",
    "    lat_range = (-12, 22)  # South to North\n",
    "    lon_range = (23, 52)  # West to East\n",
    "    col_start, row_end = geographic_to_index(lon_range[0], lat_range[0], transform)\n",
    "    col_end, row_start = geographic_to_index(lon_range[1], lat_range[1], transform)\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        year_url = f\"{base_url}{year}/\"\n",
    "        year_destination_folder = os.path.join(destination_folder, str(year))\n",
    "        \n",
    "        if not os.path.exists(year_destination_folder):\n",
    "            os.makedirs(year_destination_folder)\n",
    "        \n",
    "        file_links = fetch_file_links(year_url, file_type)\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            executor.map(lambda url: download_file(url, year_destination_folder), file_links)\n",
    "        \n",
    "        # Process and append raster data to a Polars DataFrame\n",
    "        master_df = append_and_process_files(year_destination_folder, transform, row_start, row_end, col_start, col_end)\n",
    "        \n",
    "        # Save the DataFrame as a Parquet file\n",
    "        parquet_path = os.path.join(destination_folder, f\"EDDI_{file_type}_{year}.parquet\")\n",
    "        master_df.write_parquet(parquet_path)\n",
    "        print(f\"Data for year {year} processed and saved as Parquet at {parquet_path}.\")\n",
    "        \n",
    "        shutil.rmtree(year_destination_folder)  # Optional: remove the directory after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parquet_files(directory_path: str, output_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Merges multiple Parquet files located in a specified directory into a single Parquet file.\n",
    "\n",
    "    Args:\n",
    "    directory_path (str): The path to the directory containing the Parquet files.\n",
    "    output_file_path (str): The path where the merged Parquet file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # List all Parquet files in the directory\n",
    "    parquet_files = [file for file in os.listdir(directory_path) if file.endswith('.parquet')]\n",
    "\n",
    "    # Read each Parquet file into a DataFrame and store in a list\n",
    "    dataframes = [pl.read_parquet(os.path.join(directory_path, file)) for file in parquet_files]\n",
    "\n",
    "    # Concatenate all DataFrames vertically\n",
    "    merged_df = pl.concat(dataframes)\n",
    "\n",
    "    # Write the merged DataFrame to a new Parquet file\n",
    "    merged_df.write_parquet(output_file_path)\n",
    "\n",
    "    print(f\"Merged file saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def delete_directory(dir_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Deletes a directory and all of its contents.\n",
    "    \n",
    "    Args:\n",
    "    dir_path (str): The path to the directory to be deleted.\n",
    "    \n",
    "    Raises:\n",
    "    FileNotFoundError: If the directory does not exist.\n",
    "    PermissionError: If the deletion is not allowed due to permission restrictions.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(f\"Directory '{dir_path}' has been deleted.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The directory {dir_path} does not exist.\")\n",
    "\n",
    "# Specify the directory to delete\n",
    "# directory_path = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/eddi\"\n",
    "\n",
    "# Delete the directory\n",
    "# try:\n",
    "#     delete_directory(directory_path)\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # base_url = \"https://downloads.psl.noaa.gov/Projects/EDDI/global_archive/NCEP/\"\n",
    "    # destination_folder = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/eddi\"\n",
    "    # start_year = 1981\n",
    "    # end_year = 2024\n",
    "    # file_types = [\"01wk\", \"02wk\", \"01mn\", \"02mn\", \"03mn\", \"06mn\", \"09mn\", \"12mn\"]\n",
    "\n",
    "    # for file_type in file_types:\n",
    "    #     download_and_process_raster_data(base_url, start_year, end_year, file_type, destination_folder)\n",
    "\n",
    "    #     # Construct the directory and output file paths\n",
    "    #     directory_path = '/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/eddi'\n",
    "    #     output_file_path = f'/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_{file_type}.parquet'\n",
    "\n",
    "    #     # Call the function to merge the files\n",
    "    #     merge_parquet_files(directory_path, output_file_path)\n",
    "\n",
    "    #     # Delete the directory\n",
    "    #     try:\n",
    "    #         delete_directory(directory_path)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "# Define the path to your Parquet file\n",
    "parquet_file_path_1 = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\"\n",
    "parquet_file_path_2 = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\"\n",
    "\n",
    "\n",
    "# Use Polars to read the Parquet file\n",
    "df_1 = pl.read_parquet(parquet_file_path_1)\n",
    "df_2 = pl.read_parquet(parquet_file_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (62_374_360, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time</th><th>lat</th><th>lon</th><th>eddi</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f32</td></tr></thead><tbody><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>23.0</td><td>0.000288</td></tr><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>23.5</td><td>-0.115403</td></tr><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>24.0</td><td>-0.29267</td></tr><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>24.5</td><td>-0.29267</td></tr><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>25.0</td><td>-0.415945</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>49.5</td><td>-999.0</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>50.0</td><td>-999.0</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>50.5</td><td>-999.0</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>51.0</td><td>-999.0</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>51.5</td><td>-999.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (62_374_360, 4)\n",
       "┌────────────┬───────┬──────┬───────────┐\n",
       "│ time       ┆ lat   ┆ lon  ┆ eddi      │\n",
       "│ ---        ┆ ---   ┆ ---  ┆ ---       │\n",
       "│ str        ┆ f64   ┆ f64  ┆ f32       │\n",
       "╞════════════╪═══════╪══════╪═══════════╡\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 23.0 ┆ 0.000288  │\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 23.5 ┆ -0.115403 │\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 24.0 ┆ -0.29267  │\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 24.5 ┆ -0.29267  │\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 25.0 ┆ -0.415945 │\n",
       "│ …          ┆ …     ┆ …    ┆ …         │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 49.5 ┆ -999.0    │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 50.0 ┆ -999.0    │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 50.5 ┆ -999.0    │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 51.0 ┆ -999.0    │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 51.5 ┆ -999.0    │\n",
       "└────────────┴───────┴──────┴───────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (62_374_360, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time</th><th>lat</th><th>lon</th><th>eddi</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f32</td></tr></thead><tbody><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>23.0</td><td>-0.761296</td></tr><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>23.5</td><td>-0.479941</td></tr><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>24.0</td><td>-0.479941</td></tr><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>24.5</td><td>-0.353626</td></tr><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>25.0</td><td>-0.232805</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>49.5</td><td>-999.0</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>50.0</td><td>-999.0</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>50.5</td><td>-999.0</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>51.0</td><td>-999.0</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>51.5</td><td>-999.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (62_374_360, 4)\n",
       "┌────────────┬───────┬──────┬───────────┐\n",
       "│ time       ┆ lat   ┆ lon  ┆ eddi      │\n",
       "│ ---        ┆ ---   ┆ ---  ┆ ---       │\n",
       "│ str        ┆ f64   ┆ f64  ┆ f32       │\n",
       "╞════════════╪═══════╪══════╪═══════════╡\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 23.0 ┆ -0.761296 │\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 23.5 ┆ -0.479941 │\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 24.0 ┆ -0.479941 │\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 24.5 ┆ -0.353626 │\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 25.0 ┆ -0.232805 │\n",
       "│ …          ┆ …     ┆ …    ┆ …         │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 49.5 ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 50.0 ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 50.5 ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 51.0 ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 51.5 ┆ -999.0    │\n",
       "└────────────┴───────┴──────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (62_374_360, 6)\n",
      "┌────────────┬───────┬──────┬───────────┬───────────┬───────────┐\n",
      "│ time       ┆ lat   ┆ lon  ┆ eddi_01wk ┆ eddi_02wk ┆ eddi_01mn │\n",
      "│ ---        ┆ ---   ┆ ---  ┆ ---       ┆ ---       ┆ ---       │\n",
      "│ date       ┆ f64   ┆ f64  ┆ f32       ┆ f32       ┆ f32       │\n",
      "╞════════════╪═══════╪══════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 23.0 ┆ -0.353626 ┆ -0.17379  ┆ 0.05802   │\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 23.5 ┆ -0.353626 ┆ -0.232805 ┆ -0.057442 │\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 24.0 ┆ -0.353626 ┆ -0.232805 ┆ -0.115403 │\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 24.5 ┆ -0.415945 ┆ -0.115403 ┆ -0.115403 │\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 25.0 ┆ -0.614502 ┆ -0.115403 ┆ -0.17379  │\n",
      "│ …          ┆ …     ┆ …    ┆ …         ┆ …         ┆ …         │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 49.5 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 50.0 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 50.5 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 51.0 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 51.5 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "└────────────┴───────┴──────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Define the path to your Parquet files\n",
    "parquet_file_path_1 = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\"\n",
    "parquet_file_path_2 = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\"\n",
    "parquet_file_path_3 = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet\"\n",
    "\n",
    "# Read the Parquet files\n",
    "df_1 = pl.read_parquet(parquet_file_path_1)\n",
    "df_2 = pl.read_parquet(parquet_file_path_2)\n",
    "df_3 = pl.read_parquet(parquet_file_path_3)\n",
    "\n",
    "# Rename 'eddi' column\n",
    "df_1 = df_1.rename({\"eddi\": \"eddi_01wk\"})\n",
    "df_2 = df_2.rename({\"eddi\": \"eddi_02wk\"})\n",
    "df_3 = df_3.rename({\"eddi\": \"eddi_01mn\"})\n",
    "\n",
    "# Convert 'time' column from string to date format\n",
    "df_1 = df_1.with_columns(pl.col(\"time\").str.strptime(pl.Date, \"%Y-%m-%d\"))\n",
    "df_2 = df_2.with_columns(pl.col(\"time\").str.strptime(pl.Date, \"%Y-%m-%d\"))\n",
    "df_3 = df_3.with_columns(pl.col(\"time\").str.strptime(pl.Date, \"%Y-%m-%d\"))\n",
    "\n",
    "# Merge the dataframes based on 'time', 'lat', and 'lon'\n",
    "merged_df = df_1.join(df_2, on=[\"time\", \"lat\", \"lon\"])\n",
    "merged_df = merged_df.join(df_3, on=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "# Display the merged dataframe\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (62_374_360, 11)\n",
      "┌────────────┬───────┬──────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ time       ┆ lat   ┆ lon  ┆ eddi_01mn ┆ … ┆ eddi_03mn ┆ eddi_06mn ┆ eddi_09mn ┆ eddi_mn   │\n",
      "│ ---        ┆ ---   ┆ ---  ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
      "│ date       ┆ f64   ┆ f64  ┆ f32       ┆   ┆ f32       ┆ f32       ┆ f32       ┆ f32       │\n",
      "╞════════════╪═══════╪══════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 23.0 ┆ 0.546651  ┆ … ┆ -0.545979 ┆ -0.479941 ┆ -0.115403 ┆ -0.115403 │\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 23.5 ┆ 0.416576  ┆ … ┆ -0.545979 ┆ -0.479941 ┆ -0.115403 ┆ -0.057442 │\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 24.0 ┆ 0.416576  ┆ … ┆ -0.545979 ┆ -0.614502 ┆ -0.115403 ┆ -0.115403 │\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 24.5 ┆ 0.416576  ┆ … ┆ -0.479941 ┆ -0.686047 ┆ -0.115403 ┆ -0.17379  │\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 25.0 ┆ 0.416576  ┆ … ┆ -0.415945 ┆ -0.686047 ┆ -0.232805 ┆ -0.17379  │\n",
      "│ …          ┆ …     ┆ …    ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 49.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 50.0 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 50.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 51.0 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 51.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "└────────────┴───────┴──────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Define the paths to your Parquet files\n",
    "parquet_files = [\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_03mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_06mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_09mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_12_mn.parquet\"\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over the Parquet files\n",
    "for file_path in parquet_files:\n",
    "    # Read the Parquet file\n",
    "    df = pl.read_parquet(file_path)\n",
    "    \n",
    "    # Extract the time period from the file name\n",
    "    time_period = file_path.split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    # Rename the 'eddi' column with the time period\n",
    "    df = df.rename({\"eddi\": f\"eddi_{time_period}\"})\n",
    "    \n",
    "    # Check if 'time' column is already in Date format\n",
    "    if df.dtypes[df.columns.index(\"time\")] != pl.Date:\n",
    "        # Convert 'time' column to Date format\n",
    "        df = df.with_columns(pl.col(\"time\").cast(pl.Date))\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Merge all the dataframes based on 'time', 'lat', and 'lon'\n",
    "merged_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    merged_df = merged_df.join(df, on=[\"time\", \"lat\", \"lon\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from typing import List\n",
    "\n",
    "def merge_parquet_files(parquet_files: List[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge multiple Parquet files into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        parquet_files (List[str]): List of file paths to the Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Merged DataFrame containing data from all the Parquet files.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate over the Parquet files\n",
    "    for file_path in parquet_files:\n",
    "        try:\n",
    "            # Read the Parquet file\n",
    "            df = pl.read_parquet(file_path)\n",
    "            \n",
    "            # Extract the time period from the file name\n",
    "            time_period = file_path.split(\"_\")[-1].split(\".\")[0]\n",
    "            \n",
    "            # Rename the 'eddi' column with the time period\n",
    "            df = df.rename({\"eddi\": f\"eddi_{time_period}\"})\n",
    "            \n",
    "            # Check if 'time' column is already in Date format\n",
    "            if df.dtypes[df.columns.index(\"time\")] != pl.Date:\n",
    "                # Convert 'time' column to Date format\n",
    "                df = df.with_columns(pl.col(\"time\").cast(pl.Date))\n",
    "            \n",
    "            # Append the dataframe to the list\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {file_path}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Check if any dataframes were successfully loaded\n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No valid dataframes found in the provided Parquet files.\")\n",
    "\n",
    "    # Merge all the dataframes based on 'time', 'lat', and 'lon'\n",
    "    merged_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        try:\n",
    "            merged_df = merged_df.join(df, on=[\"time\", \"lat\", \"lon\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging dataframe: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your Parquet files\n",
    "parquet_files = [\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_03mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_06mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_09mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_12mn.parquet\"\n",
    "]\n",
    "\n",
    "merged_df = merge_parquet_files(parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (62_374_360, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time</th><th>lat</th><th>lon</th><th>eddi_01mn</th><th>eddi_01wk</th><th>eddi_02mn</th><th>eddi_02wk</th><th>eddi_03mn</th><th>eddi_06mn</th><th>eddi_09mn</th><th>eddi_12mn</th></tr><tr><td>date</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>1981-01-01</td><td>22.0</td><td>23.0</td><td>0.546651</td><td>-0.545979</td><td>-0.232805</td><td>0.546651</td><td>-0.545979</td><td>-0.479941</td><td>-0.115403</td><td>-0.115403</td></tr><tr><td>1981-01-01</td><td>22.0</td><td>23.5</td><td>0.416576</td><td>-0.686047</td><td>-0.17379</td><td>0.48059</td><td>-0.545979</td><td>-0.479941</td><td>-0.115403</td><td>-0.057442</td></tr><tr><td>1981-01-01</td><td>22.0</td><td>24.0</td><td>0.416576</td><td>-0.761296</td><td>-0.17379</td><td>0.48059</td><td>-0.545979</td><td>-0.614502</td><td>-0.115403</td><td>-0.115403</td></tr><tr><td>1981-01-01</td><td>22.0</td><td>24.5</td><td>0.416576</td><td>-0.686047</td><td>-0.17379</td><td>0.416576</td><td>-0.479941</td><td>-0.686047</td><td>-0.115403</td><td>-0.17379</td></tr><tr><td>1981-01-01</td><td>22.0</td><td>25.0</td><td>0.416576</td><td>-0.614502</td><td>-0.17379</td><td>0.546651</td><td>-0.415945</td><td>-0.686047</td><td>-0.232805</td><td>-0.17379</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>49.5</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>50.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>50.5</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>51.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>51.5</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (62_374_360, 11)\n",
       "┌────────────┬───────┬──────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ time       ┆ lat   ┆ lon  ┆ eddi_01mn ┆ … ┆ eddi_03mn ┆ eddi_06mn ┆ eddi_09mn ┆ eddi_12mn │\n",
       "│ ---        ┆ ---   ┆ ---  ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ date       ┆ f64   ┆ f64  ┆ f32       ┆   ┆ f32       ┆ f32       ┆ f32       ┆ f32       │\n",
       "╞════════════╪═══════╪══════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 23.0 ┆ 0.546651  ┆ … ┆ -0.545979 ┆ -0.479941 ┆ -0.115403 ┆ -0.115403 │\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 23.5 ┆ 0.416576  ┆ … ┆ -0.545979 ┆ -0.479941 ┆ -0.115403 ┆ -0.057442 │\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 24.0 ┆ 0.416576  ┆ … ┆ -0.545979 ┆ -0.614502 ┆ -0.115403 ┆ -0.115403 │\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 24.5 ┆ 0.416576  ┆ … ┆ -0.479941 ┆ -0.686047 ┆ -0.115403 ┆ -0.17379  │\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 25.0 ┆ 0.416576  ┆ … ┆ -0.415945 ┆ -0.686047 ┆ -0.232805 ┆ -0.17379  │\n",
       "│ …          ┆ …     ┆ …    ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 49.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 50.0 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 50.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 51.0 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 51.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "└────────────┴───────┴──────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# # Define the path to your Parquet file\n",
    "# parquet_file_path = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\"\n",
    "# # parquet_file_path = \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data.parquet\"\n",
    "\n",
    "\n",
    "# # Use Polars to read the Parquet file\n",
    "# df = pl.read_parquet(parquet_file_path)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.write_parquet(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_polars_df(df, start_lat=-12.0, start_lon=23.125, new_lat_res=0.5, new_lon_res=0.625):\n",
    "    \"\"\"\n",
    "    Regrids a Polars DataFrame with latitude and longitude to a common specified grid, starting from specified latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pl.DataFrame): Polars DataFrame to be regridded.\n",
    "    - start_lat (float): Starting latitude for the grid.\n",
    "    - start_lon (float): Starting longitude for the grid.\n",
    "    - new_lat_res (float): The new resolution for latitude.\n",
    "    - new_lon_res (float): The new resolution for longitude.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: A DataFrame containing the regridded data.\n",
    "\n",
    "    Example of usage:\n",
    "    regridded_df = regrid_polars_df(polars_df)\n",
    "    \"\"\"\n",
    "    # Calculate global min and max of latitude and longitude\n",
    "    lat_min = df.select(pl.min('lat')).to_numpy().item()\n",
    "    lat_max = df.select(pl.max('lat')).to_numpy().item()\n",
    "    lon_min = df.select(pl.min('lon')).to_numpy().item()\n",
    "    lon_max = df.select(pl.max('lon')).to_numpy().item()\n",
    "\n",
    "    # Adjust the start points if they are outside the current data range\n",
    "    start_lat = max(lat_min, start_lat)\n",
    "    start_lon = max(lon_min, start_lon)\n",
    "\n",
    "    # Create common latitude and longitude grids starting from the specified points\n",
    "    common_lat = np.arange(start_lat, lat_max + new_lat_res, new_lat_res)\n",
    "    common_lon = np.arange(start_lon, lon_max + new_lon_res, new_lon_res)\n",
    "\n",
    "    # Function to find nearest grid point\n",
    "    def find_nearest(array, value):\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return array[idx]\n",
    "\n",
    "    # Map each lat and lon in the DataFrame to the nearest grid point\n",
    "    df = df.with_columns([\n",
    "        df['lat'].map_elements(lambda x: find_nearest(common_lat, x), return_dtype=float).alias('new_lat'),\n",
    "        df['lon'].map_elements(lambda x: find_nearest(common_lon, x), return_dtype=float).alias('new_lon')\n",
    "    ]).drop(['lat', 'lon']).rename({'new_lat': 'lat', 'new_lon': 'lon'})\n",
    "\n",
    "    # Optional: Aggregate data if necessary\n",
    "    df = df.group_by(['time', 'lat', 'lon']).agg(pl.col('eddi').mean().alias('eddi'))\n",
    "\n",
    "    # Sort by time, latitude, and longitude for better organization\n",
    "    df = df.sort(['time', 'lat', 'lon'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage example (assuming df is your DataFrame loaded with data)\n",
    "regridded_df = regrid_polars_df(eddi_clean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (21_086_019, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time</th><th>lat</th><th>lon</th><th>eddi</th></tr><tr><td>datetime[μs]</td><td>f64</td><td>f64</td><td>f32</td></tr></thead><tbody><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>23.125</td><td>-0.761296</td></tr><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>23.75</td><td>-0.88392</td></tr><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>24.375</td><td>-0.926714</td></tr><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>25.0</td><td>-1.019687</td></tr><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>25.625</td><td>-1.019687</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>38.75</td><td>0.567209</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>39.375</td><td>0.437543</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>40.0</td><td>0.199132</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>43.125</td><td>-0.500068</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>48.75</td><td>-0.436921</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (21_086_019, 4)\n",
       "┌─────────────────────┬───────┬────────┬───────────┐\n",
       "│ time                ┆ lat   ┆ lon    ┆ eddi      │\n",
       "│ ---                 ┆ ---   ┆ ---    ┆ ---       │\n",
       "│ datetime[μs]        ┆ f64   ┆ f64    ┆ f32       │\n",
       "╞═════════════════════╪═══════╪════════╪═══════════╡\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 23.125 ┆ -0.761296 │\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 23.75  ┆ -0.88392  │\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 24.375 ┆ -0.926714 │\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 25.0   ┆ -1.019687 │\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 25.625 ┆ -1.019687 │\n",
       "│ …                   ┆ …     ┆ …      ┆ …         │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 38.75  ┆ 0.567209  │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 39.375 ┆ 0.437543  │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 40.0   ┆ 0.199132  │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 43.125 ┆ -0.500068 │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 48.75  ┆ -0.436921 │\n",
       "└─────────────────────┴───────┴────────┴───────────┘"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regridded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "regridded_df.write_parquet(\"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/eddi.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
