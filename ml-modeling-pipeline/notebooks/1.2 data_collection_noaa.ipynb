{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import rasterio\n",
    "from affine import Affine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully: EDDI_01mn_19810103.asc\n",
      "File downloaded successfully: EDDI_01mn_19810104.asc\n",
      "File downloaded successfully: EDDI_01mn_19810108.asc\n",
      "File downloaded successfully: EDDI_01mn_19810105.asc\n",
      "File downloaded successfully: EDDI_01mn_19810106.asc\n",
      "File downloaded successfully: EDDI_01mn_19810107.asc\n",
      "File downloaded successfully: EDDI_01mn_19810102.asc\n",
      "File downloaded successfully: EDDI_01mn_19810101.asc\n",
      "File downloaded successfully: EDDI_01mn_19810109.asc\n",
      "File downloaded successfully: EDDI_01mn_19810110.asc\n",
      "File downloaded successfully: EDDI_01mn_19810113.asc\n",
      "File downloaded successfully: EDDI_01mn_19810112.asc\n",
      "File downloaded successfully: EDDI_01mn_19810114.asc\n",
      "File downloaded successfully: EDDI_01mn_19810111.asc\n",
      "File downloaded successfully: EDDI_01mn_19810115.asc\n",
      "File downloaded successfully: EDDI_01mn_19810116.asc\n",
      "File downloaded successfully: EDDI_01mn_19810117.asc\n",
      "File downloaded successfully: EDDI_01mn_19810118.asc\n",
      "File downloaded successfully: EDDI_01mn_19810119.asc\n",
      "File downloaded successfully: EDDI_01mn_19810120.asc\n",
      "File downloaded successfully: EDDI_01mn_19810121.asc\n",
      "File downloaded successfully: EDDI_01mn_19810122.asc\n",
      "File downloaded successfully: EDDI_01mn_19810123.asc\n",
      "File downloaded successfully: EDDI_01mn_19810124.asc\n",
      "File downloaded successfully: EDDI_01mn_19810125.asc\n",
      "File downloaded successfully: EDDI_01mn_19810126.asc\n",
      "File downloaded successfully: EDDI_01mn_19810128.asc\n",
      "File downloaded successfully: EDDI_01mn_19810130.asc\n",
      "File downloaded successfully: EDDI_01mn_19810127.asc\n",
      "File downloaded successfully: EDDI_01mn_19810129.asc\n",
      "File downloaded successfully: EDDI_01mn_19810131.asc\n",
      "File downloaded successfully: EDDI_01mn_19810201.asc\n",
      "File downloaded successfully: EDDI_01mn_19810202.asc\n",
      "File downloaded successfully: EDDI_01mn_19810203.asc\n",
      "File downloaded successfully: EDDI_01mn_19810204.asc\n",
      "File downloaded successfully: EDDI_01mn_19810205.asc\n",
      "File downloaded successfully: EDDI_01mn_19810206.asc\n",
      "File downloaded successfully: EDDI_01mn_19810207.asc\n",
      "File downloaded successfully: EDDI_01mn_19810208.asc\n",
      "File downloaded successfully: EDDI_01mn_19810209.asc\n",
      "File downloaded successfully: EDDI_01mn_19810210.asc\n",
      "File downloaded successfully: EDDI_01mn_19810211.asc\n",
      "File downloaded successfully: EDDI_01mn_19810212.asc\n",
      "File downloaded successfully: EDDI_01mn_19810213.asc\n",
      "File downloaded successfully: EDDI_01mn_19810215.asc\n",
      "File downloaded successfully: EDDI_01mn_19810214.asc\n",
      "File downloaded successfully: EDDI_01mn_19810216.asc\n",
      "File downloaded successfully: EDDI_01mn_19810217.asc\n",
      "File downloaded successfully: EDDI_01mn_19810218.asc\n",
      "File downloaded successfully: EDDI_01mn_19810219.asc\n",
      "File downloaded successfully: EDDI_01mn_19810220.asc\n",
      "File downloaded successfully: EDDI_01mn_19810221.asc\n",
      "File downloaded successfully: EDDI_01mn_19810222.asc\n",
      "File downloaded successfully: EDDI_01mn_19810223.asc\n",
      "File downloaded successfully: EDDI_01mn_19810224.asc\n",
      "File downloaded successfully: EDDI_01mn_19810225.asc\n",
      "File downloaded successfully: EDDI_01mn_19810226.asc\n",
      "File downloaded successfully: EDDI_01mn_19810227.asc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 178\u001b[0m\n\u001b[1;32m    172\u001b[0m file_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m01mn\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[39m# file_type = \"02mn\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m# file_type = \"03mn\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m# file_type = \"06mn\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39m# file_type = \"09mn\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m# file_type = \"12mn\"\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m download_and_process_raster_data(base_url, start_year, end_year, file_type, destination_folder)\n",
      "Cell \u001b[0;32mIn[3], line 152\u001b[0m, in \u001b[0;36mdownload_and_process_raster_data\u001b[0;34m(base_url, start_year, end_year, file_type, destination_folder)\u001b[0m\n\u001b[1;32m    149\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(year_destination_folder)\n\u001b[1;32m    151\u001b[0m file_links \u001b[39m=\u001b[39m fetch_file_links(year_url, file_type)\n\u001b[0;32m--> 152\u001b[0m \u001b[39mwith\u001b[39;49;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m) \u001b[39mas\u001b[39;49;00m executor:\n\u001b[1;32m    153\u001b[0m     executor\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m url: download_file(url, year_destination_folder), file_links)\n\u001b[1;32m    155\u001b[0m \u001b[39m# Process and append raster data to a Polars DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    648\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         t\u001b[39m.\u001b[39;49mjoin()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1140\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully: EDDI_01mn_19810301.asc\n",
      "File downloaded successfully: EDDI_01mn_19810228.asc\n",
      "File downloaded successfully: EDDI_01mn_19810302.asc\n",
      "File downloaded successfully: EDDI_01mn_19810303.asc\n",
      "File downloaded successfully: EDDI_01mn_19810304.asc\n",
      "File downloaded successfully: EDDI_01mn_19810305.asc\n",
      "File downloaded successfully: EDDI_01mn_19810306.asc\n",
      "File downloaded successfully: EDDI_01mn_19810307.asc\n",
      "File downloaded successfully: EDDI_01mn_19810309.asc\n",
      "File downloaded successfully: EDDI_01mn_19810308.asc\n",
      "File downloaded successfully: EDDI_01mn_19810310.asc\n",
      "File downloaded successfully: EDDI_01mn_19810311.asc\n",
      "File downloaded successfully: EDDI_01mn_19810312.asc\n",
      "File downloaded successfully: EDDI_01mn_19810313.asc\n",
      "File downloaded successfully: EDDI_01mn_19810314.asc\n",
      "File downloaded successfully: EDDI_01mn_19810315.asc\n",
      "File downloaded successfully: EDDI_01mn_19810316.asc\n",
      "File downloaded successfully: EDDI_01mn_19810317.asc\n",
      "File downloaded successfully: EDDI_01mn_19810318.asc\n",
      "File downloaded successfully: EDDI_01mn_19810319.asc\n",
      "File downloaded successfully: EDDI_01mn_19810322.asc\n",
      "File downloaded successfully: EDDI_01mn_19810320.asc\n",
      "File downloaded successfully: EDDI_01mn_19810321.asc\n",
      "File downloaded successfully: EDDI_01mn_19810323.asc\n",
      "File downloaded successfully: EDDI_01mn_19810324.asc\n",
      "File downloaded successfully: EDDI_01mn_19810326.asc\n",
      "File downloaded successfully: EDDI_01mn_19810327.asc\n",
      "File downloaded successfully: EDDI_01mn_19810325.asc\n",
      "File downloaded successfully: EDDI_01mn_19810328.asc\n",
      "File downloaded successfully: EDDI_01mn_19810329.asc\n",
      "File downloaded successfully: EDDI_01mn_19810330.asc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def fetch_file_links(year_url, file_type):\n",
    "#     try:\n",
    "#         response = requests.get(year_url)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         links = soup.find_all('a')\n",
    "#         file_links = [year_url + link['href'] for link in links if link['href'].endswith('.asc')]\n",
    "#         filtered_links = [link for link in file_links if file_type in link]\n",
    "#         return filtered_links\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Failed to fetch file links. Error: {e}\")\n",
    "#         return []\n",
    "\n",
    "def fetch_file_links(year_url: str, file_type: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Fetches file links from a given URL that match a specific file type and filter by additional criteria.\n",
    "    \n",
    "    Parameters:\n",
    "    - year_url: The base URL to search for file links.\n",
    "    - file_type: The file extension to filter links by.\n",
    "    \n",
    "    Returns:\n",
    "    A list of URLs that match the specified criteria.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    try:\n",
    "        response = session.get(year_url)\n",
    "        response.raise_for_status()  # Raises a HTTPError if the status is 4xx, 5xx\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        file_links = [f\"{year_url}{link['href']}\" for link in links if link['href'].endswith('.asc')]\n",
    "        filtered_links = [link for link in file_links if file_type in link]\n",
    "        return filtered_links\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Request error occurred: {err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    return []\n",
    "\n",
    "\n",
    "def download_file(url, destination_folder):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    destination_path = os.path.join(destination_folder, filename)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(destination_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File downloaded successfully: {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download the file: {filename}. Error: {e}\")\n",
    "\n",
    "def append_and_process_files(source_dir, transform, row_start, row_end, col_start, col_end):\n",
    "    master_df = pl.DataFrame()\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.asc'):\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "            with rasterio.open(file_path) as src:\n",
    "                data = src.read(1)  # Read the first band\n",
    "                filtered_data = data[row_start:row_end, col_start:col_end]\n",
    "            df = create_polars_dataframe(file_path, filtered_data, row_start, row_end, col_start, col_end, transform)\n",
    "            master_df = master_df.vstack(df) if not master_df.is_empty() else df\n",
    "    return master_df\n",
    "\n",
    "\n",
    "def create_polars_dataframe(file_name: str, data: np.ndarray, row_start: int, row_end: int, col_start: int, col_end: int, transform: Affine):\n",
    "    \"\"\"\n",
    "    Creates a Polars DataFrame from filtered raster data with columns for time, latitude, longitude, and EDDI values.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name (str): The name of the file, used to extract the time information.\n",
    "    - data (numpy.ndarray): The filtered raster data array.\n",
    "    - row_start (int): The starting index for rows of the filtered data.\n",
    "    - row_end (int): The ending index for rows of the filtered data.\n",
    "    - col_start (int): The starting index for columns of the filtered data.\n",
    "    - col_end (int): The ending index for columns of the filtered data.\n",
    "    - transform (Affine): The affine transformation used to convert indices to geographic coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: A Polars DataFrame with columns for time, latitude, longitude, and EDDI values.\n",
    "    \"\"\"\n",
    "    # Extract the date from the file name using regular expressions\n",
    "    date_match = re.search(r'\\d{8}', file_name)\n",
    "    if date_match:\n",
    "        date = date_match.group(0)\n",
    "        # Transform the date to 'yyyy-mm-dd' format\n",
    "        formatted_date = f\"{date[:4]}-{date[4:6]}-{date[6:]}\"\n",
    "    else:\n",
    "        raise ValueError(\"Date not found in file name.\")\n",
    "\n",
    "    # Generate latitude and longitude arrays\n",
    "    lon_arr = []\n",
    "    lat_arr = []\n",
    "    for row in range(row_start, row_end):\n",
    "        for col in range(col_start, col_end):\n",
    "            lon, lat = transform * (col, row)\n",
    "            lon_arr.append(lon)\n",
    "            lat_arr.append(lat)\n",
    "\n",
    "    # Flatten the data array to match the latitude and longitude arrays\n",
    "    eddi_values = data.flatten()\n",
    "\n",
    "    # Create a Polars DataFrame\n",
    "    df = pl.DataFrame({\n",
    "        \"time\": [formatted_date] * len(eddi_values),\n",
    "        \"lat\": lat_arr,\n",
    "        \"lon\": lon_arr,\n",
    "        \"eddi\": eddi_values\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "from rasterio.transform import Affine\n",
    "import numpy as np\n",
    "\n",
    "def geographic_to_index(lon, lat, transform):\n",
    "    \"\"\"\n",
    "    Convert geographic coordinates to raster indices.\n",
    "\n",
    "    Parameters:\n",
    "    - lon (float): The longitude to convert.\n",
    "    - lat (float): The latitude to convert.\n",
    "    - transform (Affine): The affine transform associated with the raster, defining how geographic coordinates are transformed into raster indices.\n",
    "\n",
    "    Returns:\n",
    "    - (int, int): The zero-based column index and row index corresponding to the given longitude and latitude.\n",
    "    \"\"\"\n",
    "    col, row = ~transform * (lon, lat)\n",
    "    return int(col), int(row)\n",
    "\n",
    "def download_and_process_raster_data(base_url, start_year, end_year, file_type, destination_folder):\n",
    "    # Define the affine transform for a raster where each pixel represents a 0.5x0.5 degree area\n",
    "    transform = Affine(0.5, 0.0, -180, 0.0, -0.5, 90)\n",
    "\n",
    "    # Define geographic bounds and calculate index ranges\n",
    "    lat_range = (-12, 22)  # South to North\n",
    "    lon_range = (23, 52)  # West to East\n",
    "    col_start, row_end = geographic_to_index(lon_range[0], lat_range[0], transform)\n",
    "    col_end, row_start = geographic_to_index(lon_range[1], lat_range[1], transform)\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        year_url = f\"{base_url}{year}/\"\n",
    "        year_destination_folder = os.path.join(destination_folder, str(year))\n",
    "        \n",
    "        if not os.path.exists(year_destination_folder):\n",
    "            os.makedirs(year_destination_folder)\n",
    "        \n",
    "        file_links = fetch_file_links(year_url, file_type)\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            executor.map(lambda url: download_file(url, year_destination_folder), file_links)\n",
    "        \n",
    "        # Process and append raster data to a Polars DataFrame\n",
    "        master_df = append_and_process_files(year_destination_folder, transform, row_start, row_end, col_start, col_end)\n",
    "        \n",
    "        # Save the DataFrame as a Parquet file\n",
    "        parquet_path = os.path.join(destination_folder, f\"EDDI_{file_type}_{year}.parquet\")\n",
    "        master_df.write_parquet(parquet_path)\n",
    "        print(f\"Data for year {year} processed and saved as Parquet at {parquet_path}.\")\n",
    "        \n",
    "        shutil.rmtree(year_destination_folder)  # Optional: remove the directory after processing\n",
    "\n",
    "# Example usage\n",
    "base_url = \"https://downloads.psl.noaa.gov/Projects/EDDI/global_archive/NCEP/\"\n",
    "destination_folder = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/eddi\"\n",
    "start_year = 1981\n",
    "end_year = 2024\n",
    "# file_type = \"01wk\"\n",
    "# file_type = \"02wk\"\n",
    "file_type = \"01mn\"\n",
    "# file_type = \"02mn\"\n",
    "# file_type = \"03mn\"\n",
    "# file_type = \"06mn\"\n",
    "# file_type = \"09mn\"\n",
    "# file_type = \"12mn\"\n",
    "download_and_process_raster_data(base_url, start_year, end_year, file_type, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to your Parquet file\n",
    "# parquet_file_path = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/eddi/EDDI_12mn_1981.parquet\"\n",
    "\n",
    "# # Use Polars to read the Parquet file\n",
    "# df = pl.read_parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shutil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m directory_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/eddi\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Remove the directory and all its contents\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m shutil\u001b[39m.\u001b[39mrmtree(directory_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the path of the directory\n",
    "directory_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/eddi'\n",
    "\n",
    "# Remove the directory and all its contents\n",
    "shutil.rmtree(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file saved to /workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "def merge_parquet_files(directory_path: str, output_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Merges multiple Parquet files located in a specified directory into a single Parquet file.\n",
    "\n",
    "    Args:\n",
    "    directory_path (str): The path to the directory containing the Parquet files.\n",
    "    output_file_path (str): The path where the merged Parquet file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # List all Parquet files in the directory\n",
    "    parquet_files = [file for file in os.listdir(directory_path) if file.endswith('.parquet')]\n",
    "\n",
    "    # Read each Parquet file into a DataFrame and store in a list\n",
    "    dataframes = [pl.read_parquet(os.path.join(directory_path, file)) for file in parquet_files]\n",
    "\n",
    "    # Concatenate all DataFrames vertically\n",
    "    merged_df = pl.concat(dataframes)\n",
    "\n",
    "    # Write the merged DataFrame to a new Parquet file\n",
    "    merged_df.write_parquet(output_file_path)\n",
    "\n",
    "    print(f\"Merged file saved to {output_file_path}\")\n",
    "\n",
    "# Specify the directory containing your Parquet files and the output file path\n",
    "directory_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/eddi'\n",
    "# output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet'\n",
    "# output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet'\n",
    "output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet'\n",
    "# output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02mn.parquet'\n",
    "# output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_03mn.parquet'\n",
    "# output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_06mn.parquet'\n",
    "# output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_09mn.parquet'\n",
    "# output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_12mn.parquet'\n",
    "\n",
    "# Call the function to merge the files\n",
    "merge_parquet_files(directory_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "# Define the path to your Parquet file\n",
    "parquet_file_path_1 = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\"\n",
    "parquet_file_path_2 = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\"\n",
    "\n",
    "\n",
    "# Use Polars to read the Parquet file\n",
    "df_1 = pl.read_parquet(parquet_file_path_1)\n",
    "df_2 = pl.read_parquet(parquet_file_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (62_374_360, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time</th><th>lat</th><th>lon</th><th>eddi</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f32</td></tr></thead><tbody><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>23.0</td><td>0.000288</td></tr><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>23.5</td><td>-0.115403</td></tr><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>24.0</td><td>-0.29267</td></tr><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>24.5</td><td>-0.29267</td></tr><tr><td>&quot;1981-01-04&quot;</td><td>22.0</td><td>25.0</td><td>-0.415945</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>49.5</td><td>-999.0</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>50.0</td><td>-999.0</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>50.5</td><td>-999.0</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>51.0</td><td>-999.0</td></tr><tr><td>&quot;2024-04-15&quot;</td><td>-11.5</td><td>51.5</td><td>-999.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (62_374_360, 4)\n",
       "┌────────────┬───────┬──────┬───────────┐\n",
       "│ time       ┆ lat   ┆ lon  ┆ eddi      │\n",
       "│ ---        ┆ ---   ┆ ---  ┆ ---       │\n",
       "│ str        ┆ f64   ┆ f64  ┆ f32       │\n",
       "╞════════════╪═══════╪══════╪═══════════╡\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 23.0 ┆ 0.000288  │\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 23.5 ┆ -0.115403 │\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 24.0 ┆ -0.29267  │\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 24.5 ┆ -0.29267  │\n",
       "│ 1981-01-04 ┆ 22.0  ┆ 25.0 ┆ -0.415945 │\n",
       "│ …          ┆ …     ┆ …    ┆ …         │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 49.5 ┆ -999.0    │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 50.0 ┆ -999.0    │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 50.5 ┆ -999.0    │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 51.0 ┆ -999.0    │\n",
       "│ 2024-04-15 ┆ -11.5 ┆ 51.5 ┆ -999.0    │\n",
       "└────────────┴───────┴──────┴───────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (62_374_360, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time</th><th>lat</th><th>lon</th><th>eddi</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f32</td></tr></thead><tbody><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>23.0</td><td>-0.761296</td></tr><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>23.5</td><td>-0.479941</td></tr><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>24.0</td><td>-0.479941</td></tr><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>24.5</td><td>-0.353626</td></tr><tr><td>&quot;1981-01-07&quot;</td><td>22.0</td><td>25.0</td><td>-0.232805</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>49.5</td><td>-999.0</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>50.0</td><td>-999.0</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>50.5</td><td>-999.0</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>51.0</td><td>-999.0</td></tr><tr><td>&quot;2024-04-29&quot;</td><td>-11.5</td><td>51.5</td><td>-999.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (62_374_360, 4)\n",
       "┌────────────┬───────┬──────┬───────────┐\n",
       "│ time       ┆ lat   ┆ lon  ┆ eddi      │\n",
       "│ ---        ┆ ---   ┆ ---  ┆ ---       │\n",
       "│ str        ┆ f64   ┆ f64  ┆ f32       │\n",
       "╞════════════╪═══════╪══════╪═══════════╡\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 23.0 ┆ -0.761296 │\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 23.5 ┆ -0.479941 │\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 24.0 ┆ -0.479941 │\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 24.5 ┆ -0.353626 │\n",
       "│ 1981-01-07 ┆ 22.0  ┆ 25.0 ┆ -0.232805 │\n",
       "│ …          ┆ …     ┆ …    ┆ …         │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 49.5 ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 50.0 ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 50.5 ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 51.0 ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 51.5 ┆ -999.0    │\n",
       "└────────────┴───────┴──────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (62_374_360, 6)\n",
      "┌────────────┬───────┬──────┬───────────┬───────────┬───────────┐\n",
      "│ time       ┆ lat   ┆ lon  ┆ eddi_01wk ┆ eddi_02wk ┆ eddi_01mn │\n",
      "│ ---        ┆ ---   ┆ ---  ┆ ---       ┆ ---       ┆ ---       │\n",
      "│ date       ┆ f64   ┆ f64  ┆ f32       ┆ f32       ┆ f32       │\n",
      "╞════════════╪═══════╪══════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 23.0 ┆ -0.353626 ┆ -0.17379  ┆ 0.05802   │\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 23.5 ┆ -0.353626 ┆ -0.232805 ┆ -0.057442 │\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 24.0 ┆ -0.353626 ┆ -0.232805 ┆ -0.115403 │\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 24.5 ┆ -0.415945 ┆ -0.115403 ┆ -0.115403 │\n",
      "│ 1981-01-06 ┆ 22.0  ┆ 25.0 ┆ -0.614502 ┆ -0.115403 ┆ -0.17379  │\n",
      "│ …          ┆ …     ┆ …    ┆ …         ┆ …         ┆ …         │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 49.5 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 50.0 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 50.5 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 51.0 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 51.5 ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "└────────────┴───────┴──────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Define the path to your Parquet files\n",
    "parquet_file_path_1 = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\"\n",
    "parquet_file_path_2 = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\"\n",
    "parquet_file_path_3 = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet\"\n",
    "\n",
    "# Read the Parquet files\n",
    "df_1 = pl.read_parquet(parquet_file_path_1)\n",
    "df_2 = pl.read_parquet(parquet_file_path_2)\n",
    "df_3 = pl.read_parquet(parquet_file_path_3)\n",
    "\n",
    "# Rename 'eddi' column\n",
    "df_1 = df_1.rename({\"eddi\": \"eddi_01wk\"})\n",
    "df_2 = df_2.rename({\"eddi\": \"eddi_02wk\"})\n",
    "df_3 = df_3.rename({\"eddi\": \"eddi_01mn\"})\n",
    "\n",
    "# Convert 'time' column from string to date format\n",
    "df_1 = df_1.with_columns(pl.col(\"time\").str.strptime(pl.Date, \"%Y-%m-%d\"))\n",
    "df_2 = df_2.with_columns(pl.col(\"time\").str.strptime(pl.Date, \"%Y-%m-%d\"))\n",
    "df_3 = df_3.with_columns(pl.col(\"time\").str.strptime(pl.Date, \"%Y-%m-%d\"))\n",
    "\n",
    "# Merge the dataframes based on 'time', 'lat', and 'lon'\n",
    "merged_df = df_1.join(df_2, on=[\"time\", \"lat\", \"lon\"])\n",
    "merged_df = merged_df.join(df_3, on=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "# Display the merged dataframe\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (62_374_360, 11)\n",
      "┌────────────┬───────┬──────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ time       ┆ lat   ┆ lon  ┆ eddi_01mn ┆ … ┆ eddi_03mn ┆ eddi_06mn ┆ eddi_09mn ┆ eddi_mn   │\n",
      "│ ---        ┆ ---   ┆ ---  ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
      "│ date       ┆ f64   ┆ f64  ┆ f32       ┆   ┆ f32       ┆ f32       ┆ f32       ┆ f32       │\n",
      "╞════════════╪═══════╪══════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 23.0 ┆ 0.546651  ┆ … ┆ -0.545979 ┆ -0.479941 ┆ -0.115403 ┆ -0.115403 │\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 23.5 ┆ 0.416576  ┆ … ┆ -0.545979 ┆ -0.479941 ┆ -0.115403 ┆ -0.057442 │\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 24.0 ┆ 0.416576  ┆ … ┆ -0.545979 ┆ -0.614502 ┆ -0.115403 ┆ -0.115403 │\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 24.5 ┆ 0.416576  ┆ … ┆ -0.479941 ┆ -0.686047 ┆ -0.115403 ┆ -0.17379  │\n",
      "│ 1981-01-01 ┆ 22.0  ┆ 25.0 ┆ 0.416576  ┆ … ┆ -0.415945 ┆ -0.686047 ┆ -0.232805 ┆ -0.17379  │\n",
      "│ …          ┆ …     ┆ …    ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 49.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 50.0 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 50.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 51.0 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "│ 2024-04-29 ┆ -11.5 ┆ 51.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
      "└────────────┴───────┴──────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Define the paths to your Parquet files\n",
    "parquet_files = [\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_03mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_06mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_09mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_12_mn.parquet\"\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over the Parquet files\n",
    "for file_path in parquet_files:\n",
    "    # Read the Parquet file\n",
    "    df = pl.read_parquet(file_path)\n",
    "    \n",
    "    # Extract the time period from the file name\n",
    "    time_period = file_path.split(\"_\")[-1].split(\".\")[0]\n",
    "    \n",
    "    # Rename the 'eddi' column with the time period\n",
    "    df = df.rename({\"eddi\": f\"eddi_{time_period}\"})\n",
    "    \n",
    "    # Check if 'time' column is already in Date format\n",
    "    if df.dtypes[df.columns.index(\"time\")] != pl.Date:\n",
    "        # Convert 'time' column to Date format\n",
    "        df = df.with_columns(pl.col(\"time\").cast(pl.Date))\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Merge all the dataframes based on 'time', 'lat', and 'lon'\n",
    "merged_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    merged_df = merged_df.join(df, on=[\"time\", \"lat\", \"lon\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from typing import List\n",
    "\n",
    "def merge_parquet_files(parquet_files: List[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge multiple Parquet files into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        parquet_files (List[str]): List of file paths to the Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Merged DataFrame containing data from all the Parquet files.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate over the Parquet files\n",
    "    for file_path in parquet_files:\n",
    "        try:\n",
    "            # Read the Parquet file\n",
    "            df = pl.read_parquet(file_path)\n",
    "            \n",
    "            # Extract the time period from the file name\n",
    "            time_period = file_path.split(\"_\")[-1].split(\".\")[0]\n",
    "            \n",
    "            # Rename the 'eddi' column with the time period\n",
    "            df = df.rename({\"eddi\": f\"eddi_{time_period}\"})\n",
    "            \n",
    "            # Check if 'time' column is already in Date format\n",
    "            if df.dtypes[df.columns.index(\"time\")] != pl.Date:\n",
    "                # Convert 'time' column to Date format\n",
    "                df = df.with_columns(pl.col(\"time\").cast(pl.Date))\n",
    "            \n",
    "            # Append the dataframe to the list\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {file_path}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Check if any dataframes were successfully loaded\n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No valid dataframes found in the provided Parquet files.\")\n",
    "\n",
    "    # Merge all the dataframes based on 'time', 'lat', and 'lon'\n",
    "    merged_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        try:\n",
    "            merged_df = merged_df.join(df, on=[\"time\", \"lat\", \"lon\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging dataframe: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your Parquet files\n",
    "parquet_files = [\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_03mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_06mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_09mn.parquet\",\n",
    "    \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_12mn.parquet\"\n",
    "]\n",
    "\n",
    "merged_df = merge_parquet_files(parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (62_374_360, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time</th><th>lat</th><th>lon</th><th>eddi_01mn</th><th>eddi_01wk</th><th>eddi_02mn</th><th>eddi_02wk</th><th>eddi_03mn</th><th>eddi_06mn</th><th>eddi_09mn</th><th>eddi_12mn</th></tr><tr><td>date</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>1981-01-01</td><td>22.0</td><td>23.0</td><td>0.546651</td><td>-0.545979</td><td>-0.232805</td><td>0.546651</td><td>-0.545979</td><td>-0.479941</td><td>-0.115403</td><td>-0.115403</td></tr><tr><td>1981-01-01</td><td>22.0</td><td>23.5</td><td>0.416576</td><td>-0.686047</td><td>-0.17379</td><td>0.48059</td><td>-0.545979</td><td>-0.479941</td><td>-0.115403</td><td>-0.057442</td></tr><tr><td>1981-01-01</td><td>22.0</td><td>24.0</td><td>0.416576</td><td>-0.761296</td><td>-0.17379</td><td>0.48059</td><td>-0.545979</td><td>-0.614502</td><td>-0.115403</td><td>-0.115403</td></tr><tr><td>1981-01-01</td><td>22.0</td><td>24.5</td><td>0.416576</td><td>-0.686047</td><td>-0.17379</td><td>0.416576</td><td>-0.479941</td><td>-0.686047</td><td>-0.115403</td><td>-0.17379</td></tr><tr><td>1981-01-01</td><td>22.0</td><td>25.0</td><td>0.416576</td><td>-0.614502</td><td>-0.17379</td><td>0.546651</td><td>-0.415945</td><td>-0.686047</td><td>-0.232805</td><td>-0.17379</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>49.5</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>50.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>50.5</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>51.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr><tr><td>2024-04-29</td><td>-11.5</td><td>51.5</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td><td>-999.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (62_374_360, 11)\n",
       "┌────────────┬───────┬──────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ time       ┆ lat   ┆ lon  ┆ eddi_01mn ┆ … ┆ eddi_03mn ┆ eddi_06mn ┆ eddi_09mn ┆ eddi_12mn │\n",
       "│ ---        ┆ ---   ┆ ---  ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ date       ┆ f64   ┆ f64  ┆ f32       ┆   ┆ f32       ┆ f32       ┆ f32       ┆ f32       │\n",
       "╞════════════╪═══════╪══════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 23.0 ┆ 0.546651  ┆ … ┆ -0.545979 ┆ -0.479941 ┆ -0.115403 ┆ -0.115403 │\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 23.5 ┆ 0.416576  ┆ … ┆ -0.545979 ┆ -0.479941 ┆ -0.115403 ┆ -0.057442 │\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 24.0 ┆ 0.416576  ┆ … ┆ -0.545979 ┆ -0.614502 ┆ -0.115403 ┆ -0.115403 │\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 24.5 ┆ 0.416576  ┆ … ┆ -0.479941 ┆ -0.686047 ┆ -0.115403 ┆ -0.17379  │\n",
       "│ 1981-01-01 ┆ 22.0  ┆ 25.0 ┆ 0.416576  ┆ … ┆ -0.415945 ┆ -0.686047 ┆ -0.232805 ┆ -0.17379  │\n",
       "│ …          ┆ …     ┆ …    ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 49.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 50.0 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 50.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 51.0 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "│ 2024-04-29 ┆ -11.5 ┆ 51.5 ┆ -999.0    ┆ … ┆ -999.0    ┆ -999.0    ┆ -999.0    ┆ -999.0    │\n",
       "└────────────┴───────┴──────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# # Define the path to your Parquet file\n",
    "# parquet_file_path = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\"\n",
    "# # parquet_file_path = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data.parquet\"\n",
    "\n",
    "\n",
    "# # Use Polars to read the Parquet file\n",
    "# df = pl.read_parquet(parquet_file_path)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/merged_eddi_data.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.write_parquet(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "cannot compare 'date/datetime/time' to a string value (create native python { 'date', 'datetime', 'time' } or compare to a temporal column)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m# Example usage, ensure that 'df' is already your loaded DataFrame with proper columns\u001b[39;00m\n\u001b[1;32m     45\u001b[0m specific_date \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1981-01-08 00:00:00\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 46\u001b[0m visualize_eddi_map(df, specific_date)\n",
      "Cell \u001b[0;32mIn[19], line 23\u001b[0m, in \u001b[0;36mvisualize_eddi_map\u001b[0;34m(df, specific_date)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mVisualizes EDDI values on a map for a specific date, excluding missing data represented by -999.0.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m- None: Displays an interactive map.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# Check and convert 'time' column to datetime if necessary\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# if df.schema()['time'] != pl.datatypes.Datetime:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m#     df = df.with_column(pl.col('time').str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\"))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m# Filter the DataFrame for the specific date and exclude missing data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m filtered_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mfilter((pl\u001b[39m.\u001b[39;49mcol(\u001b[39m\"\u001b[39;49m\u001b[39meddi\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m!=\u001b[39;49m \u001b[39m-\u001b[39;49m\u001b[39m999.0\u001b[39;49m) \u001b[39m&\u001b[39;49m (pl\u001b[39m.\u001b[39;49mcol(\u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m==\u001b[39;49m specific_date))\n\u001b[1;32m     25\u001b[0m \u001b[39m# Convert Polars DataFrame to Pandas DataFrame for plotting\u001b[39;00m\n\u001b[1;32m     26\u001b[0m pd_df \u001b[39m=\u001b[39m filtered_df\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m/workspace/soil-ml-modeling-pipeline/.venv/lib/python3.11/site-packages/polars/dataframe/frame.py:3740\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, *predicates, **constraints)\u001b[0m\n\u001b[1;32m   3642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter\u001b[39m(\n\u001b[1;32m   3643\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3644\u001b[0m     \u001b[39m*\u001b[39mpredicates: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3651\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconstraints: Any,\n\u001b[1;32m   3652\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m   3653\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3654\u001b[0m \u001b[39m    Filter the rows in the DataFrame based on one or more predicate expressions.\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3738\u001b[0m \u001b[39m    └─────┴─────┴─────┘\u001b[39;00m\n\u001b[1;32m   3739\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3740\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlazy()\u001b[39m.\u001b[39;49mfilter(\u001b[39m*\u001b[39;49mpredicates, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconstraints)\u001b[39m.\u001b[39;49mcollect(_eager\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/workspace/soil-ml-modeling-pipeline/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py:1708\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, background, _eager)\u001b[0m\n\u001b[1;32m   1705\u001b[0m \u001b[39mif\u001b[39;00m background:\n\u001b[1;32m   1706\u001b[0m     \u001b[39mreturn\u001b[39;00m InProcessQuery(ldf\u001b[39m.\u001b[39mcollect_concurrently())\n\u001b[0;32m-> 1708\u001b[0m \u001b[39mreturn\u001b[39;00m wrap_df(ldf\u001b[39m.\u001b[39;49mcollect())\n",
      "\u001b[0;31mComputeError\u001b[0m: cannot compare 'date/datetime/time' to a string value (create native python { 'date', 'datetime', 'time' } or compare to a temporal column)"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import plotly.express as px\n",
    "\n",
    "def visualize_eddi_map(df, specific_date):\n",
    "    \"\"\"\n",
    "    Visualizes EDDI values on a map for a specific date, excluding missing data represented by -999.0.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pl.DataFrame): The DataFrame containing time, latitude, longitude, and EDDI values.\n",
    "    - specific_date (str): The specific date to visualize in \"YYYY-MM-DD\" format.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays an interactive map.\n",
    "    \"\"\"\n",
    "    # Check and convert 'time' column to datetime if necessary\n",
    "    # if df.schema()['time'] != pl.datatypes.Datetime:\n",
    "    #     df = df.with_column(pl.col('time').str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    # Convert the specific date string to datetime object\n",
    "    # specific_datetime = pl.datetime.strptime(specific_date + \" 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Filter the DataFrame for the specific date and exclude missing data\n",
    "    date_filter = \"1981-01-01\"\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df.filter(pl.col(\"time\").dt.date() == pl.lit(specific_date))\n",
    "\n",
    "    # Convert Polars DataFrame to Pandas DataFrame for plotting\n",
    "    pd_df = filtered_df.to_pandas()\n",
    "\n",
    "    # Create a scatter map\n",
    "    fig = px.scatter_geo(pd_df,\n",
    "                         lat=\"lat\",\n",
    "                         lon=\"lon\",\n",
    "                         color=\"eddi\",\n",
    "                         hover_name=\"eddi\",\n",
    "                         projection=\"natural earth\",\n",
    "                         title=f\"EDDI Visualization on Map for {specific_date}\",\n",
    "                         color_continuous_scale=px.colors.sequential.Plasma)\n",
    "    \n",
    "    # Update the layout to better fit the map presentation\n",
    "    fig.update_layout(geo=dict(showland=True, landcolor=\"lightgrey\"),\n",
    "                      margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Example usage, ensure that 'df' is already your loaded DataFrame with proper columns\n",
    "specific_date = \"1981-01-08 00:00:00\"\n",
    "visualize_eddi_map(df, specific_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_polars_df(df, start_lat=-12.0, start_lon=23.125, new_lat_res=0.5, new_lon_res=0.625):\n",
    "    \"\"\"\n",
    "    Regrids a Polars DataFrame with latitude and longitude to a common specified grid, starting from specified latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pl.DataFrame): Polars DataFrame to be regridded.\n",
    "    - start_lat (float): Starting latitude for the grid.\n",
    "    - start_lon (float): Starting longitude for the grid.\n",
    "    - new_lat_res (float): The new resolution for latitude.\n",
    "    - new_lon_res (float): The new resolution for longitude.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: A DataFrame containing the regridded data.\n",
    "\n",
    "    Example of usage:\n",
    "    regridded_df = regrid_polars_df(polars_df)\n",
    "    \"\"\"\n",
    "    # Calculate global min and max of latitude and longitude\n",
    "    lat_min = df.select(pl.min('lat')).to_numpy().item()\n",
    "    lat_max = df.select(pl.max('lat')).to_numpy().item()\n",
    "    lon_min = df.select(pl.min('lon')).to_numpy().item()\n",
    "    lon_max = df.select(pl.max('lon')).to_numpy().item()\n",
    "\n",
    "    # Adjust the start points if they are outside the current data range\n",
    "    start_lat = max(lat_min, start_lat)\n",
    "    start_lon = max(lon_min, start_lon)\n",
    "\n",
    "    # Create common latitude and longitude grids starting from the specified points\n",
    "    common_lat = np.arange(start_lat, lat_max + new_lat_res, new_lat_res)\n",
    "    common_lon = np.arange(start_lon, lon_max + new_lon_res, new_lon_res)\n",
    "\n",
    "    # Function to find nearest grid point\n",
    "    def find_nearest(array, value):\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return array[idx]\n",
    "\n",
    "    # Map each lat and lon in the DataFrame to the nearest grid point\n",
    "    df = df.with_columns([\n",
    "        df['lat'].map_elements(lambda x: find_nearest(common_lat, x), return_dtype=float).alias('new_lat'),\n",
    "        df['lon'].map_elements(lambda x: find_nearest(common_lon, x), return_dtype=float).alias('new_lon')\n",
    "    ]).drop(['lat', 'lon']).rename({'new_lat': 'lat', 'new_lon': 'lon'})\n",
    "\n",
    "    # Optional: Aggregate data if necessary\n",
    "    df = df.group_by(['time', 'lat', 'lon']).agg(pl.col('eddi').mean().alias('eddi'))\n",
    "\n",
    "    # Sort by time, latitude, and longitude for better organization\n",
    "    df = df.sort(['time', 'lat', 'lon'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage example (assuming df is your DataFrame loaded with data)\n",
    "regridded_df = regrid_polars_df(eddi_clean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (21_086_019, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>time</th><th>lat</th><th>lon</th><th>eddi</th></tr><tr><td>datetime[μs]</td><td>f64</td><td>f64</td><td>f32</td></tr></thead><tbody><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>23.125</td><td>-0.761296</td></tr><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>23.75</td><td>-0.88392</td></tr><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>24.375</td><td>-0.926714</td></tr><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>25.0</td><td>-1.019687</td></tr><tr><td>2001-01-01 00:00:00</td><td>-12.0</td><td>25.625</td><td>-1.019687</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>38.75</td><td>0.567209</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>39.375</td><td>0.437543</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>40.0</td><td>0.199132</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>43.125</td><td>-0.500068</td></tr><tr><td>2024-04-13 00:00:00</td><td>21.5</td><td>48.75</td><td>-0.436921</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (21_086_019, 4)\n",
       "┌─────────────────────┬───────┬────────┬───────────┐\n",
       "│ time                ┆ lat   ┆ lon    ┆ eddi      │\n",
       "│ ---                 ┆ ---   ┆ ---    ┆ ---       │\n",
       "│ datetime[μs]        ┆ f64   ┆ f64    ┆ f32       │\n",
       "╞═════════════════════╪═══════╪════════╪═══════════╡\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 23.125 ┆ -0.761296 │\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 23.75  ┆ -0.88392  │\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 24.375 ┆ -0.926714 │\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 25.0   ┆ -1.019687 │\n",
       "│ 2001-01-01 00:00:00 ┆ -12.0 ┆ 25.625 ┆ -1.019687 │\n",
       "│ …                   ┆ …     ┆ …      ┆ …         │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 38.75  ┆ 0.567209  │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 39.375 ┆ 0.437543  │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 40.0   ┆ 0.199132  │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 43.125 ┆ -0.500068 │\n",
       "│ 2024-04-13 00:00:00 ┆ 21.5  ┆ 48.75  ┆ -0.436921 │\n",
       "└─────────────────────┴───────┴────────┴───────────┘"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regridded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "regridded_df.write_parquet(\"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/eddi.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, destination_filename):\n",
    "    \"\"\"\n",
    "    Download a file from a specified URL and save it locally.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): URL of the file to download.\n",
    "    - destination_filename (str): Name of the file to save the downloaded content.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content of the response to a file\n",
    "        with open(destination_filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"File downloaded successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "# URL of the file to be downloaded\n",
    "file_url = \"https://downloads.psl.noaa.gov/Projects/EDDI/global_archive/NCEP/2002/EDDI_06mn_20020612.asc\"\n",
    "\n",
    "# Name of the file to save locally\n",
    "local_filename = \"/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/EDDI_12mn_20020612.asc\"\n",
    "\n",
    "# Call the function to download the file\n",
    "download_file(file_url, local_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "def load_ascii_grid(file_path: str):\n",
    "    \"\"\"\n",
    "    Load an ASCII grid file using rasterio and return its data along with metadata.\n",
    "\n",
    "    This function opens the specified ASCII grid file, reads its contents,\n",
    "    and returns both the grid data as a numpy ndarray and the metadata profile\n",
    "    associated with the raster dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The file path to the ASCII grid file.\n",
    "\n",
    "    Returns:\n",
    "    - tuple:\n",
    "        - data (numpy.ndarray): The raster data read from the file.\n",
    "        - profile (dict): The metadata profile of the raster dataset, which includes\n",
    "          details about the data format, dimensions, and georeferencing.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the file does not exist at the specified path.\n",
    "    - rasterio.errors.RasterioIOError: If there is an error opening the file with rasterio.\n",
    "    \"\"\"\n",
    "\n",
    "    with rasterio.open(file_path, mode='r') as src:\n",
    "        data = src.read(1)  # Read the first and only band\n",
    "        profile = src.profile\n",
    "\n",
    "    return data, profile\n",
    "\n",
    "# Example usage\n",
    "file_path = '/workspace/soil-ml-modeling-pipeline/ml-modeling-pipeline/data/01_raw/EDDI_12mn_20020612.asc'\n",
    "try:\n",
    "    data, profile = load_ascii_grid(file_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(\"Raster profile:\", profile)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_raster(data, nodata_value, title=\"Raster Data Visualization\"):\n",
    "    \"\"\"\n",
    "    Plots the raster data using matplotlib, masking the 'nodata' values.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The raster data to plot.\n",
    "    - nodata_value (float): The value that indicates no data in the raster.\n",
    "    - title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    # Mask the no-data values\n",
    "    data_masked = np.ma.masked_where(data == nodata_value, data)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(data_masked, cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='Data values')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Longitude Index')\n",
    "    plt.ylabel('Latitude Index')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_raster(data, -999.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import Affine\n",
    "import numpy as np\n",
    "\n",
    "def geographic_to_index(lon, lat, transform):\n",
    "    \"\"\"\n",
    "    Convert geographic coordinates to raster indices.\n",
    "\n",
    "    Parameters:\n",
    "    - lon (float): The longitude to convert.\n",
    "    - lat (float): The latitude to convert.\n",
    "    - transform (Affine): The affine transform associated with the raster, defining how geographic coordinates are transformed into raster indices.\n",
    "\n",
    "    Returns:\n",
    "    - (int, int): The zero-based column index and row index corresponding to the given longitude and latitude.\n",
    "    \"\"\"\n",
    "    col, row = ~transform * (lon, lat)\n",
    "    return int(col), int(row)\n",
    "\n",
    "def filter_raster_data(data, row_start, row_end, col_start, col_end):\n",
    "    \"\"\"\n",
    "    Filter the raster data array to include only the data within specified index ranges.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The full raster data array.\n",
    "    - row_start (int): The starting index for rows.\n",
    "    - row_end (int): The ending index for rows.\n",
    "    - col_start (int): The starting index for columns.\n",
    "    - col_end (int): The ending index for columns.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The filtered raster data array.\n",
    "    \"\"\"\n",
    "    return data[row_start:row_end, col_start:col_end]\n",
    "\n",
    "# Define the affine transform for a raster where each pixel represents a 0.5x0.5 degree area\n",
    "transform = Affine(0.5, 0.0, -180, 0.0, -0.5, 90)\n",
    "\n",
    "# Define geographic bounds and calculate index ranges\n",
    "lat_range = (-12, 22)  # South to North\n",
    "lon_range = (23, 52)  # West to East\n",
    "col_start, row_end = geographic_to_index(lon_range[0], lat_range[0], transform)\n",
    "col_end, row_start = geographic_to_index(lon_range[1], lat_range[1], transform)\n",
    "\n",
    "# Assuming 'data' is your loaded raster data\n",
    "filtered_data = filter_raster_data(data, row_start, row_end, col_start, col_end)\n",
    "print(filtered_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
