{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "\n",
    "# ds = xr.open_dataset('/teamspace/studios/this_studio/ml-drought-forecasting/ml-modeling-pipeline/data/02_intermediate/merged_data.nc'')\n",
    "ds = xr.open_dataset('/Users/adamprzychodni/Documents/Repos/ml-drought-forecasting/ml-modeling-pipeline/data/02_intermediate/merged_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Assuming your dataset is called ds\n",
    "# Example dataset variables: t2m, sst, tp, pev\n",
    "\n",
    "# 1. Extract relevant variables\n",
    "targets = ['sm']\n",
    "data_arrays = [ds[var] for var in targets]\n",
    "\n",
    "# 2. Flatten latitude and longitude into a single node dimension\n",
    "# Combine the latitude and longitude as one \"node\" dimension\n",
    "data_arrays_flattened = [da.stack(node=('latitude', 'longitude')) for da in data_arrays]\n",
    "\n",
    "# 3. Convert each variable's DataArray to a numpy array and add a new channel dimension\n",
    "# (so we have the shape (time, nodes, channels))\n",
    "target = np.stack([da.to_numpy() for da in data_arrays_flattened], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .npy file\n",
    "np.save('ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/target.npy', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = np.where(~np.isnan(target), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .npy file\n",
    "np.save('ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/mask.npy', mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Assuming your dataset is called ds\n",
    "# Example dataset variables: t2m, sst, tp, pev\n",
    "\n",
    "# 1. Extract relevant variables\n",
    "variables = ['t2m', 'd2m', 'msl', 'sp', 'sst', 'skt', 'e', 'pev', 'mlspr', 'ro', 'slt', 'swvl1', 'stl1', 'cvh', 'lai_hv', 'cvl', 'tcc', 'mper', 'tco3', 'lsm']\n",
    "\n",
    "data_arrays = [ds[var] for var in variables]\n",
    "\n",
    "# 2. Flatten latitude and longitude into a single node dimension\n",
    "# Combine the latitude and longitude as one \"node\" dimension\n",
    "data_arrays_flattened = [da.stack(node=('latitude', 'longitude')) for da in data_arrays]\n",
    "\n",
    "# 3. Convert each variable's DataArray to a numpy array and add a new channel dimension\n",
    "# (so we have the shape (time, nodes, channels))\n",
    "covariates = np.stack([da.to_numpy() for da in data_arrays_flattened], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.94796997e+02,  2.91070618e+02,  1.01160688e+05, ...,\n",
       "         -1.19325705e-09,  5.65088913e-03,  0.00000000e+00],\n",
       "        [ 2.94720825e+02,  2.90953430e+02,  1.01172688e+05, ...,\n",
       "         -1.19325705e-09,  5.64719364e-03,  0.00000000e+00],\n",
       "        [ 2.94691528e+02,  2.90701477e+02,  1.01184812e+05, ...,\n",
       "         -1.19325705e-09,  5.64600155e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.91923950e+02,  2.87041321e+02,  1.01571438e+05, ...,\n",
       "         -1.19325705e-09,  5.84162399e-03,  0.00000000e+00],\n",
       "        [ 2.92054810e+02,  2.87088196e+02,  1.01587562e+05, ...,\n",
       "         -1.19325705e-09,  5.82457706e-03,  0.00000000e+00],\n",
       "        [ 2.92066528e+02,  2.87248352e+02,  1.01605938e+05, ...,\n",
       "         -1.19325705e-09,  5.80800697e-03,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.95360138e+02,  2.91539581e+02,  1.01597125e+05, ...,\n",
       "          1.60071068e-09,  5.51155582e-03,  0.00000000e+00],\n",
       "        [ 2.95342560e+02,  2.91631378e+02,  1.01593375e+05, ...,\n",
       "          1.60071068e-09,  5.50988689e-03,  0.00000000e+00],\n",
       "        [ 2.95449982e+02,  2.91689972e+02,  1.01594500e+05, ...,\n",
       "          1.60071068e-09,  5.50631061e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.91481232e+02,  2.87076691e+02,  1.01299875e+05, ...,\n",
       "          1.60071068e-09,  5.63720241e-03,  0.00000000e+00],\n",
       "        [ 2.91602325e+02,  2.87256378e+02,  1.01321125e+05, ...,\n",
       "          1.60071068e-09,  5.64340129e-03,  0.00000000e+00],\n",
       "        [ 2.91785919e+02,  2.87465363e+02,  1.01342625e+05, ...,\n",
       "          1.60071068e-09,  5.64995781e-03,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.95027771e+02,  2.90736176e+02,  1.01465000e+05, ...,\n",
       "         -2.79396772e-09,  5.47569245e-03,  0.00000000e+00],\n",
       "        [ 2.95137146e+02,  2.90894379e+02,  1.01442125e+05, ...,\n",
       "         -2.79396772e-09,  5.46853989e-03,  0.00000000e+00],\n",
       "        [ 2.95412537e+02,  2.91087738e+02,  1.01422375e+05, ...,\n",
       "         -2.79396772e-09,  5.45459241e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.90377380e+02,  2.85150238e+02,  1.01798125e+05, ...,\n",
       "         -2.79396772e-09,  5.71232289e-03,  0.00000000e+00],\n",
       "        [ 2.90340271e+02,  2.85126801e+02,  1.01812500e+05, ...,\n",
       "         -2.79396772e-09,  5.72412461e-03,  0.00000000e+00],\n",
       "        [ 2.90508240e+02,  2.85286957e+02,  1.01824875e+05, ...,\n",
       "         -2.79396772e-09,  5.73604554e-03,  0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.91278564e+02,  2.86795044e+02,  1.01857500e+05, ...,\n",
       "         -2.64844857e-09,  6.75473735e-03,  0.00000000e+00],\n",
       "        [ 2.91165283e+02,  2.86777466e+02,  1.01846625e+05, ...,\n",
       "         -2.64844857e-09,  6.74591586e-03,  0.00000000e+00],\n",
       "        [ 2.91257080e+02,  2.86828247e+02,  1.01843625e+05, ...,\n",
       "         -2.64844857e-09,  6.73268363e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.97968018e+02,  2.93185669e+02,  1.02089625e+05, ...,\n",
       "         -2.64844857e-09,  5.92909381e-03,  0.00000000e+00],\n",
       "        [ 2.97831299e+02,  2.93000122e+02,  1.02103750e+05, ...,\n",
       "         -2.64844857e-09,  5.93505427e-03,  0.00000000e+00],\n",
       "        [ 2.97616455e+02,  2.92796997e+02,  1.02115500e+05, ...,\n",
       "         -2.64844857e-09,  5.94208762e-03,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.94269531e+02,  2.91275787e+02,  1.01399625e+05, ...,\n",
       "         -8.00355338e-10,  6.20374456e-03,  0.00000000e+00],\n",
       "        [ 2.94136719e+02,  2.91182037e+02,  1.01439125e+05, ...,\n",
       "         -8.00355338e-10,  6.19683042e-03,  0.00000000e+00],\n",
       "        [ 2.93890625e+02,  2.91254303e+02,  1.01475250e+05, ...,\n",
       "         -8.00355338e-10,  6.18622079e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.95574219e+02,  2.90769928e+02,  1.01831375e+05, ...,\n",
       "         -8.00355338e-10,  5.60722128e-03,  0.00000000e+00],\n",
       "        [ 2.95650391e+02,  2.90848053e+02,  1.01843000e+05, ...,\n",
       "         -8.00355338e-10,  5.61902300e-03,  0.00000000e+00],\n",
       "        [ 2.95718750e+02,  2.90875397e+02,  1.01855250e+05, ...,\n",
       "         -8.00355338e-10,  5.62891737e-03,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.94670990e+02,  2.90892212e+02,  1.01235938e+05, ...,\n",
       "         -2.72120815e-09,  5.95810637e-03,  0.00000000e+00],\n",
       "        [ 2.94553802e+02,  2.90939087e+02,  1.01247938e+05, ...,\n",
       "         -2.72120815e-09,  5.95154986e-03,  0.00000000e+00],\n",
       "        [ 2.94516693e+02,  2.90890259e+02,  1.01260062e+05, ...,\n",
       "         -2.72120815e-09,  5.94177470e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.93079193e+02,  2.87925415e+02,  1.01529812e+05, ...,\n",
       "         -2.72120815e-09,  5.60620055e-03,  0.00000000e+00],\n",
       "        [ 2.93128021e+02,  2.87952759e+02,  1.01510438e+05, ...,\n",
       "         -2.72120815e-09,  5.63040003e-03,  0.00000000e+00],\n",
       "        [ 2.93157318e+02,  2.88009399e+02,  1.01494062e+05, ...,\n",
       "         -2.72120815e-09,  5.65674528e-03,  0.00000000e+00]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Zakładając, że 'covariates' jest Twoim array'em numpy\n",
    "covariates = np.nan_to_num(covariates, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.94796997e+02,  2.91070618e+02,  1.01160688e+05, ...,\n",
       "         -1.19325705e-09,  5.65088913e-03,  0.00000000e+00],\n",
       "        [ 2.94720825e+02,  2.90953430e+02,  1.01172688e+05, ...,\n",
       "         -1.19325705e-09,  5.64719364e-03,  0.00000000e+00],\n",
       "        [ 2.94691528e+02,  2.90701477e+02,  1.01184812e+05, ...,\n",
       "         -1.19325705e-09,  5.64600155e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.91923950e+02,  2.87041321e+02,  1.01571438e+05, ...,\n",
       "         -1.19325705e-09,  5.84162399e-03,  0.00000000e+00],\n",
       "        [ 2.92054810e+02,  2.87088196e+02,  1.01587562e+05, ...,\n",
       "         -1.19325705e-09,  5.82457706e-03,  0.00000000e+00],\n",
       "        [ 2.92066528e+02,  2.87248352e+02,  1.01605938e+05, ...,\n",
       "         -1.19325705e-09,  5.80800697e-03,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.95360138e+02,  2.91539581e+02,  1.01597125e+05, ...,\n",
       "          1.60071068e-09,  5.51155582e-03,  0.00000000e+00],\n",
       "        [ 2.95342560e+02,  2.91631378e+02,  1.01593375e+05, ...,\n",
       "          1.60071068e-09,  5.50988689e-03,  0.00000000e+00],\n",
       "        [ 2.95449982e+02,  2.91689972e+02,  1.01594500e+05, ...,\n",
       "          1.60071068e-09,  5.50631061e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.91481232e+02,  2.87076691e+02,  1.01299875e+05, ...,\n",
       "          1.60071068e-09,  5.63720241e-03,  0.00000000e+00],\n",
       "        [ 2.91602325e+02,  2.87256378e+02,  1.01321125e+05, ...,\n",
       "          1.60071068e-09,  5.64340129e-03,  0.00000000e+00],\n",
       "        [ 2.91785919e+02,  2.87465363e+02,  1.01342625e+05, ...,\n",
       "          1.60071068e-09,  5.64995781e-03,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.95027771e+02,  2.90736176e+02,  1.01465000e+05, ...,\n",
       "         -2.79396772e-09,  5.47569245e-03,  0.00000000e+00],\n",
       "        [ 2.95137146e+02,  2.90894379e+02,  1.01442125e+05, ...,\n",
       "         -2.79396772e-09,  5.46853989e-03,  0.00000000e+00],\n",
       "        [ 2.95412537e+02,  2.91087738e+02,  1.01422375e+05, ...,\n",
       "         -2.79396772e-09,  5.45459241e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.90377380e+02,  2.85150238e+02,  1.01798125e+05, ...,\n",
       "         -2.79396772e-09,  5.71232289e-03,  0.00000000e+00],\n",
       "        [ 2.90340271e+02,  2.85126801e+02,  1.01812500e+05, ...,\n",
       "         -2.79396772e-09,  5.72412461e-03,  0.00000000e+00],\n",
       "        [ 2.90508240e+02,  2.85286957e+02,  1.01824875e+05, ...,\n",
       "         -2.79396772e-09,  5.73604554e-03,  0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.91278564e+02,  2.86795044e+02,  1.01857500e+05, ...,\n",
       "         -2.64844857e-09,  6.75473735e-03,  0.00000000e+00],\n",
       "        [ 2.91165283e+02,  2.86777466e+02,  1.01846625e+05, ...,\n",
       "         -2.64844857e-09,  6.74591586e-03,  0.00000000e+00],\n",
       "        [ 2.91257080e+02,  2.86828247e+02,  1.01843625e+05, ...,\n",
       "         -2.64844857e-09,  6.73268363e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.97968018e+02,  2.93185669e+02,  1.02089625e+05, ...,\n",
       "         -2.64844857e-09,  5.92909381e-03,  0.00000000e+00],\n",
       "        [ 2.97831299e+02,  2.93000122e+02,  1.02103750e+05, ...,\n",
       "         -2.64844857e-09,  5.93505427e-03,  0.00000000e+00],\n",
       "        [ 2.97616455e+02,  2.92796997e+02,  1.02115500e+05, ...,\n",
       "         -2.64844857e-09,  5.94208762e-03,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.94269531e+02,  2.91275787e+02,  1.01399625e+05, ...,\n",
       "         -8.00355338e-10,  6.20374456e-03,  0.00000000e+00],\n",
       "        [ 2.94136719e+02,  2.91182037e+02,  1.01439125e+05, ...,\n",
       "         -8.00355338e-10,  6.19683042e-03,  0.00000000e+00],\n",
       "        [ 2.93890625e+02,  2.91254303e+02,  1.01475250e+05, ...,\n",
       "         -8.00355338e-10,  6.18622079e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.95574219e+02,  2.90769928e+02,  1.01831375e+05, ...,\n",
       "         -8.00355338e-10,  5.60722128e-03,  0.00000000e+00],\n",
       "        [ 2.95650391e+02,  2.90848053e+02,  1.01843000e+05, ...,\n",
       "         -8.00355338e-10,  5.61902300e-03,  0.00000000e+00],\n",
       "        [ 2.95718750e+02,  2.90875397e+02,  1.01855250e+05, ...,\n",
       "         -8.00355338e-10,  5.62891737e-03,  0.00000000e+00]],\n",
       "\n",
       "       [[ 2.94670990e+02,  2.90892212e+02,  1.01235938e+05, ...,\n",
       "         -2.72120815e-09,  5.95810637e-03,  0.00000000e+00],\n",
       "        [ 2.94553802e+02,  2.90939087e+02,  1.01247938e+05, ...,\n",
       "         -2.72120815e-09,  5.95154986e-03,  0.00000000e+00],\n",
       "        [ 2.94516693e+02,  2.90890259e+02,  1.01260062e+05, ...,\n",
       "         -2.72120815e-09,  5.94177470e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.93079193e+02,  2.87925415e+02,  1.01529812e+05, ...,\n",
       "         -2.72120815e-09,  5.60620055e-03,  0.00000000e+00],\n",
       "        [ 2.93128021e+02,  2.87952759e+02,  1.01510438e+05, ...,\n",
       "         -2.72120815e-09,  5.63040003e-03,  0.00000000e+00],\n",
       "        [ 2.93157318e+02,  2.88009399e+02,  1.01494062e+05, ...,\n",
       "         -2.72120815e-09,  5.65674528e-03,  0.00000000e+00]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .npy file\n",
    "np.save('ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/covariates.npy', covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def generate_and_save_metadata(df: pd.DataFrame, lat_col: str = 'lat', lon_col: str = 'lon', save_directory: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates metadata from a given DataFrame by identifying unique latitude and longitude\n",
    "    combinations and assigning a unique node ID to each combination. The metadata is then\n",
    "    saved to a specified Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - lat_col (str): The name of the column containing latitude data.\n",
    "    - lon_col (str): The name of the column containing longitude data.\n",
    "    - save_directory (str, optional): The directory where the metadata file will be saved. If None, \n",
    "                                      the file will be saved in the current working directory.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the metadata with latitude, longitude, and 'node_id' as columns.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame contains necessary columns\n",
    "    if lat_col not in df.columns or lon_col not in df.columns:\n",
    "        raise ValueError(f\"DataFrame must contain '{lat_col}' and '{lon_col}' columns.\")\n",
    "\n",
    "    # Create a unique node ID for each unique latitude-longitude combination\n",
    "    unique_lat_lon = df[[lat_col, lon_col]].drop_duplicates().reset_index(drop=True)\n",
    "    unique_lat_lon['node_id'] = unique_lat_lon.index\n",
    "\n",
    "    # Create the metadata DataFrame\n",
    "    metadata = unique_lat_lon.set_index('node_id')\n",
    "\n",
    "    # Handle save directory and save metadata to Parquet file\n",
    "    if save_directory:\n",
    "        os.makedirs(save_directory, exist_ok=True)  # Create directory if it doesn't exist\n",
    "        file_path = os.path.join(save_directory, \"metadata.parquet\")\n",
    "    else:\n",
    "        file_path = \"metadata.parquet\"  # Save in the current working directory\n",
    "\n",
    "    metadata.to_parquet(file_path)\n",
    "    print(f\"Metadata file saved at: {file_path}\")\n",
    "\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata file saved at: ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/metadata.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assume you've already stacked your data arrays as 'data_arrays_flattened'\n",
    "# We'll use one of the flattened DataArrays to extract the node information\n",
    "da_flattened = data_arrays_flattened[0]  # Using the first variable for example\n",
    "\n",
    "# Get the MultiIndex from the 'node' dimension\n",
    "node_index = da_flattened.indexes['node']\n",
    "\n",
    "# Extract latitude and longitude from the MultiIndex\n",
    "latitudes = node_index.get_level_values('latitude').values\n",
    "longitudes = node_index.get_level_values('longitude').values\n",
    "\n",
    "# Create a DataFrame with 'lat' and 'lon' columns\n",
    "df = pd.DataFrame({\n",
    "    'lat': latitudes,\n",
    "    'lon': longitudes\n",
    "})\n",
    "\n",
    "# Now use your 'generate_and_save_metadata' function\n",
    "metadata = generate_and_save_metadata(\n",
    "    df, \n",
    "    lat_col='lat', \n",
    "    lon_col='lon', \n",
    "    save_directory='ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_parquet(\"ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/metadata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-30.0</td>\n",
       "      <td>-180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-30.0</td>\n",
       "      <td>-179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-30.0</td>\n",
       "      <td>-178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-30.0</td>\n",
       "      <td>-177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-30.0</td>\n",
       "      <td>-176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21955</th>\n",
       "      <td>30.0</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21956</th>\n",
       "      <td>30.0</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21957</th>\n",
       "      <td>30.0</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21958</th>\n",
       "      <td>30.0</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21959</th>\n",
       "      <td>30.0</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21960 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lat  lon\n",
       "node_id           \n",
       "0       -30.0 -180\n",
       "1       -30.0 -179\n",
       "2       -30.0 -178\n",
       "3       -30.0 -177\n",
       "4       -30.0 -176\n",
       "...       ...  ...\n",
       "21955    30.0  175\n",
       "21956    30.0  176\n",
       "21957    30.0  177\n",
       "21958    30.0  178\n",
       "21959    30.0  179\n",
       "\n",
       "[21960 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the DataFrame to a numpy ndarray\n",
    "metadata_array = metadata.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -30., -180.],\n",
       "       [ -30., -179.],\n",
       "       [ -30., -178.],\n",
       "       ...,\n",
       "       [  30.,  177.],\n",
       "       [  30.,  178.],\n",
       "       [  30.,  179.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .npy file\n",
    "np.save('ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/metadata.npy', metadata_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.ops.similarities import geographical_distance\n",
    "# Calculate geographical distances with coordinates converted to radians.\n",
    "dist = geographical_distance(metadata_array, to_rad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.        ,   96.29745869,  192.5930838 , ..., 6679.26370783,\n",
       "        6675.06538725, 6672.54511721],\n",
       "       [  96.29745869,    0.        ,   96.29745869, ..., 6685.13689318,\n",
       "        6679.26370783, 6675.06538725],\n",
       "       [ 192.5930838 ,   96.29745869,    0.        , ..., 6692.68049995,\n",
       "        6685.13689318, 6679.26370783],\n",
       "       ...,\n",
       "       [6679.26370783, 6685.13689318, 6692.68049995, ...,    0.        ,\n",
       "          96.29745869,  192.5930838 ],\n",
       "       [6675.06538725, 6679.26370783, 6685.13689318, ...,   96.29745869,\n",
       "           0.        ,   96.29745869],\n",
       "       [6672.54511721, 6675.06538725, 6679.26370783, ...,  192.5930838 ,\n",
       "          96.29745869,    0.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .npy file\n",
    "np.save('ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/distances.npy', dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List\n",
    "import numpy as np \n",
    "\n",
    "from tsl.datasets.prototypes import TabularDataset\n",
    "\n",
    "class DroughtDataset(TabularDataset):\n",
    "\n",
    "    similarity_options = {'distance', 'correlation'}\n",
    "\n",
    "    def __init__(self,\n",
    "                 root: str = None\n",
    "                 ):\n",
    "\n",
    "        self.root = root\n",
    "\n",
    "        # Load data\n",
    "        target, mask, u, dist, metadata = self.load()\n",
    "\n",
    "        covariates = {\n",
    "            'u': (u),\n",
    "            'metadata' : (metadata),\n",
    "            'distances': (dist)\n",
    "        }\n",
    "\n",
    "        super().__init__(target=target,\n",
    "                         mask=mask,\n",
    "                         covariates=covariates,\n",
    "                         similarity_score='distance',\n",
    "                         temporal_aggregation='mean',\n",
    "                         spatial_aggregation='mean',\n",
    "                         name='DroughtDataset')\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Load data from files.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Containing target, mask, covariates, distances, and metadata.\n",
    "        \"\"\"\n",
    "        target_path = f\"{self.root}target.npy\"\n",
    "        mask_path = f\"{self.root}mask.npy\"\n",
    "        dist_path = f\"{self.root}distances.npy\"\n",
    "        covariates_path = f\"{self.root}covariates.npy\"\n",
    "        metadata_path = f\"{self.root}metadata.npy\"\n",
    "\n",
    "        # Load main data\n",
    "        target = np.load(target_path)\n",
    "        mask = np.load(mask_path)\n",
    "        u = np.load(covariates_path)\n",
    "        dist = np.load(dist_path)\n",
    "        metadata = np.load(metadata_path)\n",
    "\n",
    "        return target, mask, u, dist, metadata\n",
    "\n",
    "\n",
    "    def compute_similarity(self, method: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute similarity matrix based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            method (str): The similarity computation method ('distance' or 'correlation').\n",
    "            **kwargs: Additional keyword arguments for similarity computation.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Computed similarity matrix.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an unknown similarity method is provided.\n",
    "        \"\"\"\n",
    "        if method == \"distance\":\n",
    "            # Calculate a Gaussian kernel similarity from the distance matrix, using a default or provided 'theta'\n",
    "            theta = kwargs.get('theta', np.std(self.distances))\n",
    "            return self.gaussian_kernel(self.distances, theta=theta)\n",
    "        elif method == \"correlation\":\n",
    "            # Compute the average correlation between nodes over the target features\n",
    "            # Reshape target data to have nodes as columns\n",
    "            target_values = self.target.values.reshape(len(self.target), -1, len(self.target_node_feature))\n",
    "            # Average over the target features\n",
    "            target_mean = target_values.mean(axis=2)\n",
    "            # Compute correlation between nodes\n",
    "            corr = np.corrcoef(target_mean, rowvar=False)\n",
    "            return (corr + 1) / 2  # Normalize to [0, 1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown similarity method: {method}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian_kernel(distances, theta):\n",
    "        \"\"\"\n",
    "        Compute Gaussian kernel similarity from distances.\n",
    "\n",
    "        Args:\n",
    "            distances (numpy.ndarray): Distance matrix.\n",
    "            theta (float): Kernel bandwidth parameter.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gaussian kernel similarity matrix.\n",
    "        \"\"\"\n",
    "        return np.exp(-(distances ** 2) / (2 * (theta ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DroughtDataset(root='ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[ 0.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        ...,\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has missing values: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Has missing values: {dataset.has_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_mask(dataset.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'u': array([[[ 2.94796997e+02,  2.91070618e+02,  1.01160688e+05, ...,\n",
       "          -1.19325705e-09,  5.65088913e-03,  0.00000000e+00],\n",
       "         [ 2.94720825e+02,  2.90953430e+02,  1.01172688e+05, ...,\n",
       "          -1.19325705e-09,  5.64719364e-03,  0.00000000e+00],\n",
       "         [ 2.94691528e+02,  2.90701477e+02,  1.01184812e+05, ...,\n",
       "          -1.19325705e-09,  5.64600155e-03,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 2.91923950e+02,  2.87041321e+02,  1.01571438e+05, ...,\n",
       "          -1.19325705e-09,  5.84162399e-03,  0.00000000e+00],\n",
       "         [ 2.92054810e+02,  2.87088196e+02,  1.01587562e+05, ...,\n",
       "          -1.19325705e-09,  5.82457706e-03,  0.00000000e+00],\n",
       "         [ 2.92066528e+02,  2.87248352e+02,  1.01605938e+05, ...,\n",
       "          -1.19325705e-09,  5.80800697e-03,  0.00000000e+00]],\n",
       " \n",
       "        [[ 2.95360138e+02,  2.91539581e+02,  1.01597125e+05, ...,\n",
       "           1.60071068e-09,  5.51155582e-03,  0.00000000e+00],\n",
       "         [ 2.95342560e+02,  2.91631378e+02,  1.01593375e+05, ...,\n",
       "           1.60071068e-09,  5.50988689e-03,  0.00000000e+00],\n",
       "         [ 2.95449982e+02,  2.91689972e+02,  1.01594500e+05, ...,\n",
       "           1.60071068e-09,  5.50631061e-03,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 2.91481232e+02,  2.87076691e+02,  1.01299875e+05, ...,\n",
       "           1.60071068e-09,  5.63720241e-03,  0.00000000e+00],\n",
       "         [ 2.91602325e+02,  2.87256378e+02,  1.01321125e+05, ...,\n",
       "           1.60071068e-09,  5.64340129e-03,  0.00000000e+00],\n",
       "         [ 2.91785919e+02,  2.87465363e+02,  1.01342625e+05, ...,\n",
       "           1.60071068e-09,  5.64995781e-03,  0.00000000e+00]],\n",
       " \n",
       "        [[ 2.95027771e+02,  2.90736176e+02,  1.01465000e+05, ...,\n",
       "          -2.79396772e-09,  5.47569245e-03,  0.00000000e+00],\n",
       "         [ 2.95137146e+02,  2.90894379e+02,  1.01442125e+05, ...,\n",
       "          -2.79396772e-09,  5.46853989e-03,  0.00000000e+00],\n",
       "         [ 2.95412537e+02,  2.91087738e+02,  1.01422375e+05, ...,\n",
       "          -2.79396772e-09,  5.45459241e-03,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 2.90377380e+02,  2.85150238e+02,  1.01798125e+05, ...,\n",
       "          -2.79396772e-09,  5.71232289e-03,  0.00000000e+00],\n",
       "         [ 2.90340271e+02,  2.85126801e+02,  1.01812500e+05, ...,\n",
       "          -2.79396772e-09,  5.72412461e-03,  0.00000000e+00],\n",
       "         [ 2.90508240e+02,  2.85286957e+02,  1.01824875e+05, ...,\n",
       "          -2.79396772e-09,  5.73604554e-03,  0.00000000e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 2.91278564e+02,  2.86795044e+02,  1.01857500e+05, ...,\n",
       "          -2.64844857e-09,  6.75473735e-03,  0.00000000e+00],\n",
       "         [ 2.91165283e+02,  2.86777466e+02,  1.01846625e+05, ...,\n",
       "          -2.64844857e-09,  6.74591586e-03,  0.00000000e+00],\n",
       "         [ 2.91257080e+02,  2.86828247e+02,  1.01843625e+05, ...,\n",
       "          -2.64844857e-09,  6.73268363e-03,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 2.97968018e+02,  2.93185669e+02,  1.02089625e+05, ...,\n",
       "          -2.64844857e-09,  5.92909381e-03,  0.00000000e+00],\n",
       "         [ 2.97831299e+02,  2.93000122e+02,  1.02103750e+05, ...,\n",
       "          -2.64844857e-09,  5.93505427e-03,  0.00000000e+00],\n",
       "         [ 2.97616455e+02,  2.92796997e+02,  1.02115500e+05, ...,\n",
       "          -2.64844857e-09,  5.94208762e-03,  0.00000000e+00]],\n",
       " \n",
       "        [[ 2.94269531e+02,  2.91275787e+02,  1.01399625e+05, ...,\n",
       "          -8.00355338e-10,  6.20374456e-03,  0.00000000e+00],\n",
       "         [ 2.94136719e+02,  2.91182037e+02,  1.01439125e+05, ...,\n",
       "          -8.00355338e-10,  6.19683042e-03,  0.00000000e+00],\n",
       "         [ 2.93890625e+02,  2.91254303e+02,  1.01475250e+05, ...,\n",
       "          -8.00355338e-10,  6.18622079e-03,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 2.95574219e+02,  2.90769928e+02,  1.01831375e+05, ...,\n",
       "          -8.00355338e-10,  5.60722128e-03,  0.00000000e+00],\n",
       "         [ 2.95650391e+02,  2.90848053e+02,  1.01843000e+05, ...,\n",
       "          -8.00355338e-10,  5.61902300e-03,  0.00000000e+00],\n",
       "         [ 2.95718750e+02,  2.90875397e+02,  1.01855250e+05, ...,\n",
       "          -8.00355338e-10,  5.62891737e-03,  0.00000000e+00]],\n",
       " \n",
       "        [[ 2.94670990e+02,  2.90892212e+02,  1.01235938e+05, ...,\n",
       "          -2.72120815e-09,  5.95810637e-03,  0.00000000e+00],\n",
       "         [ 2.94553802e+02,  2.90939087e+02,  1.01247938e+05, ...,\n",
       "          -2.72120815e-09,  5.95154986e-03,  0.00000000e+00],\n",
       "         [ 2.94516693e+02,  2.90890259e+02,  1.01260062e+05, ...,\n",
       "          -2.72120815e-09,  5.94177470e-03,  0.00000000e+00],\n",
       "         ...,\n",
       "         [ 2.93079193e+02,  2.87925415e+02,  1.01529812e+05, ...,\n",
       "          -2.72120815e-09,  5.60620055e-03,  0.00000000e+00],\n",
       "         [ 2.93128021e+02,  2.87952759e+02,  1.01510438e+05, ...,\n",
       "          -2.72120815e-09,  5.63040003e-03,  0.00000000e+00],\n",
       "         [ 2.93157318e+02,  2.88009399e+02,  1.01494062e+05, ...,\n",
       "          -2.72120815e-09,  5.65674528e-03,  0.00000000e+00]]],\n",
       "       dtype=float32),\n",
       " 'metadata': array([[ -30., -180.],\n",
       "        [ -30., -179.],\n",
       "        [ -30., -178.],\n",
       "        ...,\n",
       "        [  30.,  177.],\n",
       "        [  30.,  178.],\n",
       "        [  30.,  179.]], dtype=float32),\n",
       " 'distances': array([[   0.     ,   96.29746,  192.59308, ..., 6679.2637 , 6675.0654 ,\n",
       "         6672.545  ],\n",
       "        [  96.29746,    0.     ,   96.29746, ..., 6685.1367 , 6679.2637 ,\n",
       "         6675.0654 ],\n",
       "        [ 192.59308,   96.29746,    0.     , ..., 6692.6807 , 6685.1367 ,\n",
       "         6679.2637 ],\n",
       "        ...,\n",
       "        [6679.2637 , 6685.1367 , 6692.6807 , ...,    0.     ,   96.29746,\n",
       "          192.59308],\n",
       "        [6675.0654 , 6679.2637 , 6685.1367 , ...,   96.29746,    0.     ,\n",
       "           96.29746],\n",
       "        [6672.545  , 6675.0654 , 6679.2637 , ...,  192.59308,   96.29746,\n",
       "            0.     ]], dtype=float32)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.     ,   96.29746,  192.59308, ..., 6679.2637 , 6675.0654 ,\n",
       "        6672.545  ],\n",
       "       [  96.29746,    0.     ,   96.29746, ..., 6685.1367 , 6679.2637 ,\n",
       "        6675.0654 ],\n",
       "       [ 192.59308,   96.29746,    0.     , ..., 6692.6807 , 6685.1367 ,\n",
       "        6679.2637 ],\n",
       "       ...,\n",
       "       [6679.2637 , 6685.1367 , 6692.6807 , ...,    0.     ,   96.29746,\n",
       "         192.59308],\n",
       "       [6675.0654 , 6679.2637 , 6685.1367 , ...,   96.29746,    0.     ,\n",
       "          96.29746],\n",
       "       [6672.545  , 6675.0654 , 6679.2637 , ...,  192.59308,   96.29746,\n",
       "           0.     ]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = dataset.compute_similarity(\"distance\")  # or dataset.compute_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.9998158 , 0.9992636 , ..., 0.4123332 , 0.41279253,\n",
       "        0.41306838],\n",
       "       [0.9998158 , 1.        , 0.9998158 , ..., 0.411691  , 0.4123332 ,\n",
       "        0.41279253],\n",
       "       [0.9992636 , 0.9998158 , 1.        , ..., 0.41086674, 0.411691  ,\n",
       "        0.4123332 ],\n",
       "       ...,\n",
       "       [0.4123332 , 0.411691  , 0.41086674, ..., 1.        , 0.9998158 ,\n",
       "        0.9992636 ],\n",
       "       [0.41279253, 0.4123332 , 0.411691  , ..., 0.9998158 , 1.        ,\n",
       "        0.9998158 ],\n",
       "       [0.41306838, 0.41279253, 0.4123332 , ..., 0.9992636 , 0.9998158 ,\n",
       "        1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust connectivity to reduce the number of edges\n",
    "connectivity = dataset.get_connectivity(          \n",
    "    knn=5,     \n",
    "    include_self=False,\n",
    "    normalize_axis=1,\n",
    "    layout=\"edge_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, edge_weight = connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 21959, 21959, 21959],\n",
       "       [    1,   359,   360, ..., 21599, 21600, 21958]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20002224, 0.20002224, 0.20000282, ..., 0.20000282, 0.20002224,\n",
       "       0.20002224], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(matrix):\n",
    "    return pd.DataFrame(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A (21960, 21960):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21950</th>\n",
       "      <th>21951</th>\n",
       "      <th>21952</th>\n",
       "      <th>21953</th>\n",
       "      <th>21954</th>\n",
       "      <th>21955</th>\n",
       "      <th>21956</th>\n",
       "      <th>21957</th>\n",
       "      <th>21958</th>\n",
       "      <th>21959</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21955</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21956</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21957</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21958</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21959</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21960 rows × 21960 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5      6      \\\n",
       "0      0.000000  0.200022  0.000000  0.000000  0.000000  0.000000    0.0   \n",
       "1      0.200022  0.000000  0.200022  0.000000  0.000000  0.000000    0.0   \n",
       "2      0.000000  0.200022  0.000000  0.200022  0.000000  0.000000    0.0   \n",
       "3      0.000000  0.000000  0.200022  0.000000  0.200022  0.000000    0.0   \n",
       "4      0.000000  0.000000  0.000000  0.200022  0.000000  0.200022    0.0   \n",
       "...         ...       ...       ...       ...       ...       ...    ...   \n",
       "21955  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0   \n",
       "21956  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0   \n",
       "21957  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0   \n",
       "21958  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0   \n",
       "21959  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0   \n",
       "\n",
       "       7      8      9      ...  21950  21951  21952  21953     21954  \\\n",
       "0        0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "1        0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "2        0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "3        0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "4        0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "...      ...    ...    ...  ...    ...    ...    ...    ...       ...   \n",
       "21955    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.200022   \n",
       "21956    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "21957    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "21958    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "21959    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0  0.000000   \n",
       "\n",
       "          21955     21956     21957     21958     21959  \n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "21955  0.000000  0.200022  0.000000  0.000000  0.000000  \n",
       "21956  0.200022  0.000000  0.200022  0.000000  0.000000  \n",
       "21957  0.000000  0.200022  0.000000  0.200022  0.000000  \n",
       "21958  0.000000  0.000000  0.200022  0.000000  0.200022  \n",
       "21959  0.000000  0.000000  0.000000  0.200022  0.000000  \n",
       "\n",
       "[21960 rows x 21960 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tsl.ops.connectivity import edge_index_to_adj\n",
    "\n",
    "adj = edge_index_to_adj(edge_index, edge_weight)\n",
    "print(f'A {adj.shape}:')\n",
    "print_matrix(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.data import SpatioTemporalDataset\n",
    "\n",
    "# covariates=dict(u=dataset.covariates['u'])\n",
    "covariates=dataset.covariates\n",
    "mask = dataset.mask\n",
    "\n",
    "torch_dataset = SpatioTemporalDataset(target=dataset.dataframe(),\n",
    "                                      mask=mask,\n",
    "                                      covariates=covariates,\n",
    "                                      connectivity=connectivity,\n",
    "                                      horizon=6, # Predict 7 step ahead\n",
    "                                      window=12, # Use 30 timestamps to predict the next one\n",
    "                                      stride=1 # Move 7 step forward each time\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]],\n",
       "\n",
       "        [[True],\n",
       "         [True],\n",
       "         [True],\n",
       "         ...,\n",
       "         [True],\n",
       "         [True],\n",
       "         [True]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dataset.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_dataset.update_input_map(input_mask=['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.data.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scalers = {\n",
    "    'target': MinMaxScaler(axis=(0, 1)),\n",
    "    'u': MinMaxScaler(axis=(0, 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.data.datamodule import (SpatioTemporalDataModule,\n",
    "                                 TemporalSplitter)\n",
    "                                 \n",
    "# Split data sequentially:\n",
    "#   |------------ dataset -----------|\n",
    "#   |--- train ---|- val -|-- test --|\n",
    "splitter = TemporalSplitter(val_len=0.35, test_len=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatioTemporalDataModule(train_len=None, val_len=None, test_len=None, scalers=[target, u], batch_size=8)\n"
     ]
    }
   ],
   "source": [
    "# Create a SpatioTemporalDataModule\n",
    "datamodule = SpatioTemporalDataModule(\n",
    "    dataset=torch_dataset,\n",
    "    scalers=scalers,\n",
    "    mask_scaling=True,\n",
    "    splitter=splitter,\n",
    "    batch_size=8,\n",
    "    workers=15,\n",
    "    )\n",
    "\n",
    "print(datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatioTemporalDataModule(train_len=141, val_len=69, test_len=25, scalers=[target, u], batch_size=8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]),\n",
       " 'val': array([153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
       "        166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
       "        179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n",
       "        192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
       "        205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
       "        218, 219, 220, 221]),\n",
       " 'test': array([234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.splitter.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from tsl.nn.blocks import RNNBase\n",
    "from tsl.nn.layers import Dense, GraphGRUCellBase, Activation\n",
    "\n",
    "\n",
    "class GraphAnisoConv(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 1,\n",
    "                 edge_dim: Optional[int] = None,\n",
    "                 activation: str = 'leaky_relu'):\n",
    "        super(GraphAnisoConv, self).__init__(aggr=\"add\", node_dim=-2)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.msg_mlps = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(2 * (in_channels if i == 0 else out_channels),\n",
    "                          out_channels),\n",
    "                Activation(activation),\n",
    "                nn.Linear(out_channels, out_channels),\n",
    "            )\n",
    "            for i in range(kernel_size)\n",
    "        ])\n",
    "\n",
    "        edge_dim = edge_dim or 1  # accommodate for edge_weight\n",
    "        self.lin_edge = nn.Linear(edge_dim, out_channels, bias=False)\n",
    "\n",
    "        self.gate_mlp = Dense(out_channels, 1, activation='sigmoid')\n",
    "\n",
    "        self.skip_conn = nn.Linear(in_channels, out_channels)\n",
    "        self.activation = Activation(activation)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr: Optional[Tensor] = None):\n",
    "        \"\"\"\"\"\"\n",
    "        out, x_ = 0, x\n",
    "        for idx in range(self.kernel_size):\n",
    "            x_ = self.propagate(edge_index, idx=idx, x=x_, edge_attr=edge_attr)\n",
    "            out += x_\n",
    "        out = self.activation(out + self.skip_conn(x))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, idx, edge_attr: Optional[Tensor] = None):\n",
    "        mij = self.msg_mlps[idx](torch.cat([x_i, x_j], -1))\n",
    "        if edge_attr is not None:\n",
    "            if edge_attr.ndim == 1:  # accommodate for edge_weight\n",
    "                edge_attr = edge_attr.view(-1, 1)\n",
    "            mij = mij + self.lin_edge(edge_attr)\n",
    "        return self.gate_mlp(mij) * mij\n",
    "\n",
    "\n",
    "class GraphAnisoGRUCell(GraphGRUCellBase):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int,\n",
    "                 edge_dim: Optional[int] = None,\n",
    "                 activation: str = 'leaky_relu'):\n",
    "        self.input_size = input_size\n",
    "        # instantiate gates\n",
    "        forget_gate = GraphAnisoConv(input_size + hidden_size, hidden_size,\n",
    "                                     edge_dim=edge_dim, activation=activation)\n",
    "        update_gate = GraphAnisoConv(input_size + hidden_size, hidden_size,\n",
    "                                     edge_dim=edge_dim, activation=activation)\n",
    "        candidate_gate = GraphAnisoConv(input_size + hidden_size, hidden_size,\n",
    "                                        edge_dim=edge_dim,\n",
    "                                        activation=activation)\n",
    "        super(GraphAnisoGRUCell, self).__init__(hidden_size=hidden_size,\n",
    "                                                forget_gate=forget_gate,\n",
    "                                                update_gate=update_gate,\n",
    "                                                candidate_gate=candidate_gate)\n",
    "\n",
    "\n",
    "class GraphAnisoGRU(RNNBase):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int,\n",
    "                 edge_dim: Optional[int] = None,\n",
    "                 n_layers: int = 1, cat_states_layers: bool = False,\n",
    "                 return_only_last_state: bool = False,\n",
    "                 activation: str = 'leaky_relu'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        rnn_cells = [\n",
    "            GraphAnisoGRUCell(input_size if i == 0 else hidden_size,\n",
    "                              hidden_size, edge_dim=edge_dim,\n",
    "                              activation=activation)\n",
    "            for i in range(n_layers)\n",
    "        ]\n",
    "        super(GraphAnisoGRU, self).__init__(rnn_cells, cat_states_layers,\n",
    "                                            return_only_last_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch_geometric.typing import Adj\n",
    "from tsl.nn.blocks import MLPDecoder\n",
    "from tsl.nn.layers import MultiLinear, NodeEmbedding\n",
    "from tsl.nn.models import BaseModel\n",
    "from tsl.nn.utils import maybe_cat_exog\n",
    "from tsl.utils import ensure_list\n",
    "\n",
    "\n",
    "def maybe_cat_emb(x: Tensor, emb: Optional[Tensor]):\n",
    "    if emb is None:\n",
    "        return x\n",
    "    if emb.ndim < x.ndim:\n",
    "        emb = emb[[None] * (x.ndim - emb.ndim)]\n",
    "    emb = emb.expand(*x.shape[:-1], -1)\n",
    "    return torch.cat([x, emb], dim=-1)\n",
    "\n",
    "\n",
    "class STGNN(BaseModel):\n",
    "    available_embedding_pos = {'encoding', 'decoding'}\n",
    "\n",
    "    def __init__(self, input_size: int, horizon: int,\n",
    "                 n_nodes: int = None,\n",
    "                 output_size: int = None,\n",
    "                 exog_size: int = 0,\n",
    "                 hidden_size: int = 32,\n",
    "                 emb_size: int = 0,\n",
    "                 add_embedding_before: Optional[\n",
    "                     Union[str, List[str]]] = 'encoding',\n",
    "                 use_local_weights: Union[str, List[str]] = None,\n",
    "                 activation: str = 'elu'):\n",
    "        super(STGNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.horizon = horizon\n",
    "        self.n_nodes = n_nodes\n",
    "        self.output_size = output_size or input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.exog_size = exog_size\n",
    "        self.activation = activation\n",
    "\n",
    "        # EMBEDDING\n",
    "        if add_embedding_before is None:\n",
    "            add_embedding_before = set()\n",
    "        else:\n",
    "            add_embedding_before = set(ensure_list(add_embedding_before))\n",
    "            if not add_embedding_before.issubset(self.available_embedding_pos):\n",
    "                raise ValueError(\"Parameter 'add_embedding_before' must be a \"\n",
    "                                 f\"subset of {self.available_embedding_pos}\")\n",
    "        self.add_embedding_before = add_embedding_before\n",
    "\n",
    "        if emb_size > 0:\n",
    "            self.emb = NodeEmbedding(n_nodes, emb_size)\n",
    "        else:\n",
    "            self.register_module('emb', None)\n",
    "\n",
    "        # ENCODER\n",
    "        self.encoder_input = input_size + exog_size\n",
    "        if 'encoding' in self.add_embedding_before and self.emb is not None:\n",
    "            self.encoder_input += emb_size\n",
    "\n",
    "        if use_local_weights is not None:\n",
    "            self.use_local_weights = set(ensure_list(use_local_weights))\n",
    "            if len(self.use_local_weights.difference(['encoder', 'decoder'])):\n",
    "                raise ValueError(\"Parameter 'use_local_weights' must be \"\n",
    "                                 \"'encoder', 'decoder', or both.\")\n",
    "        else:\n",
    "            self.use_local_weights = set()\n",
    "\n",
    "        if 'encoder' in self.use_local_weights:\n",
    "            self.encoder = MultiLinear(self.encoder_input, hidden_size, n_nodes)\n",
    "        else:\n",
    "            self.encoder = nn.Linear(self.encoder_input, hidden_size)\n",
    "\n",
    "        # DECODER\n",
    "        self.decoder_input = hidden_size\n",
    "        if 'decoding' in self.add_embedding_before and self.emb is not None:\n",
    "            self.decoder_input += emb_size\n",
    "        if 'decoder' in self.use_local_weights:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            self.decoder = MLPDecoder(input_size=self.decoder_input,\n",
    "                                      hidden_size=self.hidden_size,\n",
    "                                      output_size=self.output_size,\n",
    "                                      horizon=self.horizon,\n",
    "                                      activation=self.activation)\n",
    "\n",
    "    def stmp(self, x: Tensor, edge_index: Adj,\n",
    "             edge_weight: Optional[Tensor] = None,\n",
    "             emb: Optional[Tensor] = None) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: Optional[Tensor] = None,\n",
    "                u: Optional[Tensor] = None,\n",
    "                node_idx: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        # x: [batches steps nodes features]\n",
    "        x = maybe_cat_exog(x, u)\n",
    "        batch_size = x.size(0)\n",
    "        emb = self.emb(expand=(batch_size, -1, -1),\n",
    "                       node_index=node_idx) if self.emb is not None else None\n",
    "\n",
    "        if 'encoding' in self.add_embedding_before and emb is not None:\n",
    "            x = maybe_cat_emb(x, emb[:, None])\n",
    "\n",
    "        # ENCODER   ###########################################################\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # SPATIOTEMPORAL MESSAGE-PASSING   ####################################\n",
    "        out = self.stmp(x, edge_index, edge_weight, emb)\n",
    "\n",
    "        # DECODER   ###########################################################\n",
    "        if 'decoding' in self.add_embedding_before:\n",
    "            out = maybe_cat_emb(out, emb)\n",
    "\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TimeThenSpace(STGNN):\n",
    "    available_embedding_pos = {'encoding', 'message_passing', 'decoding'}\n",
    "\n",
    "    def __init__(self, input_size: int, horizon: int,\n",
    "                 temporal_encoder: nn.Module,\n",
    "                 spatial_encoder: Union[nn.Module, List[nn.Module]],\n",
    "                 n_nodes: int = None,\n",
    "                 output_size: int = None,\n",
    "                 exog_size: int = 0,\n",
    "                 hidden_size: int = 32,\n",
    "                 emb_size: int = 0,\n",
    "                 add_embedding_before: Union[str, List[str]] = 'encoding',\n",
    "                 use_local_weights: Union[str, List[str]] = None,\n",
    "                 activation: str = 'elu'):\n",
    "        super(TimeThenSpace, self).__init__(input_size=input_size,\n",
    "                                            horizon=horizon,\n",
    "                                            n_nodes=n_nodes,\n",
    "                                            output_size=output_size,\n",
    "                                            exog_size=exog_size,\n",
    "                                            hidden_size=hidden_size,\n",
    "                                            emb_size=emb_size,\n",
    "                                            add_embedding_before=add_embedding_before,\n",
    "                                            use_local_weights=use_local_weights,\n",
    "                                            activation=activation)\n",
    "        # STMP\n",
    "        self.temporal_encoder = temporal_encoder\n",
    "        if not isinstance(spatial_encoder, nn.ModuleList):\n",
    "            spatial_encoder = nn.ModuleList(ensure_list(spatial_encoder))\n",
    "        self.mp_layers = spatial_encoder\n",
    "        self.spatial_layers = len(self.mp_layers)\n",
    "\n",
    "    def stmp(self, x: Tensor, edge_index: Adj,\n",
    "             edge_weight: Optional[Tensor] = None,\n",
    "             emb: Optional[Tensor] = None) -> Tensor:\n",
    "        # temporal encoding\n",
    "        out = self.temporal_encoder(x)\n",
    "        # spatial encoding\n",
    "        for layer in self.mp_layers:\n",
    "            if 'message_passing' in self.add_embedding_before:\n",
    "                out = maybe_cat_emb(out, emb)\n",
    "            out = layer(out, edge_index, edge_weight)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TimeAndSpace(STGNN):\n",
    "\n",
    "    def __init__(self, input_size: int, horizon: int, stmp_conv: nn.Module,\n",
    "                 n_nodes: int = None,\n",
    "                 output_size: int = None,\n",
    "                 exog_size: int = 0,\n",
    "                 hidden_size: int = 32,\n",
    "                 emb_size: int = 0,\n",
    "                 add_embedding_before: Union[str, List[str]] = 'encoding',\n",
    "                 use_local_weights: Union[str, List[str]] = None,\n",
    "                 activation: str = 'elu'):\n",
    "        super(TimeAndSpace, self).__init__(input_size=input_size,\n",
    "                                           horizon=horizon,\n",
    "                                           n_nodes=n_nodes,\n",
    "                                           output_size=output_size,\n",
    "                                           exog_size=exog_size,\n",
    "                                           hidden_size=hidden_size,\n",
    "                                           emb_size=emb_size,\n",
    "                                           add_embedding_before=add_embedding_before,\n",
    "                                           use_local_weights=use_local_weights,\n",
    "                                           activation=activation)\n",
    "\n",
    "        # STMP\n",
    "        self.stmp_conv = stmp_conv\n",
    "\n",
    "    def stmp(self, x: Tensor, edge_index: Adj,\n",
    "             edge_weight: Optional[Tensor] = None,\n",
    "             emb: Optional[Tensor] = None) -> Tensor:\n",
    "        # spatiotemporal encoding\n",
    "        out = self.stmp_conv(x, edge_index, edge_weight)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "class TimeAndGraphAnisoModel(TimeAndSpace):\n",
    "\n",
    "    def __init__(self, input_size: int, horizon: int, n_nodes: int = None,\n",
    "                 output_size: int = None,\n",
    "                 exog_size: int = 0,\n",
    "                 hidden_size: int = 32,\n",
    "                 emb_size: int = 0,\n",
    "                 add_embedding_before: Union[str, List[str]] = 'encoding',\n",
    "                 use_local_weights: Union[str, List[str]] = None,\n",
    "                 n_layers: int = 1,\n",
    "                 activation: str = 'elu'):\n",
    "        stmp_conv = GraphAnisoGRU(input_size=hidden_size,\n",
    "                                  hidden_size=hidden_size,\n",
    "                                  n_layers=n_layers,\n",
    "                                  activation=activation,\n",
    "                                  return_only_last_state=True)\n",
    "        super(TimeAndGraphAnisoModel, self).__init__(\n",
    "            input_size=input_size,\n",
    "            horizon=horizon,\n",
    "            stmp_conv=stmp_conv,\n",
    "            n_nodes=n_nodes,\n",
    "            output_size=output_size,\n",
    "            exog_size=exog_size,\n",
    "            hidden_size=hidden_size,\n",
    "            emb_size=emb_size,\n",
    "            add_embedding_before=add_embedding_before,\n",
    "            use_local_weights=use_local_weights,\n",
    "            activation=activation\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32          # Number of hidden units\n",
    "ff_size = 64             # Number of units in the feed-forward layers\n",
    "n_layers = 3              # Number of SpatioTemporalConvNet blocks\n",
    "temporal_kernel_size = 3  # Size of the temporal convolution kernel\n",
    "spatial_kernel_size = 3   # Order of the spatial diffusion process\n",
    "norm='layer'\n",
    "gated=True\n",
    "\n",
    "input_size = torch_dataset.n_channels\n",
    "n_nodes = torch_dataset.n_nodes\n",
    "horizon = torch_dataset.horizon\n",
    "exog_size = torch_dataset.input_map.u.shape[-1] if 'u' in torch_dataset else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeAndGraphAnisoModel(\n",
      "  (emb): None\n",
      "  (encoder): Linear(in_features=21, out_features=32, bias=True)\n",
      "  (decoder): MLPDecoder(\n",
      "    (readout): MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Dense(\n",
      "          (affinity): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (activation): ELU(alpha=1.0)\n",
      "          (dropout): Identity()\n",
      "        )\n",
      "      )\n",
      "      (readout): Linear(in_features=32, out_features=6, bias=True)\n",
      "    )\n",
      "    (rearrange): Rearrange('b n (h f) -> b h n f', f=1, h=6)\n",
      "  )\n",
      "  (stmp_conv): GraphAnisoGRU(cell=GraphAnisoGRUCell, return_only_last_state=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TimeAndGraphAnisoModel(\n",
    "    input_size=input_size,\n",
    "    horizon=horizon,\n",
    "    n_nodes=n_nodes,\n",
    "    output_size=input_size,\n",
    "    exog_size=exog_size,\n",
    "    hidden_size=hidden_size\n",
    ")\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(model):\n",
    "    tot = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "    out = f\"Number of model ({model.__class__.__name__}) parameters:{tot:10d}\"\n",
    "    print(\"=\" * len(out))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "Number of model (TimeAndGraphAnisoModel) parameters:     23945\n"
     ]
    }
   ],
   "source": [
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsl.metrics.torch import MaskedMSE, MaskedMAE, MaskedMAPE\n",
    "from tsl.engines import Predictor\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = MaskedMSE()\n",
    "\n",
    "# Setup metrics\n",
    "metrics = {\n",
    "    'mse': MaskedMSE(),\n",
    "    'mae': MaskedMAE(),\n",
    "    'mape': MaskedMAPE(),\n",
    "    'mse_at_3': MaskedMSE(at=2),  # '2' indicates the third time step\n",
    "    'mse_at_6': MaskedMSE(at=5)\n",
    "}\n",
    "\n",
    "# Setup predictor\n",
    "predictor = Predictor(\n",
    "    model=model,\n",
    "    optim_class=torch.optim.Adam,\n",
    "    optim_kwargs={'lr': 0.001},\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_mse',\n",
    "    patience=30,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='logs',\n",
    "    save_top_k=1,\n",
    "    monitor='val_mse',  # Change this to 'val_mse'\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Setup trainer\n",
    "trainer = pl.Trainer(max_epochs=100,\n",
    "                    #  logger=logger,\n",
    "                    #  limit_train_batches=100,  # end an epoch after 200 updates\n",
    "                     callbacks=[early_stop_callback, checkpoint_callback],\n",
    "                     log_every_n_steps=2,\n",
    "                     gradient_clip_val=1.0,    # Prevent exploding gradients\n",
    "                     precision=16\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set float32 matmul precision to 'medium' or 'high'\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# multiprocessing.set_start_method('spawn', force=True)\n",
    "# trainer.fit(predictor, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_callback.best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tsl/engines/predictor.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  storage = torch.load(filename, lambda storage, loc: storage)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tsl/engines/predictor.py:121\u001b[0m, in \u001b[0;36mPredictor.load_model\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load model's weights from checkpoint at :attr:`filename`.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Differently from\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    checkpoint's model are the same of the predictor's model.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# if predictor.model has been instantiated inside predictor\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "predictor.load_model(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2048/2641369607.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/teamspace/studios/this_studio/logs/epoch=1-step=78.ckpt')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TimeAndGraphAnisoModel:\n\tMissing key(s) in state_dict: \"encoder.weight\", \"encoder.bias\", \"decoder.readout.mlp.0.affinity.weight\", \"decoder.readout.mlp.0.affinity.bias\", \"decoder.readout.readout.weight\", \"decoder.readout.readout.bias\", \"stmp_conv.cells.0.forget_gate.msg_mlps.0.0.weight\", \"stmp_conv.cells.0.forget_gate.msg_mlps.0.0.bias\", \"stmp_conv.cells.0.forget_gate.msg_mlps.0.2.weight\", \"stmp_conv.cells.0.forget_gate.msg_mlps.0.2.bias\", \"stmp_conv.cells.0.forget_gate.lin_edge.weight\", \"stmp_conv.cells.0.forget_gate.gate_mlp.affinity.weight\", \"stmp_conv.cells.0.forget_gate.gate_mlp.affinity.bias\", \"stmp_conv.cells.0.forget_gate.skip_conn.weight\", \"stmp_conv.cells.0.forget_gate.skip_conn.bias\", \"stmp_conv.cells.0.update_gate.msg_mlps.0.0.weight\", \"stmp_conv.cells.0.update_gate.msg_mlps.0.0.bias\", \"stmp_conv.cells.0.update_gate.msg_mlps.0.2.weight\", \"stmp_conv.cells.0.update_gate.msg_mlps.0.2.bias\", \"stmp_conv.cells.0.update_gate.lin_edge.weight\", \"stmp_conv.cells.0.update_gate.gate_mlp.affinity.weight\", \"stmp_conv.cells.0.update_gate.gate_mlp.affinity.bias\", \"stmp_conv.cells.0.update_gate.skip_conn.weight\", \"stmp_conv.cells.0.update_gate.skip_conn.bias\", \"stmp_conv.cells.0.candidate_gate.msg_mlps.0.0.weight\", \"stmp_conv.cells.0.candidate_gate.msg_mlps.0.0.bias\", \"stmp_conv.cells.0.candidate_gate.msg_mlps.0.2.weight\", \"stmp_conv.cells.0.candidate_gate.msg_mlps.0.2.bias\", \"stmp_conv.cells.0.candidate_gate.lin_edge.weight\", \"stmp_conv.cells.0.candidate_gate.gate_mlp.affinity.weight\", \"stmp_conv.cells.0.candidate_gate.gate_mlp.affinity.bias\", \"stmp_conv.cells.0.candidate_gate.skip_conn.weight\", \"stmp_conv.cells.0.candidate_gate.skip_conn.bias\". \n\tUnexpected key(s) in state_dict: \"model.encoder.weight\", \"model.encoder.bias\", \"model.decoder.readout.mlp.0.affinity.weight\", \"model.decoder.readout.mlp.0.affinity.bias\", \"model.decoder.readout.readout.weight\", \"model.decoder.readout.readout.bias\", \"model.stmp_conv.cells.0.forget_gate.msg_mlps.0.0.weight\", \"model.stmp_conv.cells.0.forget_gate.msg_mlps.0.0.bias\", \"model.stmp_conv.cells.0.forget_gate.msg_mlps.0.2.weight\", \"model.stmp_conv.cells.0.forget_gate.msg_mlps.0.2.bias\", \"model.stmp_conv.cells.0.forget_gate.lin_edge.weight\", \"model.stmp_conv.cells.0.forget_gate.gate_mlp.affinity.weight\", \"model.stmp_conv.cells.0.forget_gate.gate_mlp.affinity.bias\", \"model.stmp_conv.cells.0.forget_gate.skip_conn.weight\", \"model.stmp_conv.cells.0.forget_gate.skip_conn.bias\", \"model.stmp_conv.cells.0.update_gate.msg_mlps.0.0.weight\", \"model.stmp_conv.cells.0.update_gate.msg_mlps.0.0.bias\", \"model.stmp_conv.cells.0.update_gate.msg_mlps.0.2.weight\", \"model.stmp_conv.cells.0.update_gate.msg_mlps.0.2.bias\", \"model.stmp_conv.cells.0.update_gate.lin_edge.weight\", \"model.stmp_conv.cells.0.update_gate.gate_mlp.affinity.weight\", \"model.stmp_conv.cells.0.update_gate.gate_mlp.affinity.bias\", \"model.stmp_conv.cells.0.update_gate.skip_conn.weight\", \"model.stmp_conv.cells.0.update_gate.skip_conn.bias\", \"model.stmp_conv.cells.0.candidate_gate.msg_mlps.0.0.weight\", \"model.stmp_conv.cells.0.candidate_gate.msg_mlps.0.0.bias\", \"model.stmp_conv.cells.0.candidate_gate.msg_mlps.0.2.weight\", \"model.stmp_conv.cells.0.candidate_gate.msg_mlps.0.2.bias\", \"model.stmp_conv.cells.0.candidate_gate.lin_edge.weight\", \"model.stmp_conv.cells.0.candidate_gate.gate_mlp.affinity.weight\", \"model.stmp_conv.cells.0.candidate_gate.gate_mlp.affinity.bias\", \"model.stmp_conv.cells.0.candidate_gate.skip_conn.weight\", \"model.stmp_conv.cells.0.candidate_gate.skip_conn.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/logs/epoch=1-step=78.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model's state_dict\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TimeAndGraphAnisoModel:\n\tMissing key(s) in state_dict: \"encoder.weight\", \"encoder.bias\", \"decoder.readout.mlp.0.affinity.weight\", \"decoder.readout.mlp.0.affinity.bias\", \"decoder.readout.readout.weight\", \"decoder.readout.readout.bias\", \"stmp_conv.cells.0.forget_gate.msg_mlps.0.0.weight\", \"stmp_conv.cells.0.forget_gate.msg_mlps.0.0.bias\", \"stmp_conv.cells.0.forget_gate.msg_mlps.0.2.weight\", \"stmp_conv.cells.0.forget_gate.msg_mlps.0.2.bias\", \"stmp_conv.cells.0.forget_gate.lin_edge.weight\", \"stmp_conv.cells.0.forget_gate.gate_mlp.affinity.weight\", \"stmp_conv.cells.0.forget_gate.gate_mlp.affinity.bias\", \"stmp_conv.cells.0.forget_gate.skip_conn.weight\", \"stmp_conv.cells.0.forget_gate.skip_conn.bias\", \"stmp_conv.cells.0.update_gate.msg_mlps.0.0.weight\", \"stmp_conv.cells.0.update_gate.msg_mlps.0.0.bias\", \"stmp_conv.cells.0.update_gate.msg_mlps.0.2.weight\", \"stmp_conv.cells.0.update_gate.msg_mlps.0.2.bias\", \"stmp_conv.cells.0.update_gate.lin_edge.weight\", \"stmp_conv.cells.0.update_gate.gate_mlp.affinity.weight\", \"stmp_conv.cells.0.update_gate.gate_mlp.affinity.bias\", \"stmp_conv.cells.0.update_gate.skip_conn.weight\", \"stmp_conv.cells.0.update_gate.skip_conn.bias\", \"stmp_conv.cells.0.candidate_gate.msg_mlps.0.0.weight\", \"stmp_conv.cells.0.candidate_gate.msg_mlps.0.0.bias\", \"stmp_conv.cells.0.candidate_gate.msg_mlps.0.2.weight\", \"stmp_conv.cells.0.candidate_gate.msg_mlps.0.2.bias\", \"stmp_conv.cells.0.candidate_gate.lin_edge.weight\", \"stmp_conv.cells.0.candidate_gate.gate_mlp.affinity.weight\", \"stmp_conv.cells.0.candidate_gate.gate_mlp.affinity.bias\", \"stmp_conv.cells.0.candidate_gate.skip_conn.weight\", \"stmp_conv.cells.0.candidate_gate.skip_conn.bias\". \n\tUnexpected key(s) in state_dict: \"model.encoder.weight\", \"model.encoder.bias\", \"model.decoder.readout.mlp.0.affinity.weight\", \"model.decoder.readout.mlp.0.affinity.bias\", \"model.decoder.readout.readout.weight\", \"model.decoder.readout.readout.bias\", \"model.stmp_conv.cells.0.forget_gate.msg_mlps.0.0.weight\", \"model.stmp_conv.cells.0.forget_gate.msg_mlps.0.0.bias\", \"model.stmp_conv.cells.0.forget_gate.msg_mlps.0.2.weight\", \"model.stmp_conv.cells.0.forget_gate.msg_mlps.0.2.bias\", \"model.stmp_conv.cells.0.forget_gate.lin_edge.weight\", \"model.stmp_conv.cells.0.forget_gate.gate_mlp.affinity.weight\", \"model.stmp_conv.cells.0.forget_gate.gate_mlp.affinity.bias\", \"model.stmp_conv.cells.0.forget_gate.skip_conn.weight\", \"model.stmp_conv.cells.0.forget_gate.skip_conn.bias\", \"model.stmp_conv.cells.0.update_gate.msg_mlps.0.0.weight\", \"model.stmp_conv.cells.0.update_gate.msg_mlps.0.0.bias\", \"model.stmp_conv.cells.0.update_gate.msg_mlps.0.2.weight\", \"model.stmp_conv.cells.0.update_gate.msg_mlps.0.2.bias\", \"model.stmp_conv.cells.0.update_gate.lin_edge.weight\", \"model.stmp_conv.cells.0.update_gate.gate_mlp.affinity.weight\", \"model.stmp_conv.cells.0.update_gate.gate_mlp.affinity.bias\", \"model.stmp_conv.cells.0.update_gate.skip_conn.weight\", \"model.stmp_conv.cells.0.update_gate.skip_conn.bias\", \"model.stmp_conv.cells.0.candidate_gate.msg_mlps.0.0.weight\", \"model.stmp_conv.cells.0.candidate_gate.msg_mlps.0.0.bias\", \"model.stmp_conv.cells.0.candidate_gate.msg_mlps.0.2.weight\", \"model.stmp_conv.cells.0.candidate_gate.msg_mlps.0.2.bias\", \"model.stmp_conv.cells.0.candidate_gate.lin_edge.weight\", \"model.stmp_conv.cells.0.candidate_gate.gate_mlp.affinity.weight\", \"model.stmp_conv.cells.0.candidate_gate.gate_mlp.affinity.bias\", \"model.stmp_conv.cells.0.candidate_gate.skip_conn.weight\", \"model.stmp_conv.cells.0.candidate_gate.skip_conn.bias\". "
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('/teamspace/studios/this_studio/logs/epoch=1-step=78.ckpt')\n",
    "\n",
    "# Load the model's state_dict\n",
    "predictor.model.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the checkpoint file\n",
    "checkpoint_path = Path(\"/teamspace/studios/this_studio/logs/epoch=1-step=78.ckpt\")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Assuming you have a model and optimizer\n",
    "model = YourModel()  # Replace with your model class\n",
    "optimizer = torch.optim.Adam(model.parameters())  # Replace with your optimizer\n",
    "\n",
    "# Load the model state dict and optimizer state dict\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "if 'optimizer' in checkpoint:  # Check if the checkpoint includes optimizer state\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# Optionally, load the epoch and step information\n",
    "epoch = checkpoint.get('epoch', None)\n",
    "step = checkpoint.get('step', None)\n",
    "\n",
    "print(f\"Model and optimizer loaded. Epoch: {epoch}, Step: {step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2048/3928688942.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MyModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming the model and optimizer were saved, restore them\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# You would have to define your model architecture first\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMyModel\u001b[49m()  \u001b[38;5;66;03m# Replace with your actual model class\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Replace 'state_dict' if your key is different\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# If optimizer state was saved, you can also load it like this:\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyModel' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to the checkpoint\n",
    "checkpoint_path = '/teamspace/studios/this_studio/logs/epoch=98-step=891.ckpt'\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Assuming the model and optimizer were saved, restore them\n",
    "# You would have to define your model architecture first\n",
    "model = MyModel()  # Replace with your actual model class\n",
    "model.load_state_dict(checkpoint['state_dict'])  # Replace 'state_dict' if your key is different\n",
    "\n",
    "# If optimizer state was saved, you can also load it like this:\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# If you stored other information (like epoch), you can retrieve that as well:\n",
    "epoch = checkpoint['epoch']\n",
    "step = checkpoint['step']\n",
    "\n",
    "print(f\"Model restored from epoch {epoch}, step {step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec47dbc71d345b6a3e13f1ef7d8e211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arguments ['distances', 'metadata'] are filtered out. Only args ['edge_weight', 'x', 'u', 'edge_index'] are forwarded to the model (TimeAndGraphAnisoModel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     669.087158203125      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    18.043628692626953     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mape         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        741112.3125        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     669.087158203125      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_mse_at_3       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1451.0916748046875     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_mse_at_6       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1056.11474609375      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    669.087158203125     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   18.043628692626953    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mape        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       741112.3125       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    669.087158203125     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_mse_at_3      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1451.0916748046875    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_mse_at_6      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1056.11474609375     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_mae': 18.043628692626953,\n",
       "  'test_mape': 741112.3125,\n",
       "  'test_mse': 669.087158203125,\n",
       "  'test_mse_at_3': 1451.0916748046875,\n",
       "  'test_mse_at_6': 1056.11474609375,\n",
       "  'test_loss': 669.087158203125}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(predictor, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fbb5ed1d334fbe8eb1909fe28b03d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     22.19852638244629     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.7898164987564087     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_mape          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      45550.22265625       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    22.198528289794922     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_mse_at_3        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    22.995407104492188     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_mse_at_6        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     26.82210350036621     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    22.19852638244629    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.7898164987564087    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_mape         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     45550.22265625      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   22.198528289794922    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_mse_at_3       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   22.995407104492188    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_mse_at_6       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    26.82210350036621    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_mae': 1.7898164987564087,\n",
       "  'val_mape': 45550.22265625,\n",
       "  'val_mse': 22.198528289794922,\n",
       "  'val_mse_at_3': 22.995407104492188,\n",
       "  'val_mse_at_6': 26.82210350036621,\n",
       "  'val_loss': 22.19852638244629}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(predictor, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfbd1bbee474a1c8004bf164bdf5f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_test = trainer.predict(predictor, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]),\n",
       " 'val': array([153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
       "        166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
       "        179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n",
       "        192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
       "        205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
       "        218, 219, 220, 221]),\n",
       " 'test': array([234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(ds['date'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2000-01-01', '2000-02-01', '2000-03-01', '2000-04-01',\n",
       "               '2000-05-01', '2000-06-01', '2000-07-01', '2000-08-01',\n",
       "               '2000-09-01', '2000-10-01',\n",
       "               ...\n",
       "               '2022-03-01', '2022-04-01', '2022-05-01', '2022-06-01',\n",
       "               '2022-07-01', '2022-08-01', '2022-09-01', '2022-10-01',\n",
       "               '2022-11-01', '2022-12-01'],\n",
       "              dtype='datetime64[ns]', length=276, freq=None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140]),\n",
       " 'val': array([153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
       "        166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
       "        179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n",
       "        192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
       "        205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
       "        218, 219, 220, 221]),\n",
       " 'test': array([234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258])}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.splitter.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140]\n",
      "Validation indices: [153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170\n",
      " 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188\n",
      " 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206\n",
      " 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221]\n",
      "Test indices: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258]\n",
      "Shape of y: (25, 6, 21960)\n",
      "Shape of y_hat: (25, 6, 21960)\n",
      "Shape of mask: (25, 6, 21960)\n",
      "Shape of node_ids: (3294000,)\n",
      "Shape of lat_column: (3294000,)\n",
      "Shape of lon_column: (3294000,)\n",
      "Shape of test_dates_steps: (150,)\n",
      "Shape of all_dates_expanded: (3294000,)\n",
      "Shape of y_flat: (3294000,)\n",
      "Shape of y_hat_flat: (3294000,)\n",
      "Shape of mask_flat: (3294000,)\n",
      "Shape of filtered_dates: (3294000,)\n",
      "Shape of filtered_lat: (3294000,)\n",
      "Shape of filtered_lon: (3294000,)\n",
      "Shape of filtered_y: (3294000,)\n",
      "Shape of filtered_y_hat: (3294000,)\n",
      "Shape of filtered_node_ids: (3294000,)\n",
      "        date   lat    lon  node_id    y     y_hat\n",
      "0 2019-07-01 -30.0 -180.0        0  0.0 -0.333466\n",
      "1 2019-07-01 -30.0 -179.0        1 -1.0 -0.811117\n",
      "2 2019-07-01 -30.0 -178.0        2 -1.0 -0.832692\n",
      "3 2019-07-01 -30.0 -177.0        3 -1.0 -0.864189\n",
      "4 2019-07-01 -30.0 -176.0        4 -1.0 -0.835293\n",
      "(3294000, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import xarray as xr\n",
    "\n",
    "# 1. Load Metadata and Dates\n",
    "metadata_array = np.load('ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/metadata.npy')  # Shape: (21960, 2)\n",
    "latitudes = metadata_array[:, 0]\n",
    "longitudes = metadata_array[:, 1]\n",
    "num_nodes = len(latitudes)\n",
    "\n",
    "# Load the original dataset\n",
    "# ds = xr.open_dataset('path_to_your_dataset.nc')  # Replace with your actual path\n",
    "dates = pd.to_datetime(ds['date'].values)\n",
    "total_time_steps = len(dates)\n",
    "\n",
    "# Initialize splitter\n",
    "# splitter = TemporalSplitter(val_len=0.35, test_len=0.1)\n",
    "\n",
    "# Assume you have a SpatioTemporalDataset instance\n",
    "# dataset = your_spatio_temporal_dataset  # Replace with your actual dataset variable\n",
    "\n",
    "# Fit the splitter to the dataset\n",
    "# splitter.fit(dataset)\n",
    "\n",
    "# 2. Correctly Retrieve the Split Indices\n",
    "split_indices = datamodule.splitter.indices\n",
    "train_indices = split_indices['train']\n",
    "val_indices = split_indices['val']\n",
    "test_indices = split_indices['test']\n",
    "\n",
    "print(f\"Train indices: {train_indices}\")\n",
    "print(f\"Validation indices: {val_indices}\")\n",
    "print(f\"Test indices: {test_indices}\")\n",
    "\n",
    "# 3. Process Predictions and Actual Values\n",
    "y_list = []\n",
    "y_hat_list = []\n",
    "mask_list = []\n",
    "\n",
    "for batch in predictions_test:\n",
    "    y_tensor = batch['y']\n",
    "    y_hat_tensor = batch['y_hat']\n",
    "    mask_tensor = batch['mask']\n",
    "    \n",
    "    y_np = y_tensor.cpu().numpy().squeeze(-1)      # Shape: (batch_size, steps, nodes)\n",
    "    y_hat_np = y_hat_tensor.cpu().numpy().squeeze(-1)\n",
    "    mask_np = mask_tensor.cpu().numpy().squeeze(-1)\n",
    "    \n",
    "    y_list.append(y_np)\n",
    "    y_hat_list.append(y_hat_np)\n",
    "    mask_list.append(mask_np)\n",
    "\n",
    "# Concatenate along the batch dimension\n",
    "y = np.concatenate(y_list, axis=0)         # Shape: (num_test_batches, steps, nodes)\n",
    "y_hat = np.concatenate(y_hat_list, axis=0) # Shape: (num_test_batches, steps, nodes)\n",
    "mask = np.concatenate(mask_list, axis=0)   # Shape: (num_test_batches, steps, nodes)\n",
    "\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "print(f\"Shape of y_hat: {y_hat.shape}\")\n",
    "print(f\"Shape of mask: {mask.shape}\")\n",
    "\n",
    "# 4. Map Node IDs to Geographical Coordinates\n",
    "num_steps = y.shape[1]          # Number of prediction steps (e.g., 6)\n",
    "num_test_dates = y.shape[0]     # Number of test dates (e.g., 25)\n",
    "\n",
    "# Create node_ids, lat, lon repeated for each step\n",
    "node_ids = np.tile(np.arange(num_nodes), num_test_dates * num_steps)  # Shape: (3,294,000,)\n",
    "lat_column = np.tile(latitudes, num_test_dates * num_steps)\n",
    "lon_column = np.tile(longitudes, num_test_dates * num_steps)\n",
    "\n",
    "print(f\"Shape of node_ids: {node_ids.shape}\")\n",
    "print(f\"Shape of lat_column: {lat_column.shape}\")\n",
    "print(f\"Shape of lon_column: {lon_column.shape}\")\n",
    "\n",
    "# 5. Align Predictions with Dates\n",
    "\n",
    "# Repeat each test date for the number of prediction steps\n",
    "test_dates_steps = np.repeat(dates[test_indices], num_steps)  # Shape: (25 * 6,) => (150,)\n",
    "\n",
    "# Now, repeat each date for all nodes to align with the predictions\n",
    "all_dates_expanded = np.repeat(test_dates_steps, num_nodes)   # Shape: (150 * 21960,) => (3,294,000,)\n",
    "\n",
    "print(f\"Shape of test_dates_steps: {test_dates_steps.shape}\")       # (150,)\n",
    "print(f\"Shape of all_dates_expanded: {all_dates_expanded.shape}\") # (3,294,000,)\n",
    "\n",
    "# 6. Flatten the Predictions and Mask\n",
    "y_flat = y.flatten()         # Shape: (3,294,000,)\n",
    "y_hat_flat = y_hat.flatten()\n",
    "mask_flat = mask.flatten()\n",
    "\n",
    "print(f\"Shape of y_flat: {y_flat.shape}\")\n",
    "print(f\"Shape of y_hat_flat: {y_hat_flat.shape}\")\n",
    "print(f\"Shape of mask_flat: {mask_flat.shape}\")\n",
    "\n",
    "# 7. Apply Mask to Filter Valid Entries\n",
    "valid_indices = mask_flat.astype(bool)  # Ensure mask is boolean\n",
    "\n",
    "filtered_dates = all_dates_expanded[valid_indices]  # Shape: (num_valid_entries,)\n",
    "filtered_lat = lat_column[valid_indices]\n",
    "filtered_lon = lon_column[valid_indices]\n",
    "filtered_y = y_flat[valid_indices]\n",
    "filtered_y_hat = y_hat_flat[valid_indices]\n",
    "filtered_node_ids = node_ids[valid_indices]\n",
    "\n",
    "print(f\"Shape of filtered_dates: {filtered_dates.shape}\")\n",
    "print(f\"Shape of filtered_lat: {filtered_lat.shape}\")\n",
    "print(f\"Shape of filtered_lon: {filtered_lon.shape}\")\n",
    "print(f\"Shape of filtered_y: {filtered_y.shape}\")\n",
    "print(f\"Shape of filtered_y_hat: {filtered_y_hat.shape}\")\n",
    "print(f\"Shape of filtered_node_ids: {filtered_node_ids.shape}\")\n",
    "\n",
    "# 8. Create the DataFrame\n",
    "df_predictions = pd.DataFrame({\n",
    "    'date': filtered_dates,\n",
    "    'lat': filtered_lat,\n",
    "    'lon': filtered_lon,\n",
    "    'node_id': filtered_node_ids,\n",
    "    'y': filtered_y,\n",
    "    'y_hat': filtered_y_hat\n",
    "})\n",
    "\n",
    "print(df_predictions.head())\n",
    "print(df_predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>node_id</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.333466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-179.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.811117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-178.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.832692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-177.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.864189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-176.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.835293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293995</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>30.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>21955</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.903584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293996</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>30.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>21956</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.926460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293997</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>30.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>21957</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.947361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293998</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>30.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>21958</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.960774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293999</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>30.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>21959</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.930120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3294000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date   lat    lon  node_id    y     y_hat\n",
       "0       2019-07-01 -30.0 -180.0        0  0.0 -0.333466\n",
       "1       2019-07-01 -30.0 -179.0        1 -1.0 -0.811117\n",
       "2       2019-07-01 -30.0 -178.0        2 -1.0 -0.832692\n",
       "3       2019-07-01 -30.0 -177.0        3 -1.0 -0.864189\n",
       "4       2019-07-01 -30.0 -176.0        4 -1.0 -0.835293\n",
       "...            ...   ...    ...      ...  ...       ...\n",
       "3293995 2021-07-01  30.0  175.0    21955 -1.0 -0.903584\n",
       "3293996 2021-07-01  30.0  176.0    21956 -1.0 -0.926460\n",
       "3293997 2021-07-01  30.0  177.0    21957 -1.0 -0.947361\n",
       "3293998 2021-07-01  30.0  178.0    21958 -1.0 -0.960774\n",
       "3293999 2021-07-01  30.0  179.0    21959 -1.0 -0.930120\n",
       "\n",
       "[3294000 rows x 6 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def visualize_df_with_play_button(df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Optimized version of visualize_df_with_play_button function.\n",
    "    Visualizes a DataFrame with latitude and longitude data on a map, comparing 'y' and 'y_hat'.\n",
    "    A play button is included to animate the map over time.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame with columns ['date', 'lat', 'lon', 'node_id', 'y', 'y_hat'].\n",
    "\n",
    "    Returns:\n",
    "        go.Figure: A Plotly figure object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the date is in datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Create a scatter map for 'y'\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add scatter plots for 'y' (left map)\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=df['lon'],\n",
    "        lat=df['lat'],\n",
    "        text=df['y'],\n",
    "        marker=dict(color=df['y'], colorscale='Viridis', size=5, colorbar=dict(title='y')),\n",
    "        hovertemplate=\"Lat: %{lat}<br>Lon: %{lon}<br>y: %{text}<extra></extra>\",\n",
    "        name=\"y\"\n",
    "    ))\n",
    "\n",
    "    # Add scatter plots for 'y_hat' (right map)\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=df['lon'],\n",
    "        lat=df['lat'],\n",
    "        text=df['y_hat'],\n",
    "        marker=dict(color=df['y_hat'], colorscale='Cividis', size=5, colorbar=dict(title='y_hat')),\n",
    "        hovertemplate=\"Lat: %{lat}<br>Lon: %{lon}<br>y_hat: %{text}<extra></extra>\",\n",
    "        name=\"y_hat\"\n",
    "    ))\n",
    "\n",
    "    # Update layout for two maps side-by-side\n",
    "    fig.update_geos(\n",
    "        scope='world',\n",
    "        showcountries=True,\n",
    "        showland=True,\n",
    "        landcolor=\"lightgray\"\n",
    "    )\n",
    "    \n",
    "    # Split into two subplots: left for 'y', right for 'y_hat'\n",
    "    fig.update_layout(\n",
    "        title=\"Visualization of 'y' and 'y_hat' over time\",\n",
    "        grid=dict(rows=1, columns=2),\n",
    "        geo=dict(\n",
    "            showframe=False,\n",
    "            showcoastlines=True,\n",
    "            projection_type='equirectangular'\n",
    "        ),\n",
    "        updatemenus=[{\n",
    "            'type': 'buttons',\n",
    "            'showactive': False,\n",
    "            'buttons': [{\n",
    "                'label': 'Play',\n",
    "                'method': 'animate',\n",
    "                'args': [None, {\n",
    "                    'frame': {'duration': 500, 'redraw': True},\n",
    "                    'fromcurrent': True,\n",
    "                    'mode': 'immediate',\n",
    "                }]\n",
    "            }]\n",
    "        }],\n",
    "    )\n",
    "\n",
    "    # Sample a subset of the data for visualization\n",
    "    sample_size = min(len(df), 1000)  # Adjust the sample size as needed\n",
    "    sampled_df = df.sample(sample_size)\n",
    "\n",
    "    # Create frames for animation\n",
    "    frames = []\n",
    "    for date, group_df in sampled_df.groupby('date'):\n",
    "        frames.append(go.Frame(\n",
    "            data=[\n",
    "                go.Scattergeo(\n",
    "                    lon=group_df['lon'],\n",
    "                    lat=group_df['lat'],\n",
    "                    text=group_df['y'],\n",
    "                    marker=dict(color=group_df['y'], colorscale='Viridis', size=5),\n",
    "                    name=\"y\"\n",
    "                ),\n",
    "                go.Scattergeo(\n",
    "                    lon=group_df['lon'],\n",
    "                    lat=group_df['lat'],\n",
    "                    text=group_df['y_hat'],\n",
    "                    marker=dict(color=group_df['y_hat'], colorscale='Cividis', size=5),\n",
    "                    name=\"y_hat\"\n",
    "                )\n",
    "            ],\n",
    "            name=str(date)\n",
    "        ))\n",
    "\n",
    "    # Add the frames to the figure\n",
    "    fig.frames = frames\n",
    "\n",
    "    return fig\n",
    "fig = visualize_df_with_play_button(df)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def visualize_predictions_plotly(predictions, metadata, idx, horizon=None, \n",
    "                                 batches_to_visualize=None, variables_to_visualize=None):\n",
    "    \"\"\"\n",
    "    Visualize predicted vs actual values using Plotly for specified batches and variables with variable-specific normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions (dict): Contains 'y' and 'y_hat' tensors.\n",
    "    - metadata (pd.DataFrame): DataFrame with 'lat' and 'lon' columns.\n",
    "    - idx (int): Index of the item being visualized.\n",
    "    - horizon (int, optional): Number of time steps to visualize. Defaults to the maximum horizon in data.\n",
    "    - batches_to_visualize (list or range, optional): List of batch indices to visualize. Defaults to all batches.\n",
    "    - variables_to_visualize (list or range, optional): List of variable indices to visualize. Defaults to all variables.\n",
    "    \"\"\"\n",
    "    # Extract predictions and actual values\n",
    "    y_hat = predictions['y_hat'].squeeze().numpy()  # Shape: [batch, horizon, spatial_dim, variables]\n",
    "    y = predictions['y'].squeeze().numpy()          # Shape: [batch, horizon, spatial_dim, variables]\n",
    "\n",
    "    # Ensure y_hat and y have the same shape\n",
    "    assert y_hat.shape == y.shape, \"Predictions and actual values must have the same shape\"\n",
    "\n",
    "    # Determine the horizon if not provided\n",
    "    if horizon is None:\n",
    "        horizon = y_hat.shape[1]\n",
    "\n",
    "    # Determine the number of variables\n",
    "    if y_hat.ndim == 3:\n",
    "        # Shape: [batch, horizon, spatial_dim]\n",
    "        num_variables = 1\n",
    "        y_hat = y_hat[..., np.newaxis]  # Add a variables dimension\n",
    "        y = y[..., np.newaxis]\n",
    "    else:\n",
    "        num_variables = y_hat.shape[-1]\n",
    "\n",
    "    if variables_to_visualize is None:\n",
    "        variables_to_visualize = list(range(num_variables))\n",
    "    else:\n",
    "        # Ensure the variable indices are within the correct range\n",
    "        variables_to_visualize = [v for v in variables_to_visualize if v < num_variables]\n",
    "        if not variables_to_visualize:\n",
    "            raise ValueError(\"No valid variables to visualize. Check 'variables_to_visualize' indices.\")\n",
    "\n",
    "    # Get latitude and longitude from metadata\n",
    "    lats = metadata['lat'].values\n",
    "    lons = metadata['lon'].values\n",
    "\n",
    "    # Ensure that the number of spatial points matches the number of lats and lons\n",
    "    spatial_dim = y_hat.shape[2]\n",
    "    if spatial_dim != len(lats):\n",
    "        raise ValueError(f\"The number of spatial points in predictions ({spatial_dim}) does not match the number of locations in metadata ({len(lats)}).\")\n",
    "\n",
    "    # Calculate the min and max for lats and lons for setting map extent\n",
    "    lat_min, lat_max = lats.min(), lats.max()\n",
    "    lon_min, lon_max = lons.min(), lons.max()\n",
    "\n",
    "    # If batches_to_visualize is None, visualize all batches\n",
    "    if batches_to_visualize is None:\n",
    "        batches_to_visualize = range(y_hat.shape[0])\n",
    "    else:\n",
    "        # Ensure the batch indices are within the correct range\n",
    "        batches_to_visualize = [b for b in batches_to_visualize if b < y_hat.shape[0]]\n",
    "        if not batches_to_visualize:\n",
    "            raise ValueError(\"No valid batches to visualize. Check 'batches_to_visualize' indices.\")\n",
    "\n",
    "    # Precompute min and max for each variable in variables_to_visualize\n",
    "    var_min_max = {}\n",
    "    for var in variables_to_visualize:\n",
    "        var_data_y = y[..., var]  # Shape: [batch, horizon, spatial_dim]\n",
    "        var_data_y_hat = y_hat[..., var]  # Shape: [batch, horizon, spatial_dim]\n",
    "        min_val = min(np.min(var_data_y), np.min(var_data_y_hat))\n",
    "        max_val = max(np.max(var_data_y), np.max(var_data_y_hat))\n",
    "        import numpy as np\n",
    "\n",
    "        # ...\n",
    "\n",
    "        # Loop through each batch and variable\n",
    "        for batch in batches_to_visualize:\n",
    "            for var in variables_to_visualize:\n",
    "                frames = []\n",
    "                slider_steps = []\n",
    "\n",
    "                # Retrieve normalization for the current variable\n",
    "                min_val, max_val = var_min_max[var]\n",
    "\n",
    "                # Get the highest and lowest values of the variable\n",
    "                var_min = np.min(y_hat[batch, :, :, var])\n",
    "                var_max = np.max(y_hat[batch, :, :, var])\n",
    "\n",
    "                # Update the min and max values if necessary\n",
    "                if var_min < min_val:\n",
    "                    min_val = var_min\n",
    "                if var_max > max_val:\n",
    "                    max_val = var_max\n",
    "\n",
    "                # Update the normalization dictionary\n",
    "                var_min_max[var] = (min_val, max_val)\n",
    "\n",
    "                # ...\n",
    "\n",
    "                # Predicted data for current month\n",
    "                pred_data = go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=y_hat[batch, time_step, :, var],\n",
    "                        colorscale='RdYlBu_r',\n",
    "                        cmin=min_val,\n",
    "                        cmax=max_val,\n",
    "                        colorbar=dict(title='Predicted'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name='Predicted',\n",
    "                    showlegend=False\n",
    "                )\n",
    "\n",
    "                # Actual data for current month\n",
    "                actual_data = go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=y[batch, time_step, :, var],\n",
    "                        colorscale='RdYlBu_r',\n",
    "                        cmin=min_val,\n",
    "                        cmax=max_val,\n",
    "                        colorbar=dict(title='Actual'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name='Actual',\n",
    "                    showlegend=False\n",
    "                )\n",
    "\n",
    "                # ...\n",
    "\n",
    "                # Initial data (first frame)\n",
    "                pred_data_initial = go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=y_hat[batch, 0, :, var],\n",
    "                        colorscale='RdYlBu_r',\n",
    "                        cmin=min_val,\n",
    "                        cmax=max_val,\n",
    "                        colorbar=dict(title='Predicted'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name='Predicted',\n",
    "                    showlegend=True\n",
    "                )\n",
    "\n",
    "                actual_data_initial = go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=y[batch, 0, :, var],\n",
    "                        colorscale='RdYlBu_r',\n",
    "                        cmin=min_val,\n",
    "                        cmax=max_val,\n",
    "                        colorbar=dict(title='Actual'),\n",
    "                        opacity=0.8\n",
    "                    ),\n",
    "                    name='Actual',\n",
    "                    showlegend=True\n",
    "                )\n",
    "\n",
    "                # ...\n",
    "                        visible=True,\n",
    "                        prefix=\"Date: \",\n",
    "                        xanchor=\"right\",\n",
    "                        font=dict(size=14, color=\"#666\")\n",
    "                    ),\n",
    "                )],\n",
    "                geo=dict(\n",
    "                    scope='world',\n",
    "                    projection_type='natural earth',\n",
    "                    showland=True,\n",
    "                    landcolor='lightgray',\n",
    "                    showcountries=True,\n",
    "                    countrycolor='black',\n",
    "                    lataxis=dict(range=[lat_min - 1, lat_max + 1]),\n",
    "                    lonaxis=dict(range=[lon_min - 1, lon_max + 1]),\n",
    "                ),\n",
    "                geo2=dict(\n",
    "                    scope='world',\n",
    "                    projection_type='natural earth',\n",
    "                    showland=True,\n",
    "                    landcolor='lightgray',\n",
    "                    showcountries=True,\n",
    "                    countrycolor='black',\n",
    "                    lataxis=dict(range=[lat_min - 1, lat_max + 1]),\n",
    "                    lonaxis=dict(range=[lon_min - 1, lon_max + 1]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Update frames to assign traces to correct subplots\n",
    "            for frame in fig.frames:\n",
    "                time_step_idx = int(frame.name.split()[1]) - 1\n",
    "                frame.data = [\n",
    "                    go.Scattergeo(\n",
    "                        lon=lons,\n",
    "                        lat=lats,\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=6,\n",
    "                            color=y_hat[batch, time_step_idx, :, var],\n",
    "                            colorscale='RdYlBu_r',\n",
    "                            cmin=min_val,\n",
    "                            cmax=max_val,\n",
    "                            opacity=0.8\n",
    "                        ),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    go.Scattergeo(\n",
    "                        lon=lons,\n",
    "                        lat=lats,\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=6,\n",
    "                            color=y[batch, time_step_idx, :, var],\n",
    "                            colorscale='RdYlBu_r',\n",
    "                            cmin=min_val,\n",
    "                            cmax=max_val,\n",
    "                            opacity=0.8\n",
    "                        ),\n",
    "                        showlegend=False\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            # Update layout for better appearance\n",
    "            fig.update_layout(\n",
    "                height=600,\n",
    "                width=1200,\n",
    "                margin=dict(l=50, r=50, t=100, b=50)\n",
    "            )\n",
    "\n",
    "            # Display the figure\n",
    "            fig.show()\n",
    "\n",
    "            # Optionally, save the figure to an HTML file\n",
    "            # fig.write_html(f'item_{idx+1}_batch_{batch+1}_variable_{var+1}.html')\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "# Assuming you have the metadata DataFrame available\n",
    "# metadata = pd.read_parquet('ml-drought-forecasting/ml-modeling-pipeline/data/05_model_input/metadata.parquet')\n",
    "\n",
    "# Ensure that predictions_test is a list of dictionaries with 'y' and 'y_hat'\n",
    "# For example:\n",
    "# predictions_test = [{'y': tensor1, 'y_hat': tensor2}, {'y': tensor3, 'y_hat': tensor4}, ...]\n",
    "\n",
    "# # Specify the indices of the items you want to visualize\n",
    "# items_to_visualize = [1]  # Replace with desired item indices (0-based indexing)\n",
    "# batches_to_visualize = [0, 1]  # Replace with desired batch indices within each item\n",
    "# variables_to_visualize = [0]  # Replace with desired variable indices (e.g., if multiple EDDI metrics)\n",
    "\n",
    "# # Call the function for specified items in predictions_test\n",
    "# for idx in items_to_visualize:\n",
    "#     pred = predictions_test[idx]\n",
    "#     visualize_predictions_plotly(\n",
    "#         pred, metadata, idx, horizon=pred['y'].shape[1],\n",
    "#         batches_to_visualize=batches_to_visualize, \n",
    "#         variables_to_visualize=variables_to_visualize\n",
    "#     )\n",
    "#     print(f\"Processed item {idx + 1}/{len(predictions_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
