{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import regionmask\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask_geopandas as dgpd\n",
    "from shapely.geometry import Point\n",
    "from dask import dataframe as dd\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "import polars as pl\n",
    "import gc \n",
    "import pyarrow as pa\n",
    "\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert file format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/power_nasa_data.nc')\n",
    "\n",
    "df_pandas = ds.to_dataframe().reset_index()\n",
    "\n",
    "# Convert the Pandas DataFrame to a Polars DataFrame\n",
    "df_polars = pl.from_pandas(df_pandas)\n",
    "\n",
    "df_polars.write_parquet(\"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/02_intermediate/power_nasa_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge raw files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POWER NASA \n",
    "ALready merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parquet_files(parquet_files: List[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge multiple Parquet files into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        parquet_files (List[str]): List of file paths to the Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Merged DataFrame containing data from all the Parquet files.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterate over the Parquet files\n",
    "    for file_path in parquet_files:\n",
    "        try:\n",
    "            # Read the Parquet file\n",
    "            df = pl.read_parquet(file_path)\n",
    "            \n",
    "            # Extract the time period from the file name\n",
    "            time_period = file_path.split(\"_\")[-1].split(\".\")[0]\n",
    "            \n",
    "            # Rename the 'eddi' column with the time period\n",
    "            df = df.rename({\"eddi\": f\"eddi_{time_period}\"})\n",
    "            \n",
    "            # Check if 'time' column is already in Date format\n",
    "            if df.dtypes[df.columns.index(\"time\")] != pl.Date:\n",
    "                # Convert 'time' column to Date format\n",
    "                df = df.with_columns(pl.col(\"time\").cast(pl.Date))\n",
    "            \n",
    "            # Append the dataframe to the list\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {file_path}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Check if any dataframes were successfully loaded\n",
    "    if not dataframes:\n",
    "        raise ValueError(\"No valid dataframes found in the provided Parquet files.\")\n",
    "\n",
    "    # Merge all the dataframes based on 'time', 'lat', and 'lon'\n",
    "    merged_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        try:\n",
    "            merged_df = merged_df.join(df, on=[\"time\", \"lat\", \"lon\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error merging dataframe: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your Parquet files\n",
    "parquet_files = [\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_01wk.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_02wk.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_03mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_06mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_09mn.parquet\",\n",
    "    \"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/merged_eddi_data_12mn.parquet\"\n",
    "]\n",
    "\n",
    "merged_df = merge_parquet_files(parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/02_intermediate/merged_eddi_data.parquet'\n",
    "merged_df.write_parquet(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_files(file_list):\n",
    "    \"\"\"\n",
    "    Delete the specified files.\n",
    "\n",
    "    This function attempts to delete each file in the provided list.\n",
    "    It handles exceptions for file not found, permission errors, and other potential issues.\n",
    "\n",
    "    Args:\n",
    "    file_list (list): A list of file paths (strings) to be deleted.\n",
    "\n",
    "    Returns:\n",
    "    None. The function prints the status of each deletion attempt.\n",
    "    \"\"\"\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Successfully deleted: {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "delete_files(parquet_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "nino12 = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/inino12_daily.nc')\n",
    "nino3 = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/inino3_daily.nc')\n",
    "nino34 = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/inino34_daily.nc')\n",
    "nino4 = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/inino4_daily.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_convert(dataset, variable_name, start_date='1981-09-01'):\n",
    "    \"\"\"\n",
    "    Process an xarray Dataset and convert it to a Polars DataFrame.\n",
    "\n",
    "    This function works with datasets that have either 'time' or 'TIME' as the time dimension.\n",
    "\n",
    "    Args:\n",
    "    dataset (xarray.Dataset): The input dataset.\n",
    "    variable_name (str): The name of the variable to process.\n",
    "    start_date (str): The start date for data selection. Default is '1981-09-01'.\n",
    "\n",
    "    Returns:\n",
    "    polars.DataFrame: The processed data as a Polars DataFrame.\n",
    "    \"\"\"\n",
    "    # Determine the time dimension name\n",
    "    time_dim = 'time' if 'time' in dataset.dims else 'TIME'\n",
    "\n",
    "    # Find the maximum valid date dynamically\n",
    "    max_valid_date = dataset[variable_name].dropna(dim=time_dim, how='all')[time_dim].max().values\n",
    "    \n",
    "    # Trim the dataset from the first known valid date to the last known valid date\n",
    "    valid_data = dataset.sel({time_dim: slice(start_date, max_valid_date)})\n",
    "    \n",
    "    # Convert the trimmed xarray Dataset to a pandas DataFrame\n",
    "    df_valid = valid_data[variable_name].to_dataframe(name=variable_name)\n",
    "    \n",
    "    # Reset the index to convert the datetime index into a regular column\n",
    "    df_valid_reset = df_valid.reset_index()\n",
    "    \n",
    "    # Convert the pandas DataFrame with reset index to a Polars DataFrame\n",
    "    pl_valid = pl.from_pandas(df_valid_reset)\n",
    "    \n",
    "    return pl_valid\n",
    "\n",
    "# Process each dataset\n",
    "pl_nino12 = process_and_convert(nino12, 'Nino12')\n",
    "pl_nino3 = process_and_convert(nino3, 'Nino3')\n",
    "pl_nino34 = process_and_convert(nino34, 'Nino34')\n",
    "pl_nino4 = process_and_convert(nino4, 'Nino4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(*datasets):\n",
    "    \"\"\"\n",
    "    Merge multiple Polars DataFrames on their time column.\n",
    "\n",
    "    This function works with datasets that have either 'time' or 'TIME' as the time column.\n",
    "\n",
    "    Args:\n",
    "    *datasets: Variable number of Polars DataFrames to merge.\n",
    "\n",
    "    Returns:\n",
    "    polars.DataFrame: The merged DataFrame.\n",
    "    \"\"\"\n",
    "    if not datasets:\n",
    "        return None\n",
    "\n",
    "    # Determine the time column name from the first dataset\n",
    "    time_col = 'time' if 'time' in datasets[0].columns else 'TIME'\n",
    "\n",
    "    # Initial dataset to start merging from\n",
    "    merged_df = datasets[0]\n",
    "    \n",
    "    # Iterate over remaining datasets and merge them one by one\n",
    "    for data in datasets[1:]:\n",
    "        # Ensure the joining column has the same name in both DataFrames\n",
    "        if time_col not in data.columns:\n",
    "            data = data.rename({'TIME': 'time'} if 'TIME' in data.columns else {'time': 'TIME'})\n",
    "        \n",
    "        # Use Polars' join function with a custom suffix to prevent name clashes\n",
    "        merged_df = merged_df.join(data, on=time_col, how=\"outer\", suffix=\"_right\")\n",
    "        \n",
    "        # If '_right' columns are created (which contain the same data), drop them\n",
    "        right_cols = [col for col in merged_df.columns if col.endswith('_right')]\n",
    "        merged_df = merged_df.drop(right_cols)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "    # Merge the dataframes\n",
    "pl_merged_nino = merge_datasets(pl_nino12, pl_nino3, pl_nino34, pl_nino4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_merged_nino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_merged_nino.write_parquet(\"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/02_intermediate/merged_nino_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtio = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/wtio.nc')\n",
    "setio = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/setio.nc')\n",
    "dmi = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/dmi.nc')\n",
    "swio = xr.open_dataset('/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/01_raw/swio.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each dataset\n",
    "pl_wtio = process_and_convert(wtio, 'WTIO')\n",
    "pl_setio = process_and_convert(setio, 'SETIO')\n",
    "pl_dmi = process_and_convert(dmi, 'DMI')\n",
    "pl_swio = process_and_convert(swio, 'SWIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_merged_iod = merge_datasets(pl_wtio, pl_setio, pl_dmi, pl_swio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_merged_iod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_merged_iod.write_parquet(\"/workspace/ml-drought-forecasting/ml-modeling-pipeline/data/02_intermediate/merged_iod_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
